{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast, GPT2Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = str(pathlib.Path().resolve())\n",
    "DATASET_PATH = MAIN_PATH + '\\\\datasets'\n",
    "MODEL_PATH = MAIN_PATH + '\\\\models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-cased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-cased',\n",
       " 'bert-large-uncased',\n",
       " 'flan-t5-base',\n",
       " 'flan-t5-large',\n",
       " 'flan-t5-small',\n",
       " 'gpt2',\n",
       " 'gpt2-large',\n",
       " 'gpt2-medium']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = os.listdir(MODEL_PATH)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python\\\\LLM_Environment\\\\models\\\\gpt2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = MODEL_PATH + '\\\\' + models[8]\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = GPT2Config.from_pretrained(model_path)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path, config=configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Recipes.csv', 'Recipes_1000.csv', 'train.csv']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = os.listdir(DATASET_PATH)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python\\\\LLM_Environment\\\\datasets\\\\Recipes_1000.csv'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = DATASET_PATH + '\\\\' + filenames[1]\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecipeId</th>\n",
       "      <th>name</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>instructions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38</td>\n",
       "      <td>Low-Fat Berry Blue Frozen Dessert</td>\n",
       "      <td>blueberries, granulated sugar, vanilla yogurt,...</td>\n",
       "      <td>Toss 2 cups berries with sugar. Let stand for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>Biryani</td>\n",
       "      <td>saffron, milk, hot green chili peppers, onions...</td>\n",
       "      <td>Soak saffron in warm milk for 5 minutes and pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>Best Lemonade</td>\n",
       "      <td>sugar, lemons, rind of, lemon, zest of, fresh ...</td>\n",
       "      <td>Into a 1 quart Jar with tight fitting lid, put...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>Carina's Tofu-Vegetable Kebabs</td>\n",
       "      <td>extra firm tofu, eggplant, zucchini, mushrooms...</td>\n",
       "      <td>Drain the tofu, carefully squeezing out excess...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42</td>\n",
       "      <td>Cabbage Soup</td>\n",
       "      <td>plain tomato juice, cabbage, onion, carrots, c...</td>\n",
       "      <td>Mix everything together and bring to a boil. R...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecipeId                               name  \\\n",
       "0        38  Low-Fat Berry Blue Frozen Dessert   \n",
       "1        39                            Biryani   \n",
       "2        40                      Best Lemonade   \n",
       "3        41     Carina's Tofu-Vegetable Kebabs   \n",
       "4        42                       Cabbage Soup   \n",
       "\n",
       "                                         ingredients  \\\n",
       "0  blueberries, granulated sugar, vanilla yogurt,...   \n",
       "1  saffron, milk, hot green chili peppers, onions...   \n",
       "2  sugar, lemons, rind of, lemon, zest of, fresh ...   \n",
       "3  extra firm tofu, eggplant, zucchini, mushrooms...   \n",
       "4  plain tomato juice, cabbage, onion, carrots, c...   \n",
       "\n",
       "                                        instructions  \n",
       "0  Toss 2 cups berries with sugar. Let stand for ...  \n",
       "1  Soak saffron in warm milk for 5 minutes and pu...  \n",
       "2  Into a 1 quart Jar with tight fitting lid, put...  \n",
       "3  Drain the tofu, carefully squeezing out excess...  \n",
       "4  Mix everything together and bring to a boil. R...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: beef, salt, pepper and white vinegar. Mix well. When you get the meat tender, it's time to add it to your salad. I used to do this on all my salads. I was always pretty sure I had it when I saw them, but that's not always the case. But I found myself eating it a lot and I really loved the crunch of the cheese and the saltiness. I love the saltiness. I love the garlic. You can definitely use this recipe to make salad for many meals in a row and it can be quite an appetizer for anyone....\n",
      "  ---\n",
      "1: beef, salt, pepper, garlic, ginger, garlic, parsley, parsley juice, thyme, thyme seeds, oregano, parsley powder, basil, oregano, oregano leaves, parsley, parsley pulp, pepper, salt, pepper flakes, pepper, pepper paste, pepper paste vinegar, salt, pepper, vinegar, garlic, garlic leaves, vinegar, garlic powder, garlic acid, garlic acid, garlic acid, garlic acid, garlic acid, garlic acid, garlic acid, garlic acid, garlic acid, garlic acid, garlic acid, garlic acid...\n",
      "  ---\n",
      "2: beef, salt, pepper, pepper powder and black pepper. The taste is quite savory, and there are some spices in there, like coriander and cinnamon. The meat is very well cut, with the texture of a beef steak. If you have a large pot, and like to cook meat a little longer than your regular beef, you can use your smoker. This makes it perfect for making the best steak you will ever have.\n",
      "\n",
      "The beef is very tender and well seasoned. The meat is so thin and meaty that when you taste the taste, you will want to...\n",
      "  ---\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "input_sequence = \"beef, salt, pepper\"\n",
    "input_ids = tokenizer.encode(input_sequence, return_tensors='pt')\n",
    "\n",
    "model = model.to(device)\n",
    "#combine both sampling techniques\n",
    "sample_outputs = model.generate(input_ids.to(device),\n",
    "                              do_sample = True, max_length = 120,\n",
    "                              top_k = 50, top_p = 0.85,\n",
    "                              num_return_sequences = 3)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_special_tokens = True)))\n",
    "    print('  ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<|startoftext|>Ingredients: blueberries, granulated sugar, vanilla yogurt, lemon juice. Instructions: Toss 2 cups berries with sugar. Let stand for 45 minutes, stirring occasionally. Transfer berry-sugar mixture to food processor. Add yogurt and process until smooth. Strain through fine sieve. Pour into baking pan (or transfer to ice cream maker and process according to manufacturers' directions). Freeze uncovered until edges are solid but centre is soft.  Transfer to processor and blend until smooth again. Return to pan and freeze until edges are solid. Transfer to processor and blend until smooth again. Fold in remaining 2 cups of blueberries. Pour into plastic mold and freeze overnight. Let soften slightly to serve.<|endoftext|>\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def form_string(ingredient,instruction):\n",
    "    # s = f\"<|startoftext|>Ingredients:\\n{ingredient.strip()}\\n\\nInstructions:\\n{instruction.strip()}<|endoftext|>\"\n",
    "    s = f\"<|startoftext|>Ingredients: {ingredient.strip()}. \" \\\n",
    "        f\"Instructions: {instruction.strip()}<|endoftext|>\"\n",
    "    return s\n",
    "\n",
    "def extract_string(recipe):\n",
    "    str = recipe.replace('<|startoftext|>', '').replace('<|endoftext|>', '')\n",
    "    inst_pos = str.find('Instructions: ')\n",
    "    ingredients = str[len('Ingredients: '): inst_pos-1]\n",
    "    instructions = str[inst_pos+len('Instructions: '):]\n",
    "    return ingredients, instructions\n",
    "\n",
    "data = df.apply(lambda x:form_string(\n",
    "    x['ingredients'], x['instructions']), axis=1).to_list()\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_path,\n",
    "                                              bos_token='<|startoftext|>',\n",
    "                                              eos_token='<|endoftext|>',\n",
    "                                              unk_token='<|unknown|>',\n",
    "                                              pad_token='<|pad|>'\n",
    "                                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('ĠPhoto', 5555)\n",
      "('Ġplus', 5556)\n",
      "('rick', 5557)\n",
      "('arks', 5558)\n",
      "('Ġalternative', 5559)\n",
      "('Ġpil', 5560)\n",
      "('Ġapprox', 5561)\n",
      "('that', 5562)\n",
      "('Ġobjects', 5563)\n",
      "('ĠRo', 5564)\n",
      "('ĠAndroid', 5565)\n"
     ]
    }
   ],
   "source": [
    "vocab_list = sorted(tokenizer.vocab.items(), key=lambda x:x[1])\n",
    "for i in range(5555, 5566):\n",
    "    print(vocab_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The max model length is 1024 for this model\n",
      "The end of sequence token <|endoftext|> has the id 50256\n",
      "The beginning of sequence token <|startoftext|> has the id 50257\n",
      "The unknown token <|unknown|> has the id 50258\n",
      "The padding token <|pad|> has the id 50259\n"
     ]
    }
   ],
   "source": [
    "print(\"The max model length is {} for this model\".format(tokenizer.model_max_length))\n",
    "print(\"The end of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id), tokenizer.eos_token_id))\n",
    "print(\"The beginning of sequence token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.bos_token_id), tokenizer.bos_token_id))\n",
    "print(\"The unknown token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.unk_token_id), tokenizer.unk_token_id))\n",
    "print(\"The padding token {} has the id {}\".format(tokenizer.convert_ids_to_tokens(tokenizer.pad_token_id), tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "max_length = 180  # maximum sentence length\n",
    "\n",
    "# standard PyTorch approach of loading data in using a Dataset class.\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "        self.origin_ingredients = []\n",
    "        self.origin_instructions = []\n",
    "\n",
    "        for recipe in data:\n",
    "            encodings = tokenizer.encode_plus(recipe,\n",
    "                                              truncation=True,\n",
    "                                              padding='max_length',\n",
    "                                              max_length=max_length,\n",
    "                                              return_tensors='pt'       # return PyTorch tensor\n",
    "                                             )\n",
    "            self.input_ids.append(torch.squeeze(encodings['input_ids'],0))\n",
    "            # attention_mask tells model not to incorporate these PAD tokens into its interpretation of the sentence\n",
    "            self.attn_masks.append(torch.squeeze(encodings['attention_mask'],0))\n",
    "            ingredients, instructions = extract_string(recipe)\n",
    "            self.origin_ingredients.append(ingredients)\n",
    "            self.origin_instructions.append(instructions)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.input_ids[idx], self.attn_masks[idx], self.origin_ingredients[idx], self.origin_instructions[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  900 training samples\n",
      "  100 validation samples\n"
     ]
    }
   ],
   "source": [
    "dataset = RecipeDataset(data, tokenizer)\n",
    "\n",
    "# Split into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size 1000\n",
      "dataset[0]: \n",
      "  input_ids: tensor([50257, 41222,    25,  4171, 20853,    11, 19468,  4817,  7543,    11,\n",
      "        16858, 32132,    11, 18873, 13135,    13, 27759,    25,   309,   793,\n",
      "          362, 14180, 36322,   351,  7543,    13,  3914,  1302,   329,  4153,\n",
      "         2431,    11, 26547, 10491,    13, 20558,   275,  6996,    12,    82,\n",
      "        35652, 11710,   284,  2057, 12649,    13,  3060, 32132,   290,  1429,\n",
      "         1566,  7209,    13,   520,  3201,   832,  3734,   264, 12311,    13,\n",
      "        39128,   656, 16871,  3425,   357,   273,  4351,   284,  4771,  8566,\n",
      "        16009,   290,  1429,  1864,   284, 11372,     6, 11678,   737, 34917,\n",
      "        18838,  1566, 13015,   389,  4735,   475,  7372,   318,  2705,    13,\n",
      "          220, 20558,   284, 12649,   290, 13516,  1566,  7209,   757,    13,\n",
      "         8229,   284,  3425,   290, 16611,  1566, 13015,   389,  4735,    13,\n",
      "        20558,   284, 12649,   290, 13516,  1566,  7209,   757,    13, 39957,\n",
      "          287,  5637,   362, 14180,   286,  4171, 20853,    13, 39128,   656,\n",
      "         7309, 15936,   290, 16611, 13417,    13,  3914, 39536,  4622,   284,\n",
      "         4691,    13, 50256, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
      "        50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259])\n",
      "  attn_masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"dataset size {dataset.__len__()}\")\n",
    "print(f\"dataset[0]: \\n  input_ids: {dataset[0][0]}\\n  attn_masks: {dataset[0][1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation datasets.\n",
    "# We'll take training samples in random order.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,  # The training samples.\n",
    "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "            batch_size = batch_size # Trains with this batch size.\n",
    "        )\n",
    "\n",
    "# For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "validation_dataloader = DataLoader(\n",
    "            val_dataset, # The validation samples.\n",
    "            sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "            batch_size = batch_size # Evaluate with this batch size.\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight shape torch.Size([50257, 768])\n",
      "Number of tokens: 50260\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weight shape {model.transformer.wte.weight.shape}\")\n",
    "# this step is necessary because I've added some tokens (bos_token, etc.) to the embeddings\n",
    "# otherwise the tokenizer and model tensors won't match up\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"Number of tokens: {len(tokenizer)}\")\n",
    "\n",
    "# Set the seed value all over the place to make this reproducible.\n",
    "seed_val = 42\n",
    "\n",
    "random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50260, 768])\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = model.transformer.wte.weight # Word Token Embeddings\n",
    "\n",
    "print(word_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "learning_rate = 2e-5\n",
    "warmup_steps = 1e2\n",
    "# The epsilon parameter eps = 1e-8 is “a very small number to prevent any division by zero in the implementation”\n",
    "epsilon = 1e-8\n",
    "# optim = Adam(model.parameters(), lr=5e-5)\n",
    "optim = AdamW(model.parameters(), lr = learning_rate, eps = epsilon)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    return str(datetime.timedelta(seconds=int(round((elapsed)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of training steps is [number of batches] x [number of epochs].\n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Create the learning rate scheduler.\n",
    "# This changes the learning rate as the training loop progresses\n",
    "scheduler = get_linear_schedule_with_warmup(optim,\n",
    "                                            num_warmup_steps = warmup_steps,\n",
    "                                            num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(prompt):\n",
    "    input = f\"<|startoftext|>Ingredients: {prompt.strip()}\"\n",
    "    input = tokenizer(input, return_tensors=\"pt\")\n",
    "    input_ids      = input[\"input_ids\"]\n",
    "    attention_mask = input[\"attention_mask\"]\n",
    "\n",
    "    output = model.generate(input_ids.to(device),\n",
    "                            attention_mask=attention_mask.to(device),\n",
    "                            max_new_tokens=max_length,\n",
    "                            # temperature = 0.5,\n",
    "                            do_sample = True, top_k = 50, top_p = 0.85)\n",
    "                            # num_beams=5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    450. Loss: 3.9301090240478516.   Elapsed: 0:00:10.\n",
      "Ingredients: eggs, flour, butter, sugar, and vanilla, creamed egg, vanilla extract, and vanilla extractFor the oven to set the mixture and the butter, mix the butter, eggs, butter, sugar, and vanilla, and- blend the eggs in a mixer fitted with a stand mixer and Beat the butter, egg, salt, and vanilla until smooth, andBake the softened butter in a small bowl, whisking, sugar, and eggs, stirring frequently, and then the eggs. in a small bowl, heat butter, stirring, until the eggs are melted and the mixture is melted. Stir in the dry ingredients. Pour the dry mixture in a bowl.Cook mixture and butter.The waffle maker works best.Sprinkle sugar and vanilla.Form the butter on a greased pan and butter.Bake for 10 minutes, stirring, and then a stand mixer fitted with mixer, scraping and whisking together. (\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    450. Loss: 2.8541507720947266.   Elapsed: 0:00:23.\n",
      "Ingredients: eggs, flour, butter, sugar, milk, milk and water, vanilla, salt, vanilla extract, lemon juice, lemon juice, salt and pepper, vanilla extract, pepper, salt, pepper, garlic, ginger, basil, garlic powder, ginger extract, pepper, salt, pepper, garlic powder, and salt.Cook, stirring frequently, cream a 1/2 cup vegetable oil and butter until lightly browned and browned, stirring in eggs, sugar, vanilla extract and salt. In a bowl, cream the eggs, sugar and butter until creamy; add salt, salt and pepper and mix in the butter. Add in the salt. Cover and chill until it has chilled, stirring. Drain, remove from heat and drain.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   300  of    450. Loss: 2.918560266494751.   Elapsed: 0:00:35.\n",
      "Ingredients: eggs, flour, butter, sugar, milk, corn, sugar, salt, pepper, pepper pepper, garlic, and onion, Instructions: Preheat oven to 350°F. Melt butter in a pan and cook. In the butter mixture, stir together flour and butter. In the milk, stir together. Stir in onions. Cut into 4-inch thick slices and cut into thin slices. Bake in pan until golden brown. Remove from heat and cool for 5 minutes. In a large skillet over medium heat, combine eggs, flour, butter, sugar, garlic, onions, salt, pepper, garlic, and onion. Cut into small strips, and arrange in the center of pan. Bake in 350°F oven for 15 minutes or until golden brown. Remove from heat and cool for 10 minutes or until firm. Serve with the remaining bacon and onion. Instructions: Prepare oven to 350°F. Remove from heat and brown the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   400  of    450. Loss: 2.6272802352905273.   Elapsed: 0:00:47.\n",
      "Ingredients: eggs, flour, butter, sugar, garlic, and salt Instructions: Preheat oven to 350°F. Mix together eggs, flour, sugar, garlic, salt, and lemon juice in a large skillet. Add remaining ingredients. Cook until golden brown on all sides and golden brown on top. Drain eggs on a cookie sheet and cool completely before placing on a cooling rack. Let stand for 15 minutes. When ready to serve, warm butter over medium heat. Instructions: Place the butter on a cookie sheet, place in the oven until thickened and set aside. Drain the cookies on a cookie sheet.\n",
      "\n",
      "  Average training loss: 6.36\n",
      "  Training epoch took: 0:00:54\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.16\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 2 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    450. Loss: 1.7224706411361694.   Elapsed: 0:00:10.\n",
      "Ingredients: eggs, flour, butter, sugar, milk, eggs. Instructions: Put egg yolks into butter. Cover and cook on low heat for 1 hour. Remove from heat. Cut into 3 or 4 equal pieces. Refrigerate at least 5 minutes before slicing. Instructions: Preheat oven to 425 degrees Fahrenheit. Line a baking sheet with foil. Brush cake with butter and lightly sprinkle sugar. Preheat oven to 425 degrees Fahrenheit. Place butter over waxed paper. Roll out on top. Bake at 425 degrees Fahrenheit for 20 to 25 minutes. Cool completely, or until toothpick inserted into the center comes out clean. Pour the remaining chocolate mixture over waxed paper and bake until cool to room temperature. Serve hot on top of prepared cake.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    450. Loss: 1.4465839862823486.   Elapsed: 0:00:22.\n",
      "Ingredients: eggs, flour, butter, sugar, cornstarch, water, vanilla, vanilla extract, baking soda, vanilla extract, cinnamon, cinnamon sugar, baking powder, and sugar. Instructions: Beat eggs in large bowl and beat flour mixture in dry ingredients. Pour into prepared pan and bake for 20 to 25 minutes or until firm. Remove from heat and stir together flour and sugar. Whisk in milk, butter, vanilla, cinnamon sugar, cinnamon mixture. In a large bowl, beat in remaining butter. Beat until combined. Pour into prepared pan. Bake for 20 to 25 minutes or until lightly browned and browned. Transfer to rack for about 20 minutes or until cool.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   300  of    450. Loss: 2.1579365730285645.   Elapsed: 0:00:34.\n",
      "Ingredients: eggs, flour, butter, sugar, baking powder. Instructions: In a large bowl, mix together egg, flour, sugar and baking powder. Add the milk and beat well. Stir in the butter. Stir in the flour and beat well. Add the eggs and beat well. Fold in the dry ingredients. Beat until thickened. Add the vanilla bean mix and beat well. Cover and refrigerate for at least 2 hours, or until firm.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   400  of    450. Loss: 1.675294041633606.   Elapsed: 0:00:45.\n",
      "Ingredients: eggs, flour, butter, sugar, vanilla extract, sugar. Instructions: Heat butter and butter together in a nonstick skillet. Add flour mixture and sugar and cook over medium heat, stirring frequently. Add eggs and beat well. Stir in vanilla and sugar mixture and continue to cook over medium-high heat. Stir in vanilla and stir in flour mixture until melted. Pour over pan. Bake at 400 degrees Fahrenheit for 35-40 minutes. Cool on rack until ready to use.\n",
      "\n",
      "  Average training loss: 2.17\n",
      "  Training epoch took: 0:00:51\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.04\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "======== Epoch 3 / 3 ========\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   100  of    450. Loss: 1.4124552011489868.   Elapsed: 0:00:10.\n",
      "Ingredients: eggs, flour, butter, sugar, salt, butter, salt. Instructions: Mix eggs, flour, butter, salt, butter. Mix until well blended. Pour batter into muffin cups, making sure they are well coated. Bake at 350 F until the eggs are firm. Chill muffins for 30 minutes, or until the muffins are lightly browned and golden.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   200  of    450. Loss: 1.1604747772216797.   Elapsed: 0:00:21.\n",
      "Ingredients: eggs, flour, butter, sugar, egg, flour, eggs, sugar. Instructions: Melt butter, flour, sugar and eggs in microwave with a high speed blender until blended. Stir in flour and sugar. Stir in egg and butter. Gradually add flour and sugar to mixture. Beat well, stirring constantly, until mixture forms a smooth ball. Pour into a greased 7 x 8x 8-inch pan. Roll out edges on top, leaving a zipped zucchini piece. Bake at 350°F for 15 to 20 minutes, or until golden brown. Cool completely before serving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   300  of    450. Loss: 1.063637375831604.   Elapsed: 0:00:32.\n",
      "Ingredients: eggs, flour, butter, sugar, butter and milk. Instructions: Beat eggs, butter, sugar, butter and milk until stiff peaks form. Add flour, then mix until very smooth. Add eggs and mix well. Place in shallow baking dish. Bake in 350°F oven for 10 to 15 minutes or until soft. Remove from heat and let cool completely. In a large bowl, mix eggs, flour, butter and milk until stiff peaks form. Place in shallow baking dish and bake until soft. Remove from heat.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch   400  of    450. Loss: 0.8958448767662048.   Elapsed: 0:00:44.\n",
      "Ingredients: eggs, flour, butter, sugar, baking powder, salt, egg white, flour, flour blend, sugar, brown sugar. Instructions: Combine egg whites, flour, baking powder, salt, eggs, sugar, baking powder. Mix well. Beat with a whisk until light and fluffy. Spread with a greased 9x13-inch round cookie sheet. Cover tightly and refrigerate overnight.\n",
      "\n",
      "  Average training loss: 2.09\n",
      "  Training epoch took: 0:00:50\n",
      "\n",
      "Running Validation...\n",
      "  Validation Loss: 2.03\n",
      "  Validation took: 0:00:01\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:02:38 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "total_t0 = time.time()\n",
    "\n",
    "training_stats = []\n",
    "\n",
    "for epoch_i in range(0, epochs):\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()  # `train` just changes the *mode* (train vs. eval), it doesn't *perform* the training.\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):     # step from enumerate() = number of batches\n",
    "\n",
    "        b_input_ids = batch[0].to(device)   # tokens (of multiple documents in a batch)\n",
    "        b_labels    = batch[0].to(device)\n",
    "        b_masks     = batch[1].to(device)   # mask of [1] for a real word, [0] for a pad\n",
    "\n",
    "        model.zero_grad()\n",
    "        # loss = model(X.to(device), attention_mask=a.to(device), labels=X.to(device)).loss\n",
    "        outputs = model(  input_ids = b_input_ids,\n",
    "                          labels = b_labels,\n",
    "                          attention_mask = b_masks,\n",
    "                          token_type_ids = None\n",
    "                        )\n",
    "\n",
    "        loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_train_loss += batch_loss\n",
    "\n",
    "        # Get sample every x batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}. Loss: {:>5,}.   Elapsed: {:}.'.format(step, len(train_dataloader), batch_loss, elapsed))\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            sample_output = infer(\"eggs, flour, butter, sugar\")\n",
    "            print(sample_output)\n",
    "\n",
    "            # `train` just changes the *mode* (train vs. eval), it doesn't *perform* the training.\n",
    "            model.train()\n",
    "\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_labels = batch[0].to(device)\n",
    "        b_masks = batch[1].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs  = model(input_ids = b_input_ids,\n",
    "                             attention_mask = b_masks,\n",
    "                             labels = b_labels)\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "        batch_loss = loss.item()\n",
    "        total_eval_loss += batch_loss\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Valid. Loss</th>\n",
       "      <th>Training Time</th>\n",
       "      <th>Validation Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.357306</td>\n",
       "      <td>2.161529</td>\n",
       "      <td>0:00:54</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.173984</td>\n",
       "      <td>2.040881</td>\n",
       "      <td>0:00:51</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.093194</td>\n",
       "      <td>2.030801</td>\n",
       "      <td>0:00:50</td>\n",
       "      <td>0:00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Training Loss  Valid. Loss Training Time Validation Time\n",
       "epoch                                                          \n",
       "1           6.357306     2.161529       0:00:54         0:00:01\n",
       "2           2.173984     2.040881       0:00:51         0:00:01\n",
       "3           2.093194     2.030801       0:00:50         0:00:01"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame from our training statistics.\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "\n",
    "# Use the 'epoch' as the row index.\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model\\\\tokenizer_config.json',\n",
       " './model\\\\special_tokens_map.json',\n",
       " './model\\\\vocab.json',\n",
       " './model\\\\merges.txt',\n",
       " './model\\\\added_tokens.json',\n",
       " './model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_save_path = './model'\n",
    "\n",
    "print(\"Saving model to %s\" % model_save_path)\n",
    "\n",
    "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
    "# They can then be reloaded using `from_pretrained()`\n",
    "# model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50260, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50260, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(model_save_path)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients: eggs, mushroom, butter, sugar, salt, pepper, sugar, black pepper, eggs, sugar. Instructions: Combine eggs and sugar, salt, pepper, sugar, black pepper, eggs. Pour over eggs and sugar mixture. Gradually add eggs to mixture. Bake at 350 degrees for 20 to 25 minutes. Allow to cool before slicing into squares.\n"
     ]
    }
   ],
   "source": [
    "# model = GPT2LMHeadModel.from_pretrained(model_save_path)\n",
    "# tokenizer = GPT2TokenizerFast.from_pretrained(model_save_path)\n",
    "# model.to(device)\n",
    "print(infer(\"eggs, mushroom, butter, sugar\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ingredients: onion, garlic, chicken breast, salt, pepper, pepper, fresh thyme, onion, parsley, oregano, fresh parsley. Instructions: Cut onions into 2 halves and mix in garlic cloves and salt. Cover with plastic wrap and cook in a low heat until translucent, about 20 minutes. Drain, and season with salt and pepper. Garnish with parsley. Makes 2 large glasses.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(\"onion, garlic, chicken breast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients: avocado, lime, salt, fresh cilantro, pepper, coriander, garlic powder, sugar, and salt. Instructions: Combine avocado and lime, salt, fresh cilantro, garlic powder, sugar, salt. Mix well. Add cilantro to lime mixture. Cover and refrigerate overnight.\n"
     ]
    }
   ],
   "source": [
    "print(infer(\"avocado, lime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingredients: beef, salt, pepper, garlic powder, ginger, dried oregano, thyme, salt. Instructions: Put beef, garlic, oregano, thyme, salt, pepper and oregano in a blender or food processor. Blend until smooth. Add water, stirring often. Add onion, oregano, basil and thyme. Blend again. Blend for another minute, until the mixture is slightly thickened. Add broth and blend for another minute. Add oregano and stir until smooth. Combine mixture thoroughly and refrigerate. Cover and chill at room temperature until ready to serve. Instructions: Melt beef, butter, garlic, oregano, thyme and parsley in a large saucepan over medium heat. Add onion, salt and pepper and stir until smooth. Add broth and simmer for another minute, stirring occasionally. Add broth, stirring constantly, until mixture is tender. Bring to a\n"
     ]
    }
   ],
   "source": [
    "print(infer(\"beef, salt, pepper\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:577: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.08185564546079e-232\n"
     ]
    }
   ],
   "source": [
    "# Using BLEU score to compare the real sentences with the generated ones\n",
    "\n",
    "scores=[]\n",
    "\n",
    "for i in range(10):\n",
    "    ingredients = val_dataset[i][2]\n",
    "    reference = val_dataset[i][3]\n",
    "    candidate = infer(ingredients)\n",
    "    scores.append(sentence_bleu(reference, candidate))\n",
    "\n",
    "print(statistics.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
