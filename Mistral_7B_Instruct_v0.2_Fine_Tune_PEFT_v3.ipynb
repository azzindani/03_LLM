{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-15T12:58:41.939300Z","iopub.execute_input":"2024-12-15T12:58:41.939635Z","iopub.status.idle":"2024-12-15T12:59:45.216093Z","shell.execute_reply.started":"2024-12-15T12:58:41.939607Z","shell.execute_reply":"2024-12-15T12:59:45.214852Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-12-15T12:59:45.218361Z","iopub.execute_input":"2024-12-15T12:59:45.219095Z","iopub.status.idle":"2024-12-15T13:00:03.976038Z","shell.execute_reply.started":"2024-12-15T12:59:45.219048Z","shell.execute_reply":"2024-12-15T13:00:03.975107Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-12-15T13:00:03.977008Z","iopub.execute_input":"2024-12-15T13:00:03.977256Z","iopub.status.idle":"2024-12-15T13:00:03.984931Z","shell.execute_reply.started":"2024-12-15T13:00:03.977231Z","shell.execute_reply":"2024-12-15T13:00:03.983117Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"model_name = 'unsloth/mistral-7b-instruct-v0.2'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-15T13:00:03.987631Z","iopub.execute_input":"2024-12-15T13:00:03.988096Z","iopub.status.idle":"2024-12-15T13:00:04.019834Z","shell.execute_reply.started":"2024-12-15T13:00:03.988042Z","shell.execute_reply":"2024-12-15T13:00:04.019111Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-12-15T13:00:04.020927Z","iopub.execute_input":"2024-12-15T13:00:04.021187Z","iopub.status.idle":"2024-12-15T13:00:04.032160Z","shell.execute_reply.started":"2024-12-15T13:00:04.021162Z","shell.execute_reply":"2024-12-15T13:00:04.031462Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:00:04.033154Z","iopub.execute_input":"2024-12-15T13:00:04.033501Z","iopub.status.idle":"2024-12-15T13:06:32.341257Z","shell.execute_reply.started":"2024-12-15T13:00:04.033460Z","shell.execute_reply":"2024-12-15T13:06:32.340507Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/722 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e031c275ae94e62b98851ef67b228c6"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0651051cd75a49de8a745d397898774d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c39b98fe6d3b40e181e5d0c7e7b081a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd89d74af97543868e1e9acee015b267"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d3daa2c0284513a2b0f41a5b359637"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96ab3922c1844b17ab40e8f2a2726099"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20da82c2629a4c39a83eae9ac4bbea65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/155 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c92a96e2d33e424d96f70721a9ac614a"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:32.342251Z","iopub.execute_input":"2024-12-15T13:06:32.342510Z","iopub.status.idle":"2024-12-15T13:06:32.350031Z","shell.execute_reply.started":"2024-12-15T13:06:32.342483Z","shell.execute_reply":"2024-12-15T13:06:32.349225Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 3752071168\nTrainable parameters : 262410240\nTrainable percentage: 6.99%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:32.351037Z","iopub.execute_input":"2024-12-15T13:06:32.351280Z","iopub.status.idle":"2024-12-15T13:06:33.791053Z","shell.execute_reply.started":"2024-12-15T13:06:32.351254Z","shell.execute_reply":"2024-12-15T13:06:33.790361Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.13k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea2e70749f1945b3bcb4cdd7eb1c5088"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e48edf4e6fbf4674a61ae7515d3d5470"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4ef580b64084d97acd9593ff7cf2926"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"622e4aa1cb694fec84749e55c2743dc0"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"dataset_name = 'microsoft/orca-math-word-problems-200k'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:33.792159Z","iopub.execute_input":"2024-12-15T13:06:33.792525Z","iopub.status.idle":"2024-12-15T13:06:33.796788Z","shell.execute_reply.started":"2024-12-15T13:06:33.792486Z","shell.execute_reply":"2024-12-15T13:06:33.795847Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 384","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:33.799864Z","iopub.execute_input":"2024-12-15T13:06:33.800356Z","iopub.status.idle":"2024-12-15T13:06:33.806097Z","shell.execute_reply.started":"2024-12-15T13:06:33.800319Z","shell.execute_reply":"2024-12-15T13:06:33.805231Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:33.807186Z","iopub.execute_input":"2024-12-15T13:06:33.807438Z","iopub.status.idle":"2024-12-15T13:06:38.524447Z","shell.execute_reply.started":"2024-12-15T13:06:33.807413Z","shell.execute_reply":"2024-12-15T13:06:38.523717Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.91k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fff8cc29dd34a2193724db732a1927e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/84.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3479aeab74f54fdbb366a902b4cd45de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/200035 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e8c1e3134d64311b0ef620fdd7a6db0"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'answer'],\n    num_rows: 200035\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:38.525666Z","iopub.execute_input":"2024-12-15T13:06:38.526330Z","iopub.status.idle":"2024-12-15T13:06:38.532128Z","shell.execute_reply.started":"2024-12-15T13:06:38.526285Z","shell.execute_reply":"2024-12-15T13:06:38.531157Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:38.533054Z","iopub.execute_input":"2024-12-15T13:06:38.533305Z","iopub.status.idle":"2024-12-15T13:06:38.565052Z","shell.execute_reply.started":"2024-12-15T13:06:38.533279Z","shell.execute_reply":"2024-12-15T13:06:38.564240Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:38.566056Z","iopub.execute_input":"2024-12-15T13:06:38.566326Z","iopub.status.idle":"2024-12-15T13:06:38.573661Z","shell.execute_reply.started":"2024-12-15T13:06:38.566298Z","shell.execute_reply":"2024-12-15T13:06:38.572795Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:38.574791Z","iopub.execute_input":"2024-12-15T13:06:38.575046Z","iopub.status.idle":"2024-12-15T13:06:38.583568Z","shell.execute_reply.started":"2024-12-15T13:06:38.575021Z","shell.execute_reply":"2024-12-15T13:06:38.582590Z"}},"outputs":[{"name":"stdout","text":"['question', 'answer']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:38.584596Z","iopub.execute_input":"2024-12-15T13:06:38.584981Z","iopub.status.idle":"2024-12-15T13:06:38.592527Z","shell.execute_reply.started":"2024-12-15T13:06:38.584943Z","shell.execute_reply":"2024-12-15T13:06:38.591800Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  input = examples['question']\n  output = examples['answer']\n  \n  text = prompt_format.format(input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"id":"0wXJNFBWWNYP","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:38.593651Z","iopub.execute_input":"2024-12-15T13:06:38.594500Z","iopub.status.idle":"2024-12-15T13:06:38.602102Z","shell.execute_reply.started":"2024-12-15T13:06:38.594457Z","shell.execute_reply":"2024-12-15T13:06:38.601284Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:38.603246Z","iopub.execute_input":"2024-12-15T13:06:38.603906Z","iopub.status.idle":"2024-12-15T13:06:39.057780Z","shell.execute_reply.started":"2024-12-15T13:06:38.603856Z","shell.execute_reply":"2024-12-15T13:06:39.056676Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a318d5c9ae254bc68aaaf187bcd50c38"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"Kidf8H5zefDC","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:39.059172Z","iopub.execute_input":"2024-12-15T13:06:39.059898Z","iopub.status.idle":"2024-12-15T13:06:39.064719Z","shell.execute_reply.started":"2024-12-15T13:06:39.059853Z","shell.execute_reply":"2024-12-15T13:06:39.063796Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.</s>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:39.066022Z","iopub.execute_input":"2024-12-15T13:06:39.066376Z","iopub.status.idle":"2024-12-15T13:06:39.085478Z","shell.execute_reply.started":"2024-12-15T13:06:39.066337Z","shell.execute_reply":"2024-12-15T13:06:39.084613Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:39.086445Z","iopub.execute_input":"2024-12-15T13:06:39.086722Z","iopub.status.idle":"2024-12-15T13:06:46.787494Z","shell.execute_reply.started":"2024-12-15T13:06:39.086693Z","shell.execute_reply":"2024-12-15T13:06:46.786605Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c638647c0e240cea76f4cfac29f4d89"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:46.788853Z","iopub.execute_input":"2024-12-15T13:06:46.789550Z","iopub.status.idle":"2024-12-15T13:06:46.796613Z","shell.execute_reply.started":"2024-12-15T13:06:46.789506Z","shell.execute_reply":"2024-12-15T13:06:46.795601Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.</s>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:46.797659Z","iopub.execute_input":"2024-12-15T13:06:46.798058Z","iopub.status.idle":"2024-12-15T13:06:47.751171Z","shell.execute_reply.started":"2024-12-15T13:06:46.798015Z","shell.execute_reply":"2024-12-15T13:06:47.750056Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:47.752384Z","iopub.execute_input":"2024-12-15T13:06:47.752693Z","iopub.status.idle":"2024-12-15T13:06:47.766233Z","shell.execute_reply.started":"2024-12-15T13:06:47.752664Z","shell.execute_reply":"2024-12-15T13:06:47.765441Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:47.767177Z","iopub.execute_input":"2024-12-15T13:06:47.767495Z","iopub.status.idle":"2024-12-15T13:06:47.790336Z","shell.execute_reply.started":"2024-12-15T13:06:47.767466Z","shell.execute_reply":"2024-12-15T13:06:47.789509Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  ### Question:\\nThere is a two-digit natural nu...   \n1  ### Question:\\nIn a big box, there are marbles...   \n2  ### Question:\\nAdam goes to a small school, wh...   \n3  ### Question:\\nLisa is looking to attempt a Wo...   \n4  ### Question:\\nThere is a rectangular-shaped p...   \n\n                                           input_ids  \\\n0  [1, 774, 22478, 28747, 13, 5816, 349, 264, 989...   \n1  [1, 774, 22478, 28747, 13, 657, 264, 2032, 389...   \n2  [1, 774, 22478, 28747, 13, 3261, 314, 4859, 29...   \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n4  [1, 774, 22478, 28747, 13, 5816, 349, 264, 971...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nThere is a two-digit natural nu...</td>\n      <td>[1, 774, 22478, 28747, 13, 5816, 349, 264, 989...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nIn a big box, there are marbles...</td>\n      <td>[1, 774, 22478, 28747, 13, 657, 264, 2032, 389...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nAdam goes to a small school, wh...</td>\n      <td>[1, 774, 22478, 28747, 13, 3261, 314, 4859, 29...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nLisa is looking to attempt a Wo...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere is a rectangular-shaped p...</td>\n      <td>[1, 774, 22478, 28747, 13, 5816, 349, 264, 971...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:47.791322Z","iopub.execute_input":"2024-12-15T13:06:47.791587Z","iopub.status.idle":"2024-12-15T13:06:47.796847Z","shell.execute_reply.started":"2024-12-15T13:06:47.791555Z","shell.execute_reply":"2024-12-15T13:06:47.795893Z"}},"outputs":[{"name":"stdout","text":"### Question:\nThere is a two-digit natural number whose tens place is 3. Let A and B be the quotient of this number by 10 and the remainder of division by 10, respectively. If B multiplied by 10 plus A is 9 less than A multiplied by 10 plus B, what is the first number?\n### Answer:\nLet's denote the two-digit number as \\( XY \\), where \\( X \\) is the digit in the tens place and \\( Y \\) is the digit in the ones place. Since the tens place is 3, we have \\( X = 3 \\).\n\nAccording to the problem, \\( A \\) is the quotient of the number by 10, and \\( B \\) is the remainder of the division by 10. Therefore, \\( A = X = 3 \\) and \\( B = Y \\).\n\nThe problem states that \\( B \\times 10 + A \\) is 9 less than \\( A \\times 10 + B \\). This can be written as an equation:\n\n\\[ B \\times 10 + A = A \\times 10 + B - 9 \\]\n\nSubstituting \\( A \\) and \\( B \\) with \\( 3 \\) and \\( Y \\), respectively, we get:\n\n\\[ Y \\times 10 + 3 = 3 \\times 10 + Y - 9 \\]\n\nSimplifying the equation:\n\n\\[ 10Y + 3 = 30 + Y - 9 \\]\n\n\\[ 10Y + 3 = Y + 21 \\]\n\nSubtract \\( Y \\) from both sides:\n\n\\[ 9Y + 3 = 21 \\]\n\nSubtract 3 from both sides:\n\n\\[ 9Y = 18 \\]\n\nDivide both sides by 9:\n\n\\[ Y = 2 \\]\n\nSo the ones place digit is 2. Since we already know the tens place digit is 3, the two-digit number is \\( 32 \\).</s>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:47.797877Z","iopub.execute_input":"2024-12-15T13:06:47.798123Z","iopub.status.idle":"2024-12-15T13:06:47.808589Z","shell.execute_reply.started":"2024-12-15T13:06:47.798098Z","shell.execute_reply":"2024-12-15T13:06:47.807809Z"}},"outputs":[{"name":"stdout","text":"[1, 774, 22478, 28747, 13, 5816, 349, 264, 989, 28733, 7845, 279, 4229, 1474, 4636, 19391, 1633, 349, 28705, 28770, 28723, 3169, 330, 304, 365, 347, 272, 17528, 722, 302, 456, 1474, 486, 28705, 28740, 28734, 304, 272, 23317, 302, 9652, 486, 28705, 28740, 28734, 28725, 8628, 28723, 1047, 365, 6079, 3002, 486, 28705, 28740, 28734, 3285, 330, 349, 28705, 28774, 2108, 821, 330, 6079, 3002, 486, 28705, 28740, 28734, 3285, 365, 28725, 767, 349, 272, 907, 1474, 28804, 13, 27332, 26307, 28747, 13, 8779, 28742, 28713, 14543, 272, 989, 28733, 7845, 279, 1474, 390, 18823, 1500, 28802, 414, 557, 970, 18823, 1500, 414, 28731, 349, 272, 21656, 297, 272, 19391, 1633, 304, 18823, 627, 414, 28731, 349, 272, 21656, 297, 272, 4413, 1633, 28723, 4577, 272, 19391, 1633, 349, 28705, 28770, 28725, 478, 506, 18823, 1500, 327, 28705, 28770, 414, 609, 13, 13, 5604, 3059, 298, 272, 2700, 28725, 18823, 330, 414, 28731, 349, 272, 17528, 722, 302, 272, 1474, 486, 28705, 28740, 28734, 28725, 304, 18823, 365, 414, 28731, 349, 272, 23317, 302, 272, 9652, 486, 28705, 28740, 28734, 28723, 8469, 28725, 18823, 330, 327, 1500, 327, 28705, 28770, 414, 28731, 304, 18823, 365, 327, 627, 414, 609, 13, 13, 1014, 2700, 4605, 369, 18823, 365, 414, 4593, 28705, 28740, 28734, 648, 330, 414, 28731, 349, 28705, 28774, 2108, 821, 18823, 330, 414, 4593, 28705, 28740, 28734, 648, 365, 414, 609, 851, 541, 347, 4241, 390, 396, 8777, 28747, 13, 13, 16927, 365, 414, 4593, 28705, 28740, 28734, 648, 330, 327, 330, 414, 4593, 28705, 28740, 28734, 648, 365, 387, 28705, 28774, 8425, 13, 13, 3540, 303, 3288, 288, 18823, 330, 414, 28731, 304, 18823, 365, 414, 28731, 395, 18823, 28705, 28770, 414, 28731, 304, 18823, 627, 414, 557, 8628, 28725, 478, 625, 28747, 13, 13, 16927, 627, 414, 4593, 28705, 28740, 28734, 648, 28705, 28770, 327, 28705, 28770, 414, 4593, 28705, 28740, 28734, 648, 627, 387, 28705, 28774, 8425, 13, 13, 7554, 452, 6219, 272, 8777, 28747, 13, 13, 16927, 28705, 28740, 28734, 28802, 648, 28705, 28770, 327, 28705, 28770, 28734, 648, 627, 387, 28705, 28774, 8425, 13, 13, 16927, 28705, 28740, 28734, 28802, 648, 28705, 28770, 327, 627, 648, 28705, 28750, 28740, 8425, 13, 13, 3540, 2107, 18823, 627, 414, 28731, 477, 1560, 8480, 28747, 13, 13, 16927, 28705, 28774]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:47.814143Z","iopub.execute_input":"2024-12-15T13:06:47.814705Z","iopub.status.idle":"2024-12-15T13:06:47.819960Z","shell.execute_reply.started":"2024-12-15T13:06:47.814675Z","shell.execute_reply":"2024-12-15T13:06:47.819205Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:47.820967Z","iopub.execute_input":"2024-12-15T13:06:47.821240Z","iopub.status.idle":"2024-12-15T13:06:47.833122Z","shell.execute_reply.started":"2024-12-15T13:06:47.821213Z","shell.execute_reply":"2024-12-15T13:06:47.832276Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:47.834053Z","iopub.execute_input":"2024-12-15T13:06:47.834318Z","iopub.status.idle":"2024-12-15T13:06:47.847501Z","shell.execute_reply.started":"2024-12-15T13:06:47.834284Z","shell.execute_reply":"2024-12-15T13:06:47.846667Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:47.848507Z","iopub.execute_input":"2024-12-15T13:06:47.848802Z","iopub.status.idle":"2024-12-15T13:06:47.856146Z","shell.execute_reply.started":"2024-12-15T13:06:47.848752Z","shell.execute_reply":"2024-12-15T13:06:47.855233Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 32\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n\n#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:47.857125Z","iopub.execute_input":"2024-12-15T13:06:47.857383Z","iopub.status.idle":"2024-12-15T13:06:47.867327Z","shell.execute_reply.started":"2024-12-15T13:06:47.857358Z","shell.execute_reply":"2024-12-15T13:06:47.866448Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:47.868331Z","iopub.execute_input":"2024-12-15T13:06:47.869220Z","iopub.status.idle":"2024-12-15T13:06:49.072470Z","shell.execute_reply.started":"2024-12-15T13:06:47.869192Z","shell.execute_reply":"2024-12-15T13:06:49.071518Z"}},"outputs":[{"name":"stdout","text":"trainable params: 83,886,080 || all params: 7,325,618,176 || trainable%: 1.1451\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:49.073560Z","iopub.execute_input":"2024-12-15T13:06:49.073873Z","iopub.status.idle":"2024-12-15T13:06:49.089004Z","shell.execute_reply.started":"2024-12-15T13:06:49.073844Z","shell.execute_reply":"2024-12-15T13:06:49.088101Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 3835957248\nTrainable parameters : 83886080\nTrainable percentage: 2.19%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:49.090056Z","iopub.execute_input":"2024-12-15T13:06:49.090338Z","iopub.status.idle":"2024-12-15T13:06:49.097421Z","shell.execute_reply.started":"2024-12-15T13:06:49.090306Z","shell.execute_reply":"2024-12-15T13:06:49.096698Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 1\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:49.098488Z","iopub.execute_input":"2024-12-15T13:06:49.098879Z","iopub.status.idle":"2024-12-15T13:06:49.143620Z","shell.execute_reply.started":"2024-12-15T13:06:49.098846Z","shell.execute_reply":"2024-12-15T13:06:49.142794Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec15_13-06-49_5022ae46d8ac,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:49.144639Z","iopub.execute_input":"2024-12-15T13:06:49.144913Z","iopub.status.idle":"2024-12-15T13:06:51.048859Z","shell.execute_reply.started":"2024-12-15T13:06:49.144886Z","shell.execute_reply":"2024-12-15T13:06:51.048128Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7fe5c08b4040>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T13:06:51.049798Z","iopub.execute_input":"2024-12-15T13:06:51.050061Z","iopub.status.idle":"2024-12-15T14:11:36.434679Z","shell.execute_reply.started":"2024-12-15T13:06:51.050036Z","shell.execute_reply":"2024-12-15T14:11:36.433803Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 1\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 83,886,080\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241215_130718-rhxtr5oq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/rhxtr5oq' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/rhxtr5oq' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/rhxtr5oq</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 1:04:06, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.068100</td>\n      <td>0.961343</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.971300</td>\n      <td>0.831523</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.868000</td>\n      <td>0.744893</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.794000</td>\n      <td>0.695028</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.708700</td>\n      <td>0.667260</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.687200</td>\n      <td>0.654017</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.681000</td>\n      <td>0.642724</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.714400</td>\n      <td>0.637224</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.675200</td>\n      <td>0.635199</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.657900</td>\n      <td>0.634577</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=0.7825645351409912, metrics={'train_runtime': 3884.956, 'train_samples_per_second': 0.206, 'train_steps_per_second': 0.051, 'total_flos': 1.34156063342592e+16, 'train_loss': 0.7825645351409912, 'epoch': 0.08888888888888889})"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:11:36.435899Z","iopub.execute_input":"2024-12-15T14:11:36.436189Z","iopub.status.idle":"2024-12-15T14:14:48.338353Z","shell.execute_reply.started":"2024-12-15T14:11:36.436161Z","shell.execute_reply":"2024-12-15T14:14:48.337428Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 03:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.6345770359039307, 'eval_runtime': 191.8887, 'eval_samples_per_second': 1.042, 'eval_steps_per_second': 0.261, 'epoch': 0.08888888888888889}\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:14:48.339645Z","iopub.execute_input":"2024-12-15T14:14:48.339958Z","iopub.status.idle":"2024-12-15T14:14:50.266713Z","shell.execute_reply.started":"2024-12-15T14:14:48.339928Z","shell.execute_reply":"2024-12-15T14:14:50.265799Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--mistral-7b-instruct-v0.2/snapshots/5d88c8e8dbf7446e665a3633b2f047689f4e6547/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"unsloth/Mistral-7B-Instruct-v0.2\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--mistral-7b-instruct-v0.2/snapshots/5d88c8e8dbf7446e665a3633b2f047689f4e6547/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"unsloth/Mistral-7B-Instruct-v0.2\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:14:50.267854Z","iopub.execute_input":"2024-12-15T14:14:50.268113Z","iopub.status.idle":"2024-12-15T14:14:50.375823Z","shell.execute_reply.started":"2024-12-15T14:14:50.268087Z","shell.execute_reply":"2024-12-15T14:14:50.374746Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:14:50.377000Z","iopub.execute_input":"2024-12-15T14:14:50.377267Z","iopub.status.idle":"2024-12-15T14:14:50.388276Z","shell.execute_reply.started":"2024-12-15T14:14:50.377239Z","shell.execute_reply":"2024-12-15T14:14:50.387563Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:14:50.389387Z","iopub.execute_input":"2024-12-15T14:14:50.389676Z","iopub.status.idle":"2024-12-15T14:14:51.800999Z","shell.execute_reply.started":"2024-12-15T14:14:50.389651Z","shell.execute_reply":"2024-12-15T14:14:51.800032Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:14:51.802278Z","iopub.execute_input":"2024-12-15T14:14:51.802652Z","iopub.status.idle":"2024-12-15T14:15:09.484611Z","shell.execute_reply.started":"2024-12-15T14:14:51.802608Z","shell.execute_reply":"2024-12-15T14:15:09.483863Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--mistral-7b-instruct-v0.2/snapshots/5d88c8e8dbf7446e665a3633b2f047689f4e6547/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"unsloth/mistral-7b-instruct-v0.2\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 0,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 32000\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--mistral-7b-instruct-v0.2/snapshots/5d88c8e8dbf7446e665a3633b2f047689f4e6547/model.safetensors.index.json\nInstantiating MistralForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 0\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1ebc92376eaa49ba8d56dd871120784f"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing MistralForCausalLM.\n\nAll the weights of MistralForCausalLM were initialized from the model checkpoint at unsloth/mistral-7b-instruct-v0.2.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--mistral-7b-instruct-v0.2/snapshots/5d88c8e8dbf7446e665a3633b2f047689f4e6547/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 2,\n  \"max_length\": 32768,\n  \"pad_token_id\": 0\n}\n\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:15:09.485666Z","iopub.execute_input":"2024-12-15T14:15:09.485955Z","iopub.status.idle":"2024-12-15T14:15:09.495450Z","shell.execute_reply.started":"2024-12-15T14:15:09.485928Z","shell.execute_reply":"2024-12-15T14:15:09.494776Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 3752071168\nTrainable parameters : 262410240\nTrainable percentage: 6.99%\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:15:09.496516Z","iopub.execute_input":"2024-12-15T14:15:09.496802Z","iopub.status.idle":"2024-12-15T14:15:09.523425Z","shell.execute_reply.started":"2024-12-15T14:15:09.496770Z","shell.execute_reply":"2024-12-15T14:15:09.522608Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=14336, out_features=32, bias=False)\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:15:09.524343Z","iopub.execute_input":"2024-12-15T14:15:09.524591Z","iopub.status.idle":"2024-12-15T14:15:09.552873Z","shell.execute_reply.started":"2024-12-15T14:15:09.524564Z","shell.execute_reply":"2024-12-15T14:15:09.552010Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 3919843328\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## 14 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:15:09.553958Z","iopub.execute_input":"2024-12-15T14:15:09.554207Z","iopub.status.idle":"2024-12-15T14:15:09.560230Z","shell.execute_reply.started":"2024-12-15T14:15:09.554183Z","shell.execute_reply":"2024-12-15T14:15:09.559409Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def post_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:15:09.561162Z","iopub.execute_input":"2024-12-15T14:15:09.561422Z","iopub.status.idle":"2024-12-15T14:15:09.571041Z","shell.execute_reply.started":"2024-12-15T14:15:09.561396Z","shell.execute_reply":"2024-12-15T14:15:09.570228Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:15:09.572068Z","iopub.execute_input":"2024-12-15T14:15:09.572309Z","iopub.status.idle":"2024-12-15T14:15:09.580435Z","shell.execute_reply.started":"2024-12-15T14:15:09.572285Z","shell.execute_reply":"2024-12-15T14:15:09.579674Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:18:21.317783Z","iopub.execute_input":"2024-12-15T14:18:21.318045Z","iopub.status.idle":"2024-12-15T14:19:51.483525Z","shell.execute_reply.started":"2024-12-15T14:18:21.318019Z","shell.execute_reply":"2024-12-15T14:19:51.482585Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    3752071168                     |                     3919843328                    \n================================================== | ==================================================\n### Question: There is the expression 691-6A7=4.    |  ### Question: There is the expression 691-6A7=4.  \nFind the number that goes into A ### Answer: To     |  Find the number that goes into A ### Answer: To   \nfind the number that goes into A, we need to        |  find the number that goes into A, we need to      \nsubtract 6A7 from 691 and find the difference.      |  subtract 6A7 from 691 and find the difference.    \nThen, we can determine the number that goes into A  |  Then, we can determine the number that goes into A\nby finding the difference between the numbers in    |  by finding the difference between the numbers in  \ndecimal form and identifying the difference         |  decimal form and identifying the difference       \nbetween the digits in the seventh place.  First,    |  between the digits in the seventh place.  First,  \nlet's subtract 6A7 from 691:  691 - 6A7 = 624       |  let's subtract 6A7 from 691:  691 - 6A7 = 624     \nNow, let's find the difference between the numbers  |  Now, let's find the difference between the numbers\nin decimal form:  691 = 6 * 10^2 + 9 * 10^1 + 1 *   |  in decimal form:  691 = 6 * 10^2 + 9 * 10^1 + 1 * \n10^0 6A7 = 6 * 10^2 + A * 10^1 + 7 * 10^0           |  10^0 6A7 = 6 * 10^2 + A * 10^1 + 7 * 10^0         \nSubtracting the right sides:  (6 * 10^2 + 9 * 10^1  |  Subtracting the right sides:  (6 * 10^2 + 9 * 10^1\n+ 1 * 10^0) - (6 * 10^2 + A * 10^1 + 7 * 10^0)      |  + 1 * 10^0) - (6 * 10^2 + A * 10^1 + 7 * 10^0)    \nSimplifying:  6 * 10^2 + (9 * 10^1 - A * 10^1 - 7   |  Simplifying:  6 * 10^2 + (9 * 10^1 - A * 10^1 - 7 \n* 10^0)  Since the difference is 4, we can set up   |  * 10^0)  Since the difference is 4, we can set up \nthe equation:  6 * 10^2 + (9 * 10^1 - A * 10^1 - 7  |  the equation:  6 * 10^2 + (9 * 10^1 - A * 10^1 - 7\n* 10^0) = 6 * 10^2 + 4  Subtracting 6 * 10^2 from   |  * 10^0) = 6 * 10^2 + 4  Subtracting 6 * 10^2 from \nboth sides:  9 * 10^1 - A * 10^1 - 7 * 10^0 = 4     |  both sides:  9 * 10^1 - A * 10^1 - 7 * 10^0 = 4   \nDividing both sides by 10:  90 - A - 7 = 0.4        |  Dividing both sides by 10:  90 - A - 7 = 0.4      \nSubtracting 0.4 from both sides:  90 - A = 0.6      |  Subtracting 0.4 from both sides:  90 - A = 0.6    \nDividing both sides by 0.6:  A = 150 - (90 / 0.6)   |  Dividing both sides by 0.6:  A = 150 - (90 / 0.6) \nSince 90 / 0.6 is not a real number, this equation  |  However, this equation is undefined because 0.6   \ndoes not have a solution. Therefore, it is not      |  cannot be divided by 90. Therefore, it is not     \npossible to determine the number that goes into A   |  possible to find a unique value for A that results\nbased on the given expression.                      |  in the difference 691 - 6A7 = 4.                  \n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:31:36.505834Z","iopub.execute_input":"2024-12-15T14:31:36.506208Z","iopub.status.idle":"2024-12-15T14:32:21.992943Z","shell.execute_reply.started":"2024-12-15T14:31:36.506178Z","shell.execute_reply":"2024-12-15T14:32:21.992078Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    3752071168                     |                     3919843328                    \n================================================== | ==================================================\n### Question: There are 40 students in a class. If  |  ### Question: There are 40 students in a class. If\na certain fraction of students are absent, 3/4 of   |  a certain fraction of students are absent, 3/4 of \nthe students who are present are in the classroom,  |  the students who are present are in the classroom,\nand the rest are in the canteen. There are 9        |  and the rest are in the canteen. There are 9      \nstudents in the canteen. What fraction of students  |  students in the canteen. What fraction of students\nare absent? ### Answer: First, let's find out how   |  are absent? ### Answer: First, let's find out how \nmany students are present in the classroom. We      |  many students are present in the classroom. We    \nknow that 3/4 of the students who are present are   |  know that 3/4 of the students who are present are \nin the classroom, and there are 40 students in      |  in the classroom, and there are 40 students in    \ntotal. Let x be the number of students who are      |  total. Let x be the number of students who are    \npresent. Then, 3x/4 are in the classroom, and (x -  |  present. Then, 3x/4 are in the classroom, and (x -\n9) are in the canteen. We know that there are 9     |  9) are in the canteen. We know that there are 9   \nstudents in the canteen, so we can set up the       |  students in the canteen, so we can set up the     \nequation: 3x/4 + (x - 9) = 40 To solve this         |  equation: 3x/4 + (x - 9) = 40 To solve this       \nequation, we can first combine the terms with x:    |  equation, we can first combine the terms with x:  \n3x/4 + x - 9 = 40 4x/4 - 9 = 40 - 3x/4 Multiplying  |  3x/4 + x - 9 = 40 4x/4 - 9 = 40 - 3x/4 Multiplying\nboth sides by 4/4 to clear the fraction, we get: x  |  both sides by 4/4 to clear the fraction, we get: x\n= 48 So there are 48 students who are present.      |  = 48 So there are 48 students present in the      \nSince there are 40 students in total, there must    |  classroom. Since there are 40 students in total,  \nbe 8 students who are absent. Therefore, the        |  there must be 12 students absent. To find the     \nfraction of students who are absent is 8/40 = 1/5.  |  fraction of students who are absent, we can divide\n                                                    |  the number of absent students by the total number \n                                                    |  of students: 12/40 = 3/10 So 3/10 of the students \n                                                    |  are absent.                                       \n","output_type":"stream"}],"execution_count":73},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:36:39.339849Z","iopub.execute_input":"2024-12-15T14:36:39.340718Z","iopub.status.idle":"2024-12-15T14:37:39.322099Z","shell.execute_reply.started":"2024-12-15T14:36:39.340682Z","shell.execute_reply":"2024-12-15T14:37:39.321134Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    3752071168                     |                     3919843328                    \n================================================== | ==================================================\n### Question: Mr. Callen bought 10 paintings at     |  ### Question: Mr. Callen bought 10 paintings at   \n$40 each and 8 wooden toys at $20 each from the     |  $40 each and 8 wooden toys at $20 each from the   \ncrafts store to resell at a profit. However, when   |  crafts store to resell at a profit. However, when \nhe sold the items, the selling price of a painting  |  he sold the items, the selling price of a painting\nwas 10% less and the selling price of a hat 15%     |  was 10% less and the selling price of a hat 15%   \nless. Calculate the total loss Mr. Callen made      |  less. Calculate the total loss Mr. Callen made    \nfrom the sale of the items. ### Answer: First,      |  from the sale of the items. ### Answer: First,    \nlet's calculate the cost and selling price of the   |  let's calculate the cost and selling price of the \npaintings and wooden toys that Mr. Callen bought.   |  paintings and wooden toys that Mr. Callen bought. \nCost of 10 paintings = 10 * $40 = $400 Cost of 8    |  Cost of 10 paintings = 10 * $40 = $400 Cost of 8  \nwooden toys = 8 * $20 = $160  Total cost of items   |  wooden toys = 8 * $20 = $160  Total cost of items \n= $400 + $160 = $560  Now, let's calculate the      |  = $400 + $160 = $560  Now, let's calculate the    \nselling price of each painting and wooden toy       |  selling price of each painting and wooden toy     \nafter the price reduction.  Selling price of a      |  after the price reduction.  Selling price of a    \npainting = $40 * 0.9 = $36 Selling price of a       |  painting = $40 * 0.9 = $36 Selling price of a     \nwooden toy = $20 * 0.85 = $17.  Next, we need to    |  wooden toy = $20 * 0.85 = $17.10 (rounded to the  \nfind out how many paintings and wooden toys Mr.     |  nearest cent)  Next, let's calculate the total    \nCallen sold. Let's assume he sold all the items he  |  selling price of the paintings and wooden toys.   \nbought.  Total number of paintings sold = 10 Total  |  Total selling price of paintings = 10 * $36 = $360\nnumber of wooden toys sold = 8  Total selling       |  Total selling price of wooden toys = 8 * $17.10 = \nprice of paintings = 10 * $36 = $360 Total selling  |  $136.80 (rounded to the nearest cent)  Finally,   \nprice of wooden toys = 8 * $17 = $136  Total        |  let's calculate the total loss Mr. Callen made    \nselling price of all items = $360 + $136 = $496     |  from the sale of the items.  Total loss = Total   \nFinally, we can calculate the total loss Mr.        |  cost of items - Total selling price of items =    \nCallen made from the sale of the items.  Total      |  $560 - ($360 + $136.80) = $560 - $496.80 = $63.20 \nloss = Total cost of items - Total selling price    |  (rounded to the nearest cent)  Therefore, Mr.     \nof all items = $560 - $496 = $64.  Therefore, Mr.   |  Callen made a total loss of $63.20 from the sale  \nCallen made a total loss of $64 from the sale of    |  of the items.                                     \nthe items.                                          |                                                    \n","output_type":"stream"}],"execution_count":76},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:37:44.642645Z","iopub.execute_input":"2024-12-15T14:37:44.643355Z","iopub.status.idle":"2024-12-15T14:38:48.830849Z","shell.execute_reply.started":"2024-12-15T14:37:44.643319Z","shell.execute_reply":"2024-12-15T14:38:48.829935Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    3752071168                     |                     3919843328                    \n================================================== | ==================================================\n### Question: Nine weights of the same weight       |  ### Question: Nine weights of the same weight     \nweigh a total of 4.5 kilograms (kg). Two of these   |  weigh a total of 4.5 kilograms (kg). Two of these \nweights and several pencil cases that weigh 850     |  weights and several pencil cases that weigh 850   \ngrams (g) each were placed on one side of a pan     |  grams (g) each were placed on one side of a pan   \nbalance scale, and five dictionaries that weigh     |  balance scale, and five dictionaries that weigh   \n1050 grams (g) each were placed on the other side,  |  1050 grams (g) each were placed on the other side,\nand they were level. How many pencil cases are      |  and they were level. How many pencil cases are    \nthere? ### Answer: First, let's find out how much   |  there? ### Answer: First, let's find out how much \nthe nine identical weights weigh in total:  9       |  the nine identical weights weigh in total:  9     \nweights = 4.5 kg weight of one weight = 4.5 kg / 9  |  weights = 4.5 kg weight of one weight = 4.5 kg / 9\n= 0.5 kg  Now, let's find out how much the two      |  = 0.5 kg  Now, let's find out how much the two    \nweights and the pencil cases weigh:  two weights =  |  weights and the pencil cases weigh:  two weights =\n0.5 kg * 2 = 1 kg pencil cases = x * 850 g  Since   |  0.5 kg * 2 = 1 kg pencil cases = x * 850 g  Since \nthe two sides of the scale are level, the total     |  the two sides of the scale are level, the total   \nweight on both sides must be equal:  total weight   |  weight on both sides must be equal:  total weight \n= total weight on one side + total weight on the    |  = total weight on one side + total weight on the  \nother side 1 kg (two weights) + x * 850 g (pencil   |  other side 1 kg (two weights) + x * 850 g (pencil \ncases) = 5 * 1050 g (five dictionaries)  First,     |  cases) = 5 * 1050 g (five dictionaries)  First,   \nlet's find out how much the five dictionaries       |  let's find out how much the five dictionaries     \nweigh in kg:  5 * 1050 g = 5250 g = 5.25 kg  Now,   |  weigh in kg:  5 * 1050 g = 5250 g = 5.25 kg  Now, \nwe can find the number of pencil cases:  1 kg (two  |  we can find the number of pencil cases:  1 kg (two\nweights) + x * 850 g (pencil cases) = 5.25 kg x *   |  weights) + x * 850 g (pencil cases) = 5.25 kg x * \n850 g = 4.25 kg x = 4.25 kg / 850 g = 0.005 kg *    |  850 g = 4.25 kg x = 4.25 kg / 850 g = 0.005 kg *  \n850 = 4.25 pencil cases * 850 / 1000 = 3.61 ≈ 4     |  850 = 4.25 pencil cases  However, since we cannot \npencil cases  Therefore, there are approximately 4  |  have a fraction of a pencil case, we need to round\npencil cases.                                       |  up or down to the nearest whole number. In this   \n                                                    |  case, since 0.25 kg is relatively small compared  \n                                                    |  to the total weight, we can round down to 4 pencil\n                                                    |  cases.  Therefore, there are 4 pencil cases.      \n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T14:48:29.795286Z","iopub.execute_input":"2024-12-15T14:48:29.796147Z"}},"outputs":[],"execution_count":null}]}