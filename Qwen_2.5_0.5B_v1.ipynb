{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    TextDataset,\n",
    "    EvalPrediction,\n",
    "    DataCollatorWithPadding,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModelForSequenceClassification,\n",
    "    TaskType,\n",
    "    AutoPeftModelForSequenceClassification,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = str(pathlib.Path().resolve())\n",
    "DATASET_PATH = MAIN_PATH + '\\\\datasets'\n",
    "MODEL_PATH = MAIN_PATH + '\\\\models'\n",
    "SAVE_PATH = MAIN_PATH + '\\\\fine_tuned_models'\n",
    "MODELS = 'D:\\\\AI\\\\LLM\\\\models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.yaml',\n",
       " 'gemma-2-2b',\n",
       " 'Llama-3-8B-GPTQ-4-Bit.safetensors',\n",
       " 'Llama-3-8B-Instruct-GPTQ-4-Bit.safetensors',\n",
       " 'Llama-3.2-11B-Vision-Instruct-bnb-4bit',\n",
       " 'Llama-3.2-1B-Instruct',\n",
       " 'Llama-3.2-3B-Instruct',\n",
       " 'Meta-Llama-3.1-8B-Instruct-GPTQ-INT4',\n",
       " 'Phi-3-mini-128k-instruct',\n",
       " 'Phi-3-mini-128k-instruct-onnx',\n",
       " 'Phi-3-mini-4k-instruct-q4.gguf',\n",
       " 'place-your-models-here.txt',\n",
       " 'Qwen2.5-0.5B',\n",
       " 'Qwen2.5-1.5B',\n",
       " 'Qwen2.5-3B',\n",
       " 'Qwen2.5-7B-Instruct-GPTQ-Int4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = os.listdir(MODELS)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\AI\\\\LLM\\\\models\\\\Qwen2.5-0.5B'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = MODELS + '\\\\' + models[12]\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    ")\n",
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code = True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\422in\\.cache\\huggingface\\hub\\datasets--Ali-C137--Arabic_guanaco_oasst1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 9846/9846 [00:00<00:00, 184998.89 examples/s]\n",
      "Generating test split: 100%|██████████| 518/518 [00:00<00:00, 112345.49 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>### الإنسان: هل يمكنك كتابة مقدمة قصيرة حول أه...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>### الإنسان: ما هي مراحل التطور وماذا تتكون وف...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>### الإنسان: هل يمكنك شرح التعلم التباين في ال...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>### الإنسان: أريد أن أبدأ في التصوير الفوتوغرا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>### الإنسان: طريقة Biclassical Perceptron: تعر...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  ### الإنسان: هل يمكنك كتابة مقدمة قصيرة حول أه...\n",
       "1  ### الإنسان: ما هي مراحل التطور وماذا تتكون وف...\n",
       "2  ### الإنسان: هل يمكنك شرح التعلم التباين في ال...\n",
       "3  ### الإنسان: أريد أن أبدأ في التصوير الفوتوغرا...\n",
       "4  ### الإنسان: طريقة Biclassical Perceptron: تعر..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'Ali-C137/Arabic_guanaco_oasst1'\n",
    "\n",
    "dataset = load_dataset(dataset_name, split = 'train')\n",
    "dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 9846\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha = lora_alpha,\n",
    "    lora_dropout = lora_dropout,\n",
    "    r = lora_r,\n",
    "    bias = 'none',\n",
    "    task_type = 'CAUSAL_LM',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'paged_adamw_32bit'\n",
    "save_steps = 100\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 100\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = 'constant'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = SAVE_PATH,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    optim = optim,\n",
    "    save_steps = save_steps,\n",
    "    logging_steps = logging_steps,\n",
    "    learning_rate = learning_rate,\n",
    "    fp16 = True,\n",
    "    max_grad_norm = max_grad_norm,\n",
    "    max_steps = max_steps,\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    group_by_length = True,\n",
    "    lr_scheduler_type = lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 9846/9846 [00:02<00:00, 3675.56 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    peft_config = peft_config,\n",
    "    dataset_text_field = 'text',\n",
    "    max_seq_length = max_seq_length,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, module in trainer.model.named_modules():\n",
    "    if 'norm' in name:\n",
    "        module = module.to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters : 319444864\n",
      "Trainable parameters : 4325376\n",
      "Trainable percentage: 1.35%\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "trainable_percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print('Total parameters :', total_params)\n",
    "print('Trainable parameters :', trainable_params)\n",
    "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:623: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 10%|█         | 10/100 [01:19<11:28,  7.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4915, 'grad_norm': 0.14193467795848846, 'learning_rate': 0.0002, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [02:48<13:29, 10.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4875, 'grad_norm': 0.17195671796798706, 'learning_rate': 0.0002, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [04:16<09:44,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5491, 'grad_norm': 0.2242295742034912, 'learning_rate': 0.0002, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [04:45<01:59,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7021, 'grad_norm': 0.30632200837135315, 'learning_rate': 0.0002, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [04:57<00:52,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.839, 'grad_norm': 1.2592346668243408, 'learning_rate': 0.0002, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [06:12<04:50,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4626, 'grad_norm': 0.20777428150177002, 'learning_rate': 0.0002, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [07:38<04:55,  9.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4001, 'grad_norm': 0.16684085130691528, 'learning_rate': 0.0002, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [09:09<02:56,  8.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4816, 'grad_norm': 0.21610809862613678, 'learning_rate': 0.0002, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [09:44<00:21,  2.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6349, 'grad_norm': 0.3142939507961273, 'learning_rate': 0.0002, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:56<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7545, 'grad_norm': 1.3272628784179688, 'learning_rate': 0.0002, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:57<00:00,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 597.3421, 'train_samples_per_second': 2.679, 'train_steps_per_second': 0.167, 'train_loss': 2.5802895164489748, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=2.5802895164489748, metrics={'train_runtime': 597.3421, 'train_samples_per_second': 2.679, 'train_steps_per_second': 0.167, 'total_flos': 1194442093937664.0, 'train_loss': 2.5802895164489748, 'epoch': 0.16246953696181965})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "#save_model.save_pretrained(SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
