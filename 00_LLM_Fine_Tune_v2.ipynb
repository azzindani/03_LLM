{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzindani/03_LLM/blob/main/00_LLM_Fine_Tune_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNW_MCROx_hX"
      },
      "source": [
        "## Import Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-QxfiDVyT74",
        "outputId": "20bf1304-7f46-4767-a797-eb98e75bf706"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.34.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.8.30)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.11.4)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.5.0+cu121)\n",
            "Requirement already satisfied: transformers>=4.40.0 in /usr/local/lib/python3.10/dist-packages (from trl) (4.44.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (0.34.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (3.0.2)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.14)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.10/dist-packages (from trl) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.40.0->trl) (4.66.5)\n",
            "Requirement already satisfied: docstring-parser>=0.16 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.9.3)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.10.10)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.16.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.40.0->trl) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.18.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.4.0->trl) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "#!pip install --upgrade transformers\n",
        "!pip install peft\n",
        "!pip install -U bitsandbytes\n",
        "!pip install datasets\n",
        "!pip install trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIgNx9Orx0It",
        "outputId": "5ff44f2a-4049-43d6-977f-1e4aab06df32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from trl import SFTTrainer\n",
        "\n",
        "from transformers import (\n",
        "  AutoTokenizer,\n",
        "  AutoModelForCausalLM,\n",
        "  AutoModelForSeq2SeqLM,\n",
        "  AutoModel,\n",
        "  AutoModelForSequenceClassification,\n",
        "  DataCollatorForLanguageModeling,\n",
        "  Trainer,\n",
        "  TrainingArguments,\n",
        "  pipeline,\n",
        "  TextDataset,\n",
        "  EvalPrediction,\n",
        "  DataCollatorWithPadding,\n",
        "  GenerationConfig,\n",
        "  BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "  LoraConfig,\n",
        "  PeftModelForSequenceClassification,\n",
        "  TaskType,\n",
        "  AutoPeftModelForSequenceClassification,\n",
        "  get_peft_model,\n",
        "  prepare_model_for_kbit_training\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  print(\"GPU is available!\")\n",
        "else:\n",
        "  print(\"GPU is not available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-nwkyTDybqY",
        "outputId": "d69378f9-685b-4ba8-e689-d8139f201331"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grIeJpUdyX0Y"
      },
      "source": [
        "## Import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "14Lkvw4cyZkY"
      },
      "outputs": [],
      "source": [
        "#url = 'https://huggingface.co/Qwen/Qwen2.5-0.5B'\n",
        "#model_name = url.split('.co/')[-1]\n",
        "\n",
        "model_name = 'Qwen/Qwen2.5-0.5B-Instruct'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "GlskFscYyeco",
        "outputId": "0a52f8fb-6e3f-48e2-abb9-bfe18b8bfd92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"bnb_config = BitsAndBytesConfig(\\n  load_in_4bit = True,\\n  bnb_4bit_quant_type = 'nf4',\\n  bnb_4bit_compute_dtype = torch.float16,\\n  bnb_4bit_use_double_quant = True,\\n)\\n\\nmodel = AutoModelForCausalLM.from_pretrained(\\n  model_path,\\n  quantization_config = bnb_config,\\n  trust_remote_code = True\\n)#.to(device) #\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "'''bnb_config = BitsAndBytesConfig(\n",
        "  load_in_4bit = True,\n",
        "  bnb_4bit_quant_type = 'nf4',\n",
        "  bnb_4bit_compute_dtype = torch.float16,\n",
        "  bnb_4bit_use_double_quant = True,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_path,\n",
        "  quantization_config = bnb_config,\n",
        "  trust_remote_code = True\n",
        ")#.to(device) #'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ3WZpjeyfGg",
        "outputId": "0198b0e7-2471-44f2-f09d-9608543294ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "  model_name,\n",
        "  torch_dtype = torch.float16,\n",
        "  trust_remote_code = True\n",
        ").to(device) #'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HIYgZ1xF1qsl",
        "outputId": "492dd8cc-f51b-452b-827b-e8ff69c8887c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2SdpaAttention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "          (rotary_emb): Qwen2RotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6d6uYBfzCC4",
        "outputId": "dc50e619-20f7-4afa-ec84-9c0f22dfbd15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters : 494032768\n",
            "Trainable parameters : 494032768\n",
            "Trainable percentage: 100.00%\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU_19rT5zEIZ"
      },
      "source": [
        "## Import Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "lpB5JUjSzGtJ"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rebT_Mf54eI",
        "outputId": "28865aeb-37e5-464f-f64a-f8ed19019722"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-0.5B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|im_end|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
              "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
              "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QJUqcUVzNoJ"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "U01UXJdLzPXS"
      },
      "outputs": [],
      "source": [
        "#url = 'https://huggingface.co/datasets/KingNish/reasoning-base-20k'\n",
        "#dataset_name = url.split('datasets/')[-1]\n",
        "\n",
        "dataset_name = 'migtissera/Synthia-v1.5-II'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128"
      ],
      "metadata": {
        "id": "ZGIUyIDhNJC2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ucM3l_FzUkp",
        "outputId": "126efe9d-0a60-4808-ed77-e1801cb6e70d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['system', 'instruction', 'response'],\n",
              "    num_rows: 21819\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset = load_dataset(dataset_name, split = 'train')\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "FLRSMhJDzY5Z",
        "outputId": "5fd8d61a-469a-4d53-d738-a253a071f901"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              system  \\\n",
              "0  You are an AI assistant. Provide a detailed an...   \n",
              "1  \\n    Answer the Question by exploring multipl...   \n",
              "2  You are an AI assistant. User will you give yo...   \n",
              "3  You are an AI assistant that follows instructi...   \n",
              "4  You are an AI assistant that helps people find...   \n",
              "\n",
              "                                         instruction  \\\n",
              "0  Here is a detailed, multi-part question on a s...   \n",
              "1  Here is a potential multi-part question on a s...   \n",
              "2  Here is a multi-part graduate level question o...   \n",
              "3  Here is a multi-part question on the topic of ...   \n",
              "4  Here is a detailed, multi-part question on an ...   \n",
              "\n",
              "                                            response  \n",
              "0  Here is a detailed response to your multi-part...  \n",
              "1  Here is my attempt at answering this complex m...  \n",
              "2  Here is my analysis of the graduate-level cybe...  \n",
              "3  This is a great question that delves into some...  \n",
              "4  Thank you for this excellent, comprehensive qu...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1045cdb2-beb1-44a1-a487-08c34efa17a8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>system</th>\n",
              "      <th>instruction</th>\n",
              "      <th>response</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>You are an AI assistant. Provide a detailed an...</td>\n",
              "      <td>Here is a detailed, multi-part question on a s...</td>\n",
              "      <td>Here is a detailed response to your multi-part...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>\\n    Answer the Question by exploring multipl...</td>\n",
              "      <td>Here is a potential multi-part question on a s...</td>\n",
              "      <td>Here is my attempt at answering this complex m...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>You are an AI assistant. User will you give yo...</td>\n",
              "      <td>Here is a multi-part graduate level question o...</td>\n",
              "      <td>Here is my analysis of the graduate-level cybe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>You are an AI assistant that follows instructi...</td>\n",
              "      <td>Here is a multi-part question on the topic of ...</td>\n",
              "      <td>This is a great question that delves into some...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>You are an AI assistant that helps people find...</td>\n",
              "      <td>Here is a detailed, multi-part question on an ...</td>\n",
              "      <td>Thank you for this excellent, comprehensive qu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1045cdb2-beb1-44a1-a487-08c34efa17a8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1045cdb2-beb1-44a1-a487-08c34efa17a8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1045cdb2-beb1-44a1-a487-08c34efa17a8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ab9bcf00-1508-47f2-b9b8-21e2a83e7876\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab9bcf00-1508-47f2-b9b8-21e2a83e7876')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ab9bcf00-1508-47f2-b9b8-21e2a83e7876 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"dataset\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"system\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"\\n    Answer the Question by exploring multiple reasoning paths as follows:\\n    - First, carefully analyze the question to extract the key information components and break it down into logical sub-questions. This helps set up the framework for reasoning. The goal is to construct an internal search tree.\\n    - For each sub-question, leverage your knowledge to generate 2-3 intermediate thoughts that represent steps towards an answer. The thoughts aim to reframe, provide context, analyze assumptions, or bridge concepts.\\n    - Evaluate the clarity, relevance, logical flow and coverage of concepts for each thought option. Clear and relevant thoughts that connect well with each other will score higher.\\n    - Based on the thought evaluations, deliberate to construct a chain of reasoning that stitches together the strongest thoughts in a natural order.\\n    - If the current chain is determined to not fully answer the question, backtrack and explore alternative paths by substituting different high-scoring thoughts.\\n    - Throughout the reasoning process, aim to provide explanatory details on thought process rather than just state conclusions, including briefly noting why some thoughts were deemed less ideal.\\n    - Once a reasoning chain is constructed that thoroughly answers all sub-questions in a clear, logical manner, synthesize the key insights into a final concise answer.\\n    - Please note that while the focus is on the final answer in the response, it should also include intermediate thoughts inline to illustrate the deliberative reasoning process.\\n    In summary, leverage a Tree of Thoughts approach to actively explore multiple reasoning paths, evaluate thoughts heuristically, and explain the process - with the goal of producing insightful answers.\\n    \",\n          \"You are an AI assistant that helps people find information. User will you give you a question. Your task is to answer as faithfully as you can. While answering think step-by-step and justify your answer.\",\n          \"You are an AI assistant. User will you give you a task. Your goal is to complete the task as faithfully as you can. While performing the task think step-by-step and justify your steps.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"instruction\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Here is a potential multi-part question on a specific topic within constitutional law relating to civil liberties and separation of powers:\\n\\nRegarding the tension between national security and individual privacy rights in the context of government surveillance programs:\\n\\na) Discuss the key arguments on both sides of the debate around the constitutionality of warrantless surveillance and bulk metadata collection by intelligence agencies like the NSA. Reference specific constitutional amendments, case law, and legal doctrines in your analysis.\\n\\nb) Explain the role and stances of the different branches of government (executive, legislative, judicial) when it comes to oversight and authorization of surveillance activities. How does the separation of powers doctrine factor into the ability of each branch to check potential overreach by the others? \\n\\nc) Explore how the courts have attempted to strike a balance between protecting civil liberties and enabling national security efforts. Discuss the implications and limitations of key court decisions like Katz v. United States (1967), Smith v. Maryland (1979), and more recently ACLU v. Clapper (2015). Have the courts established a clear and consistent constitutional doctrine in this area?\\n\\nd) Analyze the effectiveness of existing statutory frameworks like the Foreign Intelligence Surveillance Act (FISA), FISA Amendments Act, and USA PATRIOT Act in regulating surveillance powers while preserving privacy safeguards and civil liberties. Are legislative reforms needed?\\n\\ne) In your view, how should the constitutional balance between national security imperatives and individual privacy rights be struck in an age of global terrorism and rapidly advancing surveillance technologies? Justify your position.\\n\\nThis complex question requires substantive legal analysis from multiple angles, while touching on core constitutional law principles around civil liberties and the separation of powers. It demands critical thinking to grapple with the difficult tradeoffs involved. Let me know if you would like me to try generating a different multi-part question on another specific aspect of constitutional law.\",\n          \"Here is a detailed, multi-part question on an important, narrow topic within the subject area of Earth systems in Environmental Science:\\n\\nThe global carbon cycle is a critical biogeochemical cycle that involves the exchange of carbon between the atmosphere, hydrosphere, lithosphere, and biosphere. Anthropogenic activities like fossil fuel combustion are altering the natural carbon cycle. \\n\\nPart A) Describe the main reservoirs and fluxes in the global carbon cycle. Include in your answer the typical residence times of carbon in each of the major reservoirs.\\n\\nPart B) Explain the difference between the short-term and long-term carbon cycles. What are the key processes that move carbon between reservoirs on short vs long timescales? \\n\\nPart C) How have human activities, especially since the Industrial Revolution, quantitatively changed the amount of carbon held in the atmospheric reservoir? Describe the evidence for this change.\\n\\nPart D) Increased atmospheric CO2 leads to ocean acidification when CO2 dissolves in seawater. Write the chemical reaction that occurs when CO2 dissolves in water and explain how it lowers ocean pH. What are the impacts of ocean acidification on marine life?\\n\\nPart E) Scientists can use proxies to estimate past atmospheric CO2 levels, before direct measurements began. Describe 2 proxies used to reconstruct past atmospheric CO2 concentrations, and the strengths and limitations of each. How far back in time can each one be reliably used?\\n\\nPart F) Climate models predict a range of future atmospheric CO2 concentrations and global temperatures, depending on emissions scenarios. Summarize the range of CO2 levels and global temperature change projected by models for the year 2100 under \\\"business-as-usual\\\" vs aggressive emissions reduction scenarios. What key uncertainties limit the precision of these projections?\",\n          \"Here is a multi-part graduate level question on a narrow but important topic in cybersecurity:\\n\\nConsider a large enterprise organization that relies heavily on web-based applications for business critical functions. These web apps handle sensitive customer data and process financial transactions. \\n\\nPart A) What are the key attack vectors and vulnerabilities that attackers could exploit in the organization's web applications? Discuss at least 3 major classes of web app vulnerabilities.\\n\\nPart B) Design a comprehensive and layered security architecture to protect the organization's web apps and infrastructure. Your solution should cover:\\n- Secure application development practices the org should follow\\n- Network security controls including proper segmentation, firewalls, IPS/IDS  \\n- Robust authentication and access control for the web apps\\n- Encryption of data in transit and at rest\\n- Monitoring, logging and incident response capabilities\\n\\nPart C) Despite the defensive measures, assume a skilled attacker is able to breach the perimeter and gain unauthorized access to the internal network. Describe the steps of an attack kill chain the attacker could follow to ultimately compromise sensitive data. What are some techniques they may use to escalate privileges and move laterally within the network?\\n\\nPart D) The organization is considering engaging an external firm to conduct penetration testing of its web applications and infrastructure. What are the benefits of this type of offensive security testing? What are important considerations when scoping and overseeing a penetration testing engagement?\\n\\nIn your answer, cite specific examples and reference relevant cybersecurity standards/frameworks where applicable. Discuss the tradeoffs of different approaches. The goal is to provide thorough analysis and actionable recommendations grounded in cybersecurity best practices.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"response\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Here is my attempt at answering this complex multi-part question on the tension between national security and individual privacy rights in the context of government surveillance programs:\\n\\na) Key constitutional arguments:\\n- Arguments for constitutionality of surveillance: \\n(1) No reasonable expectation of privacy in metadata voluntarily shared with third parties per Smith v. Maryland (1979). \\n(2) Special needs exception to 4th Amendment warrant requirement for national security purposes.\\n(3) Art. II executive power to protect nation enables surveillance.\\n- Arguments against constitutionality:\\n(1) 4th Amendment prohibits unreasonable searches and seizures. Warrantless bulk collection is unreasonable search.\\n(2) Chills 1st Amendment free speech and association.\\n(3) Violates privacy and liberty without due process (5th/14th Amendments).\\n(4) Exceeds enumerated powers, no general police power (10th Amendment).\\n\\nStrongest arguments against seem to be 4th Amendment prohibition on unreasonable searches and lack of individualized suspicion. But complex balance with executive national security powers.\\n\\nb) Separation of powers implications:\\n- Executive branch (President, AG, agencies like NSA) initiates and conducts surveillance. Claims Art. II inherent power. \\n- Congress provides legislative framework and oversight through statutes like FISA. But concern over potential overreach and abdicating role.\\n- Courts evaluate constitutionality and apply statutes. FISC provides secret review. But historically deferential to executive on national security. Regular courts more probing.\\nSeparation of powers should enable each branch to check the others. But history of excessive deference to executive branch out of national security concerns, undermining oversight by Congress and courts.\\n\\nc) Key court decisions:\\n- Katz v. US (1967): 4th Amendment protects reasonable expectation of privacy. Wiretapping constitutes search requiring warrant.\\n- Smith v. Maryland (1979): No expectation of privacy in phone numbers dialed. Allows metadata collection.\\n- ACLU v. Clapper (2015): Divided 2nd Circuit panel found bulk metadata collection exceeded PATRIOT Act authority. But new USA FREEDOM Act mooted appeal.\\n- Carpenter v. US (2018): Acquiring cell-site location data is 4th Amendment search. Suggests shifting doctrine on digital surveillance.\\nCourts have not definitively resolved constitutionality of bulk warrantless surveillance. Carpenter indicates growing scrutiny of intrusive digital monitoring under 4th Amendment. But national security exception looms.\\n\\nd) Effectiveness of statutory frameworks:\\n- FISA and FISA Amendments Act provide statutory process for foreign surveillance. But expansive authority, secret FISC review.\\n- PATRIOT Act Sec. 215 controversially allowed bulk metadata collection until USA FREEDOM Act reforms in 2015.\\n- New laws aimed to increase transparency, add privacy safeguards, require more specific selectors. \\n- Critics argue reforms still inadequate to prevent potential abuse for bulk surveillance and protect privacy/civil liberties.\\nLegislative frameworks provide some regulation and oversight of surveillance, but contain broad loopholes. Struggle to keep pace with rapidly evolving technical capabilities. Stronger reforms and more robust judicial review likely needed.\\n\\ne) In my view, while national security is of paramount importance, warrantless bulk surveillance programs that indiscriminately collect data on millions of Americans' communications raise grave constitutional concerns under the 4th Amendment. The founders enshrined protections against unreasonable government searches precisely to prevent the specter of a surveillance state that chills free speech and intrudes on privacy. \\n\\nTerrorism undoubtedly poses severe threats that warrant intelligence gathering efforts. But these programs must be subject to meaningful judicial review, constrained by probable cause, and collect only data necessary for specific investigations - not dragnet operations. Congress should enact stronger reforms restricting untargeted bulk collection, require FISC to apply standard 4th Amendment scrutiny, and increase transparency around surveillance activities.\\n\\nAs Justices Sotomayor and Gorsuch have incisively noted, the third-party doctrine allowing metadata collection is ill-suited to the realities of the digital age, where we share vast data with providers as an unavoidable part of participating in modern society. Courts should refine doctrine to properly protect reasonable expectations of privacy in electronic data. The executive branch cannot have a blank check to secretly amass private databases on citizens without individualized suspicion. \\n\\nThe Constitution demands we place meaningful limits on surveillance powers, lest liberty be forsaken in the name of security. New technologies will only heighten these tensions. All three branches must play an active role in vigilantly safeguarding privacy and due process while still empowering agencies to thwart genuine threats. Only then will the constitutional balance between national security and sacred individual freedoms be properly struck.\",\n          \"Thank you for this excellent, comprehensive question on the carbon cycle and climate change. This is a critical topic in Earth system science. I'll answer each part in turn:\\n\\nPart A) The main reservoirs in the global carbon cycle are:\\n1) Atmosphere (720 Pg C, as CO2) \\n2) Terrestrial biosphere (2000 Pg C in biomass and soils)\\n3) Oceans (38,000 Pg C as dissolved inorganic carbon) \\n4) Sedimentary rocks (100,000,000 Pg C in limestone and other sediments)\\n\\nThe main fluxes are:\\n- Photosynthesis and respiration between atmosphere and biosphere (120 Pg C/yr)  \\n- Gas exchange between atmosphere and ocean surface (90 Pg C/yr)\\n- Weathering of rocks, releasing C to rivers (0.2 Pg C/yr)\\n- Burial of organic C and calcium carbonate in sediments (0.2 Pg C/yr)\\n\\nTypical residence times:\\n- Atmosphere: 3-4 years  \\n- Biosphere: years to centuries\\n- Surface ocean: months to decades\\n- Deep ocean: centuries to millennia\\n- Sedimentary rocks: >10,000 years\\n\\nPart B) The short-term carbon cycle involves exchange of CO2 between atmosphere, biosphere and surface ocean on timescales up to centuries. Key processes are photosynthesis, respiration, air-sea gas exchange, and ocean circulation.\\n\\nThe long-term carbon cycle involves exchanges with rocks and sediments over >10,000 year timescales. Key processes are chemical weathering of silicate rocks which consumes atmospheric CO2, precipitation of calcium carbonate in the ocean which releases CO2, and burial of organic carbon and carbonates in sediments which removes C.\\n\\nPart C) Since the Industrial Revolution, human activities have increased atmospheric CO2 from ~280 ppm in 1750 to 413 ppm in 2020, a 48% increase. The main causes are fossil fuel burning and deforestation. We know this increase is anthropogenic because:\\n1) The rate of CO2 rise far exceeds natural changes seen in ice core records \\n2) The isotopic signature of the added CO2 matches that of fossil fuels\\n3) Atmospheric oxygen is declining as CO2 rises, consistent with combustion\\n4) CO2 is rising more in the Northern Hemisphere where most emissions occur\\n\\nPart D) When CO2 dissolves in seawater it reacts to form carbonic acid:\\nCO2 + H2O \\u21cc H2CO3\\nCarbonic acid dissociates to form bicarbonate and carbonate ions:  \\nH2CO3 \\u21cc H+ + HCO3\\u2212\\nHCO3\\u2212 \\u21cc H+ + CO32-\\nThe extra H+ lowers the pH (increases acidity). Ocean pH has dropped from 8.2 to 8.1 since pre-industrial times. Impacts on marine life include:\\n- Acidic water corrodes shells/skeletons of corals, molluscs, some plankton\\n- Larvae and juveniles are more vulnerable than adults  \\n- Ecosystem effects as calcifying species decline\\n- Some seagrasses/algae may benefit from higher CO2\\n\\nPart E) Two key CO2 proxies are:\\n1) Ice cores - Tiny bubbles in ice trap samples of ancient air up to 800,000 years old. CO2 is measured directly by extracting the air. Pros: Direct CO2 measurement, resolves glacial-interglacial changes. Cons: Ice cores only occur in polar regions. Limited to last 800kyr.  \\n2) Carbon isotope ratios in sediments - Plants prefer 12C during photosynthesis. More 12C in organic sediments means more plant biomass which removes CO2. Pros: Sediments allow CO2 estimates back 100s of millions of years. Cons: Indirect measure with more assumptions. Lower time resolution.\\n\\nPart F) Climate model projections for 2100:\\n- \\\"Business-as-usual\\\": 600-900 ppm CO2, 3-5\\u00b0C warming\\n- Aggressive reduction: 400-500 ppm CO2, 1-2\\u00b0C warming\\n\\nKey uncertainties:\\n- Emission rates (depends on population, technology, policy decisions)\\n- Climate sensitivity - how much warming occurs per CO2 doubling  \\n- Feedbacks - clouds, ice albedo, methane release, etc. Can amplify or reduce warming.\\n- Regional climate changes harder to predict than global average\\n\\nIn summary, the carbon cycle is being significantly perturbed by human CO2 emissions, with potentially severe climate impacts and ocean acidification. Reducing emissions is crucial to limit these risks, but predicting the exact future changes is challenging due to various uncertainties. Continuing to improve our understanding of the carbon cycle is essential for informing policy decisions.\",\n          \"Here is my analysis of the graduate-level cybersecurity question:\\n\\nPart A) Major classes of web application vulnerabilities that could be exploited include:\\n\\n1. Injection flaws (e.g. SQL injection, command injection) - Attackers manipulate input fields to inject malicious code that gets executed by the backend. This can allow unauthorized data access, deletion, or remote code execution.\\n\\n2. Cross-site scripting (XSS) - Attackers inject client-side scripts into web pages to steal user session data, deface sites, or redirect users to malicious pages. XSS exploits the browser's trust of content received from the server.\\n\\n3. Broken authentication and session management - Vulnerabilities in user authentication (e.g. weak passwords, unprotected credentials) or improper handling of session IDs/tokens can allow attackers to assume legitimate users' identities.\\n\\nOther common web app vulnerabilities include security misconfigurations, sensitive data exposure, using components with known vulnerabilities, and lack of sufficient logging/monitoring.\\n\\nPart B) A layered security architecture should incorporate:\\n\\n- Secure development practices like input validation, parameterized queries, least privilege, and frequent security testing in the SDLC. Follow OWASP Top 10 guidance.\\n\\n- Network segmentation using firewalls, DMZs, and VLANs to isolate web apps from internal resources. Deploy IPS/IDS to monitor for and block malicious traffic. \\n\\n- Strong authentication using MFA, Federation, or risk-based methods. Implement granular access control (RBAC, ABAC) and promptly de-provision unused accounts.\\n\\n- Encrypt all sensitive data in transit using TLS and at rest using AES-256 or similar. Secure key management is critical.\\n\\n- Centralized logging (SIEM) to collect events across web servers, apps, DBs and correlate to detect anomalies. Have an incident response plan and tools in place.\\n\\nPart C) A potential attack kill chain after perimeter breach:\\n\\n1. Reconnaissance - Scan network for live systems, ports, services, vulnerabilities. Gather data on users, configurations.\\n\\n2. Initial exploitation - Exploit unpatched system, misconfigurations, default credentials, or social engineer user. Establish persistence and C2 channel.\\n\\n3. Privilege escalation - Exploit local vulnerabilities, misconfigurations to gain admin/root privileges on system. Use pass-the-hash, keylogging, or credential access techniques.\\n\\n4. Lateral movement - Use exploited system as beachhead. Scan for open shares, RDP, SSH. Use gathered credentials or exploit vulnerabilities to compromise more systems and accounts. \\n\\n5. Data exfiltration - Identify location of target data/IP. Use encryption and covert channels (DNS tunneling, stenography) to slowly exfil data to avoid detection.\\n\\nPart D) Benefits of penetration testing for web apps and infrastructure:\\n- Identifies vulnerabilities and misconfigurations in a controlled manner\\n- Validates efficacy of security controls and detects gaps\\n- Provides risk-based outcome to help prioritize remediation \\n- Helps meet regulatory requirements and customer expectations\\n- Proactively strengthens security posture against real-world attacks\\n\\nKey considerations for penetration testing:\\n- Define clear scope and rules of engagement. Limit to test environment if possible.\\n- Select qualified vendor with relevant certifications (e.g. OSCP) and references  \\n- Require documentation of findings with risk ratings and remediation advice\\n- Ensure testing methodology aligns with standards like NIST, OSSTMM, PTES\\n- Retest to validate that findings are remediated, not just the initial assessment\\n\\nIn summary, protecting web applications handling sensitive data requires a comprehensive approach spanning secure development, in-depth network defenses, strong authentication/encryption, and proactive testing. Effective execution demands involvement from developers, IT ops, security teams, and risk stakeholders.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "dataset.select(range(5)).to_pandas().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3exPEy0JdLyI",
        "outputId": "38fe6b6b-ce1d-42a2-8dc7-8b38c726eeb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'system': \"You are an AI assistant. Provide a detailed answer so user don't need to search outside to understand the answer.\",\n",
              " 'instruction': \"Here is a detailed, multi-part question on a specific topic within Blockchain and Distributed Ledger Technologies:\\n\\nConsensus mechanisms are critical components of blockchain systems that enable network participants to reach agreement on the state of the distributed ledger in a trustless, decentralized manner. Proof-of-Work (PoW) and Proof-of-Stake (PoS) are two widely used consensus algorithms.\\n\\na) Explain the fundamental differences between PoW and PoS consensus mechanisms, including their respective processes for validating transactions and adding new blocks to the blockchain. \\n\\nb) Analyze the security implications of each algorithm, considering potential attack vectors such as 51% attacks, selfish mining, and long-range attacks. Discuss how each algorithm's design features and economic incentives contribute to network security.\\n\\nc) Compare the energy consumption and scalability of PoW and PoS. How do these factors impact the long-term sustainability and adoption potential of blockchain networks using these consensus mechanisms?\\n\\nd) Smart contracts are self-executing programs stored on the blockchain that automatically enforce the terms of an agreement. Describe how the choice of consensus mechanism (PoW or PoS) affects the security, reliability, and performance of smart contract execution. \\n\\ne) Propose a hypothetical hybrid consensus mechanism that combines elements of both PoW and PoS. Explain how this novel approach could potentially optimize security, scalability, and decentralization. Discuss the challenges and trade-offs involved in implementing such a hybrid system.\",\n",
              " 'response': 'Here is a detailed response to your multi-part question on Proof-of-Work (PoW) vs. Proof-of-Stake (PoS) consensus mechanisms in blockchain systems:\\n\\na) Fundamental differences between PoW and PoS:\\n\\nProof-of-Work (PoW) involves miners competing to solve complex cryptographic puzzles to validate transactions and add new blocks. The first miner to solve the puzzle gets to add the next block and receives a block reward in the form of newly minted coins and transaction fees. This process is extremely computationally intensive.\\n\\nIn contrast, Proof-of-Stake (PoS) selects validators based on the amount of cryptocurrency they hold and \"stake\" as collateral. Validators are chosen in a pseudo-random way, with those staking more coins having a higher chance of being selected. Validators verify transactions, create new blocks, and receive transaction fees as a reward. PoS does not involve mining.\\n\\nb) Security implications:\\n\\nPoW is generally considered more battle-tested and secure than PoS at this stage. The enormous amount of computational power required makes launching 51% attacks prohibitively expensive in established PoW networks like Bitcoin. However, PoW is vulnerable to selfish mining where miners hide new blocks and release them strategically to gain an unfair share of rewards.\\n\\nPoS can be more vulnerable to 51% attacks if an entity acquires over half the staked coins. Malicious stakers could also validate fraudulent transactions. PoS is also susceptible to long-range attacks where attackers attempt to rewrite a blockchain\\'s history by creating an alternate chain from an old block.\\n\\nOverall, both have vulnerabilities that can be mitigated through careful protocol design, incentive structures that deter misbehavior, and decentralization of mining/staking power.\\n\\nc) Energy consumption and scalability:\\n\\nPoW\\'s competitive mining process consumes massive amounts of computational power and electricity. Networks like Bitcoin have an estimated annualized electricity consumption comparable to countries like Argentina. This raises major environmental sustainability concerns.\\n\\nPoS is far more energy-efficient since stakers are not expending computing resources to validate blocks. From a scalability perspective, PoS can typically achieve higher transaction throughput and lower fees than PoW. PoS enables faster block confirmation times.\\n\\nHowever, some argue PoW\\'s high energy usage contributes to security by making attacks costly. Researchers are exploring ways to make PoW more efficient. Overall, PoS is seen as more sustainable and scalable.\\n\\nd) Impact on smart contracts: \\n\\nThe reliability and security of smart contract execution depends heavily on the underlying blockchain\\'s resistance to double-spending attacks, staking-related attacks, and other vulnerabilities. \\n\\nPoW\\'s energy-intensive mining provides robust prevention of double-spending that could corrupt smart contract state. However, PoW\\'s probabilistic finality means smart contracts may need to wait longer for sufficient block confirmations.\\n\\nPoS offers faster finality which can benefit smart contracts requiring quick settlement. PoS may provide higher throughput for contract-heavy applications. However, PoS-based chains must have secure validator selection and strong incentives against malicious stakers to maintain smart contract integrity.\\n\\nUltimately, smart contract security relies on rigorous auditing and battle-tested virtual machines like the EVM. Choice of PoW vs PoS involves complex tradeoffs around speed, finality and attack vectors.\\n\\ne) Hybrid PoW/PoS consensus:\\n\\nSome researchers have proposed hybrid PoW/PoS mechanisms to get the \"best of both worlds.\" For example, miners could use PoW to produce blocks but a randomly selected group of stakers would finalize them. Alternatively, PoS could be used for regular blocks and PoW for periodic \"checkpoints.\"\\n\\nThe goal is to combine PoW\\'s proven security with PoS\\'s finality and efficiency. A hybrid model could resist 51% attacks and long range attacks better than pure PoS while being more sustainable than pure PoW.\\n\\nHowever, hybridizing consensus is complex and may introduce new attack surfaces. Economic incentives, manager selection and integration between the PoW and PoS components must be carefully designed. Real-world implementation and stress-testing are needed to validate theoretical security. Striking the optimal balance of security, decentralization and scalability with a hybrid model is an ongoing challenge.\\n\\nIn summary, while PoW is battle-hardened and secure, PoS offers superior energy efficiency and scalability. The choice involves nuanced tradeoffs based on a blockchain\\'s threat model, performance needs and sustainability priorities. Hybrid designs are an exciting area of research in the quest for an optimal consensus system. Rigorous testing of new models in practice will be critical.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYKmTDtkAnt5",
        "outputId": "6baf9687-f361-4703-8eeb-0d66b5226692"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['system', 'instruction', 'response']\n"
          ]
        }
      ],
      "source": [
        "features = list(dataset.features.keys())\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nrMsAT9weHXs"
      },
      "outputs": [],
      "source": [
        "def format_text(example):\n",
        "  example[\"text\"] = f\"System: {example['system']} Instruction: {example['instruction']} Response: {example['response']}\"\n",
        "  return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6sSaCr5eaL7",
        "outputId": "c4d24d75-4dcd-4192-f2bb-113987e1bff0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 21819\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "formatted_dataset = dataset.map(format_text, remove_columns = features)\n",
        "formatted_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kidf8H5zefDC",
        "outputId": "7218f6a4-c5e3-449d-f805-b210cfd336e5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'System: You are an AI assistant. Provide a detailed answer so user don\\'t need to search outside to understand the answer. Instruction: Here is a detailed, multi-part question on a specific topic within Blockchain and Distributed Ledger Technologies:\\n\\nConsensus mechanisms are critical components of blockchain systems that enable network participants to reach agreement on the state of the distributed ledger in a trustless, decentralized manner. Proof-of-Work (PoW) and Proof-of-Stake (PoS) are two widely used consensus algorithms.\\n\\na) Explain the fundamental differences between PoW and PoS consensus mechanisms, including their respective processes for validating transactions and adding new blocks to the blockchain. \\n\\nb) Analyze the security implications of each algorithm, considering potential attack vectors such as 51% attacks, selfish mining, and long-range attacks. Discuss how each algorithm\\'s design features and economic incentives contribute to network security.\\n\\nc) Compare the energy consumption and scalability of PoW and PoS. How do these factors impact the long-term sustainability and adoption potential of blockchain networks using these consensus mechanisms?\\n\\nd) Smart contracts are self-executing programs stored on the blockchain that automatically enforce the terms of an agreement. Describe how the choice of consensus mechanism (PoW or PoS) affects the security, reliability, and performance of smart contract execution. \\n\\ne) Propose a hypothetical hybrid consensus mechanism that combines elements of both PoW and PoS. Explain how this novel approach could potentially optimize security, scalability, and decentralization. Discuss the challenges and trade-offs involved in implementing such a hybrid system. Response: Here is a detailed response to your multi-part question on Proof-of-Work (PoW) vs. Proof-of-Stake (PoS) consensus mechanisms in blockchain systems:\\n\\na) Fundamental differences between PoW and PoS:\\n\\nProof-of-Work (PoW) involves miners competing to solve complex cryptographic puzzles to validate transactions and add new blocks. The first miner to solve the puzzle gets to add the next block and receives a block reward in the form of newly minted coins and transaction fees. This process is extremely computationally intensive.\\n\\nIn contrast, Proof-of-Stake (PoS) selects validators based on the amount of cryptocurrency they hold and \"stake\" as collateral. Validators are chosen in a pseudo-random way, with those staking more coins having a higher chance of being selected. Validators verify transactions, create new blocks, and receive transaction fees as a reward. PoS does not involve mining.\\n\\nb) Security implications:\\n\\nPoW is generally considered more battle-tested and secure than PoS at this stage. The enormous amount of computational power required makes launching 51% attacks prohibitively expensive in established PoW networks like Bitcoin. However, PoW is vulnerable to selfish mining where miners hide new blocks and release them strategically to gain an unfair share of rewards.\\n\\nPoS can be more vulnerable to 51% attacks if an entity acquires over half the staked coins. Malicious stakers could also validate fraudulent transactions. PoS is also susceptible to long-range attacks where attackers attempt to rewrite a blockchain\\'s history by creating an alternate chain from an old block.\\n\\nOverall, both have vulnerabilities that can be mitigated through careful protocol design, incentive structures that deter misbehavior, and decentralization of mining/staking power.\\n\\nc) Energy consumption and scalability:\\n\\nPoW\\'s competitive mining process consumes massive amounts of computational power and electricity. Networks like Bitcoin have an estimated annualized electricity consumption comparable to countries like Argentina. This raises major environmental sustainability concerns.\\n\\nPoS is far more energy-efficient since stakers are not expending computing resources to validate blocks. From a scalability perspective, PoS can typically achieve higher transaction throughput and lower fees than PoW. PoS enables faster block confirmation times.\\n\\nHowever, some argue PoW\\'s high energy usage contributes to security by making attacks costly. Researchers are exploring ways to make PoW more efficient. Overall, PoS is seen as more sustainable and scalable.\\n\\nd) Impact on smart contracts: \\n\\nThe reliability and security of smart contract execution depends heavily on the underlying blockchain\\'s resistance to double-spending attacks, staking-related attacks, and other vulnerabilities. \\n\\nPoW\\'s energy-intensive mining provides robust prevention of double-spending that could corrupt smart contract state. However, PoW\\'s probabilistic finality means smart contracts may need to wait longer for sufficient block confirmations.\\n\\nPoS offers faster finality which can benefit smart contracts requiring quick settlement. PoS may provide higher throughput for contract-heavy applications. However, PoS-based chains must have secure validator selection and strong incentives against malicious stakers to maintain smart contract integrity.\\n\\nUltimately, smart contract security relies on rigorous auditing and battle-tested virtual machines like the EVM. Choice of PoW vs PoS involves complex tradeoffs around speed, finality and attack vectors.\\n\\ne) Hybrid PoW/PoS consensus:\\n\\nSome researchers have proposed hybrid PoW/PoS mechanisms to get the \"best of both worlds.\" For example, miners could use PoW to produce blocks but a randomly selected group of stakers would finalize them. Alternatively, PoS could be used for regular blocks and PoW for periodic \"checkpoints.\"\\n\\nThe goal is to combine PoW\\'s proven security with PoS\\'s finality and efficiency. A hybrid model could resist 51% attacks and long range attacks better than pure PoS while being more sustainable than pure PoW.\\n\\nHowever, hybridizing consensus is complex and may introduce new attack surfaces. Economic incentives, manager selection and integration between the PoW and PoS components must be carefully designed. Real-world implementation and stress-testing are needed to validate theoretical security. Striking the optimal balance of security, decentralization and scalability with a hybrid model is an ongoing challenge.\\n\\nIn summary, while PoW is battle-hardened and secure, PoS offers superior energy efficiency and scalability. The choice involves nuanced tradeoffs based on a blockchain\\'s threat model, performance needs and sustainability priorities. Hybrid designs are an exciting area of research in the quest for an optimal consensus system. Rigorous testing of new models in practice will be critical.'}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "formatted_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "m7bxU8fiewb7"
      },
      "outputs": [],
      "source": [
        "def tokenize_data(example, max_length = max_length):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3BO26k-BmdS",
        "outputId": "f0efd69d-6819-4f9c-82d9-26ff4067160a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 21819\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenized_dataset = formatted_dataset.map(tokenize_data, batched = True)#, remove_columns = 'text')\n",
        "tokenized_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEHhMdV4pEFH",
        "outputId": "a847a34e-043a-44aa-a8a8-b35a1171dd49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'System: You are an AI assistant. Provide a detailed answer so user don\\'t need to search outside to understand the answer. Instruction: Here is a detailed, multi-part question on a specific topic within Blockchain and Distributed Ledger Technologies:\\n\\nConsensus mechanisms are critical components of blockchain systems that enable network participants to reach agreement on the state of the distributed ledger in a trustless, decentralized manner. Proof-of-Work (PoW) and Proof-of-Stake (PoS) are two widely used consensus algorithms.\\n\\na) Explain the fundamental differences between PoW and PoS consensus mechanisms, including their respective processes for validating transactions and adding new blocks to the blockchain. \\n\\nb) Analyze the security implications of each algorithm, considering potential attack vectors such as 51% attacks, selfish mining, and long-range attacks. Discuss how each algorithm\\'s design features and economic incentives contribute to network security.\\n\\nc) Compare the energy consumption and scalability of PoW and PoS. How do these factors impact the long-term sustainability and adoption potential of blockchain networks using these consensus mechanisms?\\n\\nd) Smart contracts are self-executing programs stored on the blockchain that automatically enforce the terms of an agreement. Describe how the choice of consensus mechanism (PoW or PoS) affects the security, reliability, and performance of smart contract execution. \\n\\ne) Propose a hypothetical hybrid consensus mechanism that combines elements of both PoW and PoS. Explain how this novel approach could potentially optimize security, scalability, and decentralization. Discuss the challenges and trade-offs involved in implementing such a hybrid system. Response: Here is a detailed response to your multi-part question on Proof-of-Work (PoW) vs. Proof-of-Stake (PoS) consensus mechanisms in blockchain systems:\\n\\na) Fundamental differences between PoW and PoS:\\n\\nProof-of-Work (PoW) involves miners competing to solve complex cryptographic puzzles to validate transactions and add new blocks. The first miner to solve the puzzle gets to add the next block and receives a block reward in the form of newly minted coins and transaction fees. This process is extremely computationally intensive.\\n\\nIn contrast, Proof-of-Stake (PoS) selects validators based on the amount of cryptocurrency they hold and \"stake\" as collateral. Validators are chosen in a pseudo-random way, with those staking more coins having a higher chance of being selected. Validators verify transactions, create new blocks, and receive transaction fees as a reward. PoS does not involve mining.\\n\\nb) Security implications:\\n\\nPoW is generally considered more battle-tested and secure than PoS at this stage. The enormous amount of computational power required makes launching 51% attacks prohibitively expensive in established PoW networks like Bitcoin. However, PoW is vulnerable to selfish mining where miners hide new blocks and release them strategically to gain an unfair share of rewards.\\n\\nPoS can be more vulnerable to 51% attacks if an entity acquires over half the staked coins. Malicious stakers could also validate fraudulent transactions. PoS is also susceptible to long-range attacks where attackers attempt to rewrite a blockchain\\'s history by creating an alternate chain from an old block.\\n\\nOverall, both have vulnerabilities that can be mitigated through careful protocol design, incentive structures that deter misbehavior, and decentralization of mining/staking power.\\n\\nc) Energy consumption and scalability:\\n\\nPoW\\'s competitive mining process consumes massive amounts of computational power and electricity. Networks like Bitcoin have an estimated annualized electricity consumption comparable to countries like Argentina. This raises major environmental sustainability concerns.\\n\\nPoS is far more energy-efficient since stakers are not expending computing resources to validate blocks. From a scalability perspective, PoS can typically achieve higher transaction throughput and lower fees than PoW. PoS enables faster block confirmation times.\\n\\nHowever, some argue PoW\\'s high energy usage contributes to security by making attacks costly. Researchers are exploring ways to make PoW more efficient. Overall, PoS is seen as more sustainable and scalable.\\n\\nd) Impact on smart contracts: \\n\\nThe reliability and security of smart contract execution depends heavily on the underlying blockchain\\'s resistance to double-spending attacks, staking-related attacks, and other vulnerabilities. \\n\\nPoW\\'s energy-intensive mining provides robust prevention of double-spending that could corrupt smart contract state. However, PoW\\'s probabilistic finality means smart contracts may need to wait longer for sufficient block confirmations.\\n\\nPoS offers faster finality which can benefit smart contracts requiring quick settlement. PoS may provide higher throughput for contract-heavy applications. However, PoS-based chains must have secure validator selection and strong incentives against malicious stakers to maintain smart contract integrity.\\n\\nUltimately, smart contract security relies on rigorous auditing and battle-tested virtual machines like the EVM. Choice of PoW vs PoS involves complex tradeoffs around speed, finality and attack vectors.\\n\\ne) Hybrid PoW/PoS consensus:\\n\\nSome researchers have proposed hybrid PoW/PoS mechanisms to get the \"best of both worlds.\" For example, miners could use PoW to produce blocks but a randomly selected group of stakers would finalize them. Alternatively, PoS could be used for regular blocks and PoW for periodic \"checkpoints.\"\\n\\nThe goal is to combine PoW\\'s proven security with PoS\\'s finality and efficiency. A hybrid model could resist 51% attacks and long range attacks better than pure PoS while being more sustainable than pure PoW.\\n\\nHowever, hybridizing consensus is complex and may introduce new attack surfaces. Economic incentives, manager selection and integration between the PoW and PoS components must be carefully designed. Real-world implementation and stress-testing are needed to validate theoretical security. Striking the optimal balance of security, decentralization and scalability with a hybrid model is an ongoing challenge.\\n\\nIn summary, while PoW is battle-hardened and secure, PoS offers superior energy efficiency and scalability. The choice involves nuanced tradeoffs based on a blockchain\\'s threat model, performance needs and sustainability priorities. Hybrid designs are an exciting area of research in the quest for an optimal consensus system. Rigorous testing of new models in practice will be critical.',\n",
              " 'input_ids': [2320,\n",
              "  25,\n",
              "  1446,\n",
              "  525,\n",
              "  458,\n",
              "  15235,\n",
              "  17847,\n",
              "  13,\n",
              "  39565,\n",
              "  264,\n",
              "  11682,\n",
              "  4226,\n",
              "  773,\n",
              "  1196,\n",
              "  1513,\n",
              "  944,\n",
              "  1184,\n",
              "  311,\n",
              "  2711,\n",
              "  4889,\n",
              "  311,\n",
              "  3535,\n",
              "  279,\n",
              "  4226,\n",
              "  13,\n",
              "  29051,\n",
              "  25,\n",
              "  5692,\n",
              "  374,\n",
              "  264,\n",
              "  11682,\n",
              "  11,\n",
              "  7299,\n",
              "  28037,\n",
              "  3405,\n",
              "  389,\n",
              "  264,\n",
              "  3151,\n",
              "  8544,\n",
              "  2878,\n",
              "  42885,\n",
              "  323,\n",
              "  43955,\n",
              "  70306,\n",
              "  24561,\n",
              "  1447,\n",
              "  15220,\n",
              "  13626,\n",
              "  23783,\n",
              "  525,\n",
              "  9023,\n",
              "  6813,\n",
              "  315,\n",
              "  17944,\n",
              "  5942,\n",
              "  429,\n",
              "  7283,\n",
              "  3922,\n",
              "  13026,\n",
              "  311,\n",
              "  5545,\n",
              "  9128,\n",
              "  389,\n",
              "  279,\n",
              "  1584,\n",
              "  315,\n",
              "  279,\n",
              "  4237,\n",
              "  46933,\n",
              "  304,\n",
              "  264,\n",
              "  6950,\n",
              "  1717,\n",
              "  11,\n",
              "  47963,\n",
              "  11566,\n",
              "  13,\n",
              "  36991,\n",
              "  8668,\n",
              "  12,\n",
              "  6776,\n",
              "  320,\n",
              "  32904,\n",
              "  54,\n",
              "  8,\n",
              "  323,\n",
              "  36991,\n",
              "  8668,\n",
              "  39620,\n",
              "  726,\n",
              "  320,\n",
              "  32904,\n",
              "  50,\n",
              "  8,\n",
              "  525,\n",
              "  1378,\n",
              "  13570,\n",
              "  1483,\n",
              "  23869,\n",
              "  25185,\n",
              "  382,\n",
              "  64,\n",
              "  8,\n",
              "  81917,\n",
              "  279,\n",
              "  15811,\n",
              "  11799,\n",
              "  1948,\n",
              "  13808,\n",
              "  54,\n",
              "  323,\n",
              "  13808,\n",
              "  50,\n",
              "  23869,\n",
              "  23783,\n",
              "  11,\n",
              "  2670,\n",
              "  862,\n",
              "  19511,\n",
              "  11364,\n",
              "  369,\n",
              "  68672,\n",
              "  14131,\n",
              "  323,\n",
              "  7842,\n",
              "  501,\n",
              "  10010,\n",
              "  311],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "tokenized_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2m-e-ivDn1A",
        "outputId": "c48eda2b-7a1f-4c6d-a13a-8e0846909209"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 19637\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'input_ids', 'attention_mask'],\n",
              "        num_rows: 2182\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHs-BnR_zd9C",
        "outputId": "5dc94e9a-b7bc-414c-fa7b-2803dabf84e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text', 'input_ids', 'attention_mask'],\n",
              "    num_rows: 19637\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "train_dataset = dataset['train']\n",
        "test_dataset = dataset['test']\n",
        "train_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "-CUZuEENF2mW",
        "outputId": "a0531cee-d504-4686-b561-612a8c3909c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                text  \\\n",
              "0  System:  Instruction: Here is a detailed, mult...   \n",
              "1  System:  Instruction: Here is a detailed, mult...   \n",
              "2  System:  Instruction: Here is a detailed, mult...   \n",
              "3  System:  Instruction: Here is a graduate level...   \n",
              "4  System:  Instruction: Here is a detailed, mult...   \n",
              "\n",
              "                                           input_ids  \\\n",
              "0  [2320, 25, 220, 29051, 25, 5692, 374, 264, 116...   \n",
              "1  [2320, 25, 220, 29051, 25, 5692, 374, 264, 116...   \n",
              "2  [2320, 25, 220, 29051, 25, 5692, 374, 264, 116...   \n",
              "3  [2320, 25, 220, 29051, 25, 5692, 374, 264, 190...   \n",
              "4  [2320, 25, 220, 29051, 25, 5692, 374, 264, 116...   \n",
              "\n",
              "                                      attention_mask  \n",
              "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
              "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9d4800d2-1dc7-4bbe-ad36-af2188023752\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>input_ids</th>\n",
              "      <th>attention_mask</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>System:  Instruction: Here is a detailed, mult...</td>\n",
              "      <td>[2320, 25, 220, 29051, 25, 5692, 374, 264, 116...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>System:  Instruction: Here is a detailed, mult...</td>\n",
              "      <td>[2320, 25, 220, 29051, 25, 5692, 374, 264, 116...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>System:  Instruction: Here is a detailed, mult...</td>\n",
              "      <td>[2320, 25, 220, 29051, 25, 5692, 374, 264, 116...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>System:  Instruction: Here is a graduate level...</td>\n",
              "      <td>[2320, 25, 220, 29051, 25, 5692, 374, 264, 190...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>System:  Instruction: Here is a detailed, mult...</td>\n",
              "      <td>[2320, 25, 220, 29051, 25, 5692, 374, 264, 116...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9d4800d2-1dc7-4bbe-ad36-af2188023752')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9d4800d2-1dc7-4bbe-ad36-af2188023752 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9d4800d2-1dc7-4bbe-ad36-af2188023752');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ef199a3b-8672-48b7-9650-aabcb904c24c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ef199a3b-8672-48b7-9650-aabcb904c24c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ef199a3b-8672-48b7-9650-aabcb904c24c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"train_dataset\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"System:  Instruction: Here is a detailed, multi-part question on an important, narrow topic in art history:\\n\\nAnalyze the influence of Japonisme on Post-Impressionist artists in late 19th century France. \\n\\na) Describe how ukiyo-e Japanese woodblock prints first became popular and widely circulated in Paris in the 1860s. What were the key characteristics and subject matter of these prints?\\n\\nb) Examine the specific influence of Japanese art on the work of three Post-Impressionist artists: Vincent van Gogh, Henri de Toulouse-Lautrec, and Paul Gauguin. Identify direct examples where compositional elements, perspective, or subject matter in their paintings or prints show clear inspiration from Japanese prints. \\n\\nc) Explain how the flat planes of color, bold outlines, and asymmetric compositions in Japanese art contributed to the Post-Impressionists rejecting the illusionism and realism of Western art tradition. How did studying Japanese aesthetics help catalyze their move towards abstraction and expressionism?\\n\\nd) Discuss how the context of 19th century colonialism and trade enabled this intercultural artistic exchange. What does this reveal about the globalization of art and the power dynamics between cultures during this colonial period? Use specific examples to support your argument. Response: Thank you for this fascinating and in-depth question about the influence of Japanese art on Post-Impressionist artists in late 19th century France. This is an important topic that reveals a lot about intercultural exchange, the globalization of art, and the political context of colonialism during this period. I'll do my best to address each part of your question:\\n\\na) Ukiyo-e Japanese woodblock prints first became popular in Paris in the 1860s through world expositions and art dealers. In 1867, Japan had a pavilion at the Paris World Exposition where ukiyo-e prints were displayed. Art collector and critic Philippe Burty coined the term \\\"Japonisme\\\" to describe the craze for Japanese art and design. Key characteristics of ukiyo-e prints included flat planes of bold colors, asymmetric compositions, truncated perspective, and everyday subject matter like actors, courtesans, landscapes, and scenes from Japanese folktales and history. \\n\\nb) Many Post-Impressionist artists avidly collected Japanese prints and incorporated elements into their work:\\n\\nVincent van Gogh copied ukiyo-e prints in oil, like Bridge in the Rain which closely mimics Hiroshige's Sudden Shower Over Shin-Ohashi Bridge. The bold outlines, flat color planes, and everyday subject matter in Japanese prints inspired Van Gogh's expressive, non-naturalistic use of color and pattern in works like The Bedroom.\\n\\nHenri de Toulouse-Lautrec was influenced by the elegant lines, flat color planes and snapshot-like compositions of ukiyo-e in his posters and prints of cabaret performers, like Jane Avril. The influence of Japanese visual culture can also be seen in the fashionable kimono worn by the subject of La Japonaise.\\n\\nPaul Gauguin's Vision After the Sermon shows the influence of Japanese prints in the flattened picture plane, bold patches of color, and placement of a tree branch overhead, a convention in Japanese landscape prints. His Tahitian paintings incorporate asymmetric compositions and patterning inspired by Southeast Asian textiles, reflecting the wider impact of colonialism on artists' exposure to non-Western aesthetics.\\n\\nc) Japanese woodblock prints offered Post-Impressionists a radically different aesthetic model from the illusionistic tradition of Western art since the Renaissance. The absence of single-point perspective, flat color planes, and decorative, anti-naturalistic use of line and pattern in ukiyo-e prints catalyzed the Post-Impressionist's move towards abstraction and more expressive, subjective approaches to representation. \\n\\nStudying Japanese art helped artists like Van Gogh break free of what he called \\\"the sweaty and coarse reality\\\" of Western academic painting. By rejecting modeling, chiaroscuro and linear perspective, Post-Impressionists could experiment with conveying inner visions and emotions through flattened, fragmented, dream-like compositions, advancing the transition to 20th century Modernism.\\n\\nd) The popularity of Japanese art in late 19th century France was enabled by the Meiji Restoration in 1868, when Japan opened up trade with the West after centuries of isolationist policy. This coincided with increased colonial expansion, as European powers competed for control over territories in Asia, Africa and the Pacific. \\n\\nThe craze for Japanese prints should be understood in the context of this colonial power imbalance. While Japanese art offered creative inspiration for European avant-gardes, it was consumed through an Orientalist lens as novel and exotic. The kimono-clad white woman in Toulouse-Lautrec's La Japonaise can be read as an allegory of France's colonial fascination with and desire to control Japan.\\n\\nThe unequal power dynamic of colonialism meant that influence only went one way - from Japan to the West. Japanese art was appreciated for its decorative beauty but not engaged with on its own cultural terms. Nonetheless, exposure to Japanese aesthetics via colonial trade routes did enable a dramatic cross-pollination of artistic ideas that shaped the birth of Modernism.\\n\\nIn conclusion, the influence of Japanese ukiyo-e woodblock prints on Post-Impressionist artists in late 19th century Paris is a fascinating case study of how intercultural contact zones created by colonialism led to new artistic forms, expanding the boundaries of Western art. At the same time, it's important to examine the political context of Japonisme as a craze enabled by and contributing to Orientalist attitudes in the age of empire. Nonetheless, the impact of Japanese aesthetics on 19th century French avant-gardes is a powerful demonstration of how artistic exchange can transcend national and cultural borders.\",\n          \"System:  Instruction: Here is a detailed, multi-part question on a narrow but important topic in Ecosystem Ecology:\\n\\nBiodiversity is critical to the health and functioning of ecosystems. Consider a hypothetical 100 hectare temperate forest ecosystem with 50 native plant species, 30 bird species, 20 mammal species, and hundreds of insect and microbe species.\\n\\na) Discuss the different types and levels of biodiversity present in this ecosystem (genetic diversity, species diversity, ecosystem diversity, alpha vs beta diversity). How does the biodiversity of this temperate forest likely compare to a tropical rainforest and to an agricultural monoculture?  \\n\\nb) Imagine an invasive insect pest is accidentally introduced that kills off the 5 most common tree species, which currently make up 70% of the forest canopy. Using specific ecological concepts and relationships, predict how this loss of species richness and evenness would impact the ecosystem's:\\n- Primary productivity \\n- Nutrient cycling\\n- Food web complexity\\n- Resilience to environmental changes and stresses\\n\\nc) The forest is then fragmented into smaller disconnected patches by human development. Discuss how this habitat fragmentation would affect biodiversity and species persistence, considering concepts like minimum viable population sizes, edge effects, metapopulation dynamics and island biogeography theory. \\n\\nd) Propose a detailed conservation plan to monitor and maximize the forest's biodiversity over the next 50 years, considering aspects like:\\n- Reforestation and restoration strategies \\n- Landscape connectivity via habitat corridors\\n- Approaches to make the ecosystem more resilient to climate change\\n- Strategies to monitor key indicator species\\n- Ways to factor in economic value and ecosystem services  \\n\\nInclude specific examples to justify your analyses and suggestions. Discuss how preserving the biodiversity of even small ecosystems is crucial to global ecological and human wellbeing. Response: Thank you for this fascinating and thought-provoking question that dives deep into crucial topics in ecosystem ecology and biodiversity conservation. I'll aim to address each part as thoroughly as I can.\\n\\na) Biodiversity levels and comparisons:\\nThis hypothetical temperate forest contains multiple levels and dimensions of biodiversity:\\n- Genetic diversity within each species, representing the variety of genes and traits present. \\n- Species diversity, with richness of 50 plant, 30 bird, 20 mammal and hundreds of insect/microbe species. There is likely moderate species evenness, with some dominant species but many others as well.\\n- Ecosystem diversity, with the forest containing varied habitats and niches (e.g. canopy, understory, forest floor, streams, etc.)\\n- High alpha (within-habitat) diversity but likely lower beta diversity (between habitats) than a more heterogeneous landscape.\\n\\nCompared to a tropical rainforest, this temperate forest likely has significantly lower species richness, especially among taxa like trees, epiphytes, insects and soil microbes. Rainforests are biodiversity hotspots, with the Amazon alone containing 16,000 tree species for instance. However, the temperate forest may have higher evenness than rainforests that are often dominated by a smaller portion of species.\\n\\nAn agricultural monoculture would have drastically lower diversity at all levels - with only one crop genome, one dominant plant species, highly simplified ecology, and almost no beta diversity across homogeneous fields. Monocultures are ecological deserts compared to natural forests.\\n\\nb) Ecological impacts of invasive pest and tree loss:\\nLosing the 5 most common tree species (70% of canopy) to an invasive insect would drastically alter the forest ecosystem and biodiversity:\\n\\n- Primary productivity would likely plummet in the short term. With 70% canopy loss, the forest would lose most of its photosynthetic capacity and carbon sequestration. Invasive insects can cause large productivity losses, like the emerald ash borer that has killed millions of ash trees in North America.\\n\\n- Nutrient cycling would be disrupted. Mass tree die-offs cause spikes in woody debris, leaf litter and root death. This would alter inputs to soil nutrient pools and carbon/nitrogen ratios. Decomposer communities and soil respiration would change. Leaching and nutrient loss may increase without live plant roots.\\n\\n- Food web complexity would decrease. Many specialist herbivores, pollinators, seed dispersers and fungi likely depend on the dominant trees. With their host plants gone, these species and their predators would disappear too, simplifying food chains. Generalists and invasive species may increase and food webs would re-arrange around the new dominant plants.\\n\\n- Resilience would be reduced. With 70% of the canopy gone, the forest has lost key species that drive its productivity, carbon storage, water retention, and environmental buffering. Lacking its foundational species, the forest would be more vulnerable to droughts, storms, pathogens, climate change and human impacts. Ecosystem resilience often depends on functional redundancy and response diversity across species - if too many are lost, resilience declines.\\n\\nc) Habitat fragmentation effects:\\nWhen the forest is fragmented into smaller isolated patches, multiple ecological dynamics threaten its biodiversity:\\n\\n- Smaller subpopulations in each patch are more vulnerable to extirpation from random chance and inbreeding. Most species require a minimum viable population size to persist. Fragmentation shrinks and isolates populations below this MVP threshold, causing local extinctions. \\n- Edge effects also increase. More forest patches are exposed to hot/dry/windy conditions, invasive species, human disturbance, predators and pathogens from outside. In fragmented forests, >50% of area can experience harmful edge effects. Species adapted to cool, humid interior conditions are especially vulnerable.\\n- Metapopulation dynamics become critical. The forest now exists as a network of patches, with species persistence depending on patch size, isolation and colonization rates between them. Species will go locally extinct in small patches but can re-colonize them from larger source populations. The long-term survival of species in fragmented landscapes often requires a metapopulation spread across many patches.  \\n- Island biogeography theory predicts that biodiversity in forest fragments will depend on their size and isolation. Smaller and more isolated patches will lose species faster than larger connected ones. Species-area relationships suggest that a 90% reduction in habitat area will result in a 50% loss of species over time. Rare species with large ranges are usually the first to disappear from fragments.\\n\\nd) Conservation plan for the forest:\\nTo maximize this temperate forest's biodiversity over the next 50 years, I propose the following strategies:\\n\\nPrioritize reforestation and restoration:\\n- Focus on replanting and restoring the 5 dominant native tree species lost to the invasive insect, to recover the forest's foundational species, canopy cover, productivity and resilience. \\n- Source genetically diverse and locally-adapted tree seedlings to maximize their survival and adaptability.\\n- Reforest in a way that recreates the complex forest structure, with multiple vegetation layers, standing dead trees, and diverse tree spacing and ages.\\n- Actively manage and exclude the invasive insect to allow successful tree re-establishment.\\n\\nMaintain landscape connectivity:  \\n- Identify the largest, most intact forest patches and prioritize them for protection and connectivity.  \\n- Create a network of wide habitat corridors between forest patches, with native vegetation and minimal edge-to-area ratios. Riparian corridors along streams are especially valuable.\\n- Corridors should be designed to facilitate dispersal and genetic exchange for the widest possible range of forest plants and animals - consider attributes like width, structure, composition and human disturbance.\\n- Encourage private landowners to restore or maintain forest patches and corridors outside protected areas as well, perhaps through policy incentives.\\n\\nIncrease ecosystem resilience to climate change:\\n- Manage for tree species and genotypes expected to be tolerant of warmer, drier future conditions. Plant more southerly genotypes of the dominant tree species during restoration.\\n- Maintain forest complexity, with diverse microclimates and niches to serve as refugia during climate extremes. Protect cool, moist areas like ravines and north-facing slopes.  \\n- Restore riparian zones and wetlands to retain water and buffer the wider forest during droughts.\\n- Conduct prescribed burns in fire-suppressed areas to reduce fuel loads and the risk of severe canopy fires.\\n- Minimize other stressors like pollution, pests, and soil damage that compound with climate change.\\n\\nMonitor indicator species:\\n- Select a manageable set of native indicator species that are sensitive to environmental changes and represent key ecological roles, such as: \\n1) Dominant canopy trees to track forest composition and health\\n2) Amphibians to indicate stream and wetland quality  \\n3) Insectivorous birds to reveal food web complexity\\n4) Large predators to assess landscape connectivity\\n5) Rare understory plants to monitor biodiversity losses\\n- Establish a rigorous, long-term monitoring program to track the details of indicator species' population sizes, distributions, reproduction, survival rates and genetic diversity across the fragmented forest network.\\n- Use monitoring results to identify problems early and adapt management strategies as needed.\\n\\nConsider ecosystem services and economic values:  \\n- Quantify the value of the forest's ecosystem services, such as carbon storage, water filtration, flood control, pollination, recreation and tourism. Use this to justify increased forest protection and management resources.\\n- Develop a sustainable forest products industry around selective logging, maple syrup, mushrooms, etc. to provide income without compromising forest health. \\n- Promote forest-based recreation and tourism to build public support and funding for conservation.\\n- Develop payment mechanisms for private landowners who protect and restore forest, funded by the beneficiaries of forest ecosystem services (carbon credits, water utilities, tourism taxes, etc.)\\n\\nIn conclusion, protecting the biodiversity of even small temperate forests is crucial because these ecosystems:\\n1) Are reservoirs of locally and globally important species, genetics and evolutionary potential. \\n2) Provide critical ecosystem services like carbon sequestration, water regulation and soil formation.\\n3) Support human health through clean air, water, food, medicine and mental health benefits.  \\n4) Contribute to regional and global biodiversity that underpins the stability and functioning of the entire biosphere we depend on.\\n\\nEven 100 hectare forest patches matter in a fragmented world where biodiversity is under threat from so many human pressures. We must think carefully and act intentionally to monitor, manage and maximize forest biodiversity at all scales. With strategic science-based actions like those outlined above, we can sustain diverse temperate forests and the benefits they provide for generations to come.\",\n          \"System:  Instruction: Here is a detailed, multi-part question on a specific topic within the syntax area of linguistics:\\n\\nConsider the following English sentences:\\n1) John is likely to win the race. \\n2) John is eager to please his parents.\\n\\nBoth sentences have the form \\\"John is X to VP\\\", where X is an adjective and VP is a verb phrase. However, there are important syntactic differences between the two sentences.\\n\\nPart A) Using syntactic tests and arguments based on the underlying phrase structure and movement rules, demonstrate that in sentence 1, \\\"John\\\" is the semantic SUBJECT of the infinitival complement clause \\\"to win the race\\\", while in sentence 2, \\\"John\\\" is NOT the semantic SUBJECT of \\\"to please his parents\\\". Specifically, provide at least 3 pieces of syntactic evidence that point to this crucial difference in the argument structure and thematic relations between the two sentences.\\n\\nPart B) Draw the detailed X-bar style phrase structure tree for each of the sentences, clearly labeling all the lexical and functional categories and showing the hierarchical structure. Explain how the differing syntactic positions of \\\"John\\\" in each tree represent the semantic differences outlined in part A. \\n\\nPart C) Using your arguments from part A and phrase structure representations from part B as the basis, propose an explicit formal syntactic analysis to capture the key differences between sentence 1 and 2. Your analysis should utilize precise technical notions from syntactic theory such as PRO, control, raising, theta-role assignment, and the argument structure of the adjectival predicates \\\"likely\\\" vs \\\"eager\\\". Justify your analysis.\\n\\nPart D) Discuss the broader implications of these contrasts between sentences like 1 and 2 for the theory of syntax and the syntax-semantics interface. What general conclusions can we draw about the mapping between surface syntactic positions and underlying thematic relations and predicate-argument structure? How do these data points inform the Universal Grammar hypothesis and our understanding of the abstract properties of human language syntax? Response: This is a great question that delves into some of the core issues in syntactic theory and the syntax-semantics interface. Let me address each part in turn:\\n\\nPart A) There are several key pieces of syntactic evidence that show that \\\"John\\\" is the semantic subject of the infinitival clause in sentence 1 but not sentence 2:\\n\\n1. Passivization: Sentence 1 can be passivized as \\\"The race is likely to be won by John\\\", where \\\"the race\\\" becomes the syntactic subject. This suggests \\\"the race\\\" is the underlying object of \\\"win\\\", which in turn implies \\\"John\\\" is the semantic subject of \\\"win\\\". In contrast, sentence 2 cannot be passivized in this way (*\\\"His parents are eager to be pleased by John\\\"), indicating \\\"John\\\" is not the semantic subject of \\\"please\\\".\\n\\n2. Reflexive binding: We can say \\\"John is likely to win the race himself\\\", where the reflexive \\\"himself\\\" is bound by \\\"John\\\". This suggests \\\"John\\\" c-commands \\\"himself\\\" and is the subject of the clause containing it. But we cannot say *\\\"John is eager to please his parents himself\\\" with \\\"himself\\\" bound by \\\"John\\\", because \\\"John\\\" does not c-command the infinitival clause.\\n\\n3. Semantic entailment: Sentence 1 entails that \\\"John will win the race\\\" is likely, i.e. John is the one doing the winning. But sentence 2 does not entail that John will please his parents; rather, John has the intention/desire for his parents to be pleased (by someone, not necessarily John). This points to \\\"John\\\" bearing the AGENT thematic role for \\\"win\\\" but not \\\"please\\\".\\n\\nPart B) The simplified phrase structure trees would be:\\n\\nSentence 1:\\n[TP [DP John] [VP is [AP likely [TP [DP PRO] to [VP win the race]]]]]\\n\\nSentence 2: \\n[TP [DP John] [VP is [AP eager [CP [TP PRO to [VP please his parents]]]]]]\\n\\nThe key difference is that in sentence 1, the DP \\\"John\\\" raises from being the semantic subject of \\\"win\\\" (where it originates in the embedded [Spec,TP] position) to the matrix [Spec,TP] to become the syntactic subject of the entire sentence. Whereas in sentence 2, \\\"John\\\" is the semantic subject of \\\"eager\\\" but not \\\"please\\\". The embedded [Spec,TP] subject of \\\"please\\\" is occupied by the null PRO (which refers back to \\\"John\\\" as the controller).\\n\\nPart C) We can propose the following analysis:\\n- \\\"Likely\\\" is a raising predicate that does not assign an external theta-role, so its semantic subject position ([Spec,TP] of the infinitival clause) is not theta-marked. This allows the DP \\\"John\\\" to raise from there to the main clause subject position to satisfy the Extended Projection Principle (EPP).\\n- \\\"Eager\\\" is a control predicate that assigns an external theta-role (EXPERIENCER) to \\\"John\\\" in the main clause. The subject of the infinitival clause is PRO, which is controlled by \\\"John\\\" and receives a distinct theta-role (AGENT) from the embedded predicate \\\"please\\\". \\n- Raising predicates like \\\"likely\\\" thus allow cross-clausal A-movement, while control predicates like \\\"eager\\\" involve two separate theta-role assignments linked by an anaphoric dependency.\\n\\nPart D) These contrasts highlight the complex mapping between syntactic argument positions and semantic thematic relations. The same surface string \\\"John is X to VP\\\" can correspond to very different argument structures depending on the properties of X. This shows that we need abstract syntactic notions like PRO and movement to capture the underlying structure and derivation behind the surface form.\\n\\nMore generally, this supports the idea in generative syntax that semantic predicate-argument relationships are determined by syntactic configurations at an underlying level (D-structure), and surface forms are derived by movement operations. The Universal Grammar hypothesis holds that the ability to construct such abstract syntactic representations with embedded clauses and null categories is part of the innate human language faculty.\\n\\nAt the same time, these phenomena also show the close interplay between syntax and semantics - theta-roles originate from the lexical semantics of individual predicates like \\\"likely\\\" vs \\\"eager\\\", which in turn impacts the syntactic structure and derivation. The argument structure is not purely syntactic but is semantically grounded.\\n\\nSo in summary, while the surface similarity of sentences 1 and 2 might suggest a simple and uniform mapping from syntactic positions to thematic relations, the underlying distinctions revealed by syntactic tests point to a more nuanced and abstract system of argument structure realization. Unpacking these distinctions requires us to posit a rich syntactic architecture with movement and empty categories that interface with lexical semantic factors. This is the kind of abstract knowledge that linguists argue must be innate to allow children to acquire these subtle contrasts from limited input.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_ids\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attention_mask\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "train_dataset.select(range(5)).to_pandas().head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PxxrK5Rd4gk",
        "outputId": "8a9ff3e5-5d30-4ed1-f334-6d39e3eeaf7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'text': 'System:  Instruction: Here is a detailed, multi-part question on an important topic in normative ethics:\\n\\nOne of the central challenges in normative ethics is reconciling the tension between agent-neutral and agent-relative moral reasons. \\n\\nPart 1: Explain the key differences between agent-neutral and agent-relative moral reasons, using specific examples to illustrate each category. Discuss how consequentialist moral theories tend to prioritize agent-neutral reasons, while deontological theories give more weight to agent-relative reasons.\\n\\nPart 2: Critically evaluate the plausibility and implications of ethical egoism, the view that moral agents ought to do what is in their own self-interest. Does ethical egoism represent a coherent and defensible moral theory? Or does it fail to properly incorporate agent-neutral moral considerations? Explain your reasoning.\\n\\nPart 3: Consider the demandingness objection often posed against consequentialist theories - the idea that maximizing overall utility or well-being would require extreme self-sacrifice on the part of moral agents. Does commonsense morality recognize a general duty of beneficence (helping others), but place limits on the sacrifice it requires of agents? How might a consequentialist respond to this objection? \\n\\nPart 4: Imagine that a trolley is headed toward five innocent people who will be killed if it proceeds on its present course. You are standing next to a large stranger on a footbridge above the tracks, in between the approaching trolley and the five people. The only way to save the lives of the five people is to push this stranger off the bridge and onto the tracks below where his body will stop the trolley. He will die if you do this, but the five others will be saved.\\n\\nAnalyze this case from the perspectives of both consequentialism and deontology. Is it morally permissible or required to push the man to his death in order to save a greater number? Or would doing so violate a fundamental duty not to use someone merely as a means? Justify your answer with reference to the deeper theoretical commitments of each view. Response: Thank you for this thought-provoking multi-part question on normative ethics. This touches on some core issues and tensions in moral philosophy. I will do my best to address each part in turn.\\n\\nPart 1: Agent-neutral reasons are considerations that apply impartially to all moral agents. They are reasons that any moral agent has to promote certain outcomes or states of affairs, regardless of their relation to that outcome. For example, there are arguably agent-neutral reasons to reduce suffering or promote well-being in the world. These reasons are independent of whether it is my suffering or a stranger\\'s. \\n\\nIn contrast, agent-relative reasons are considerations that apply differently to an agent based on their specific relation or role. Examples include special obligations we have to family, friends, or those to whom we\\'ve made promises. I have stronger reason to help my child than a stranger\\'s child, even if the needs are equivalent.\\n\\nConsequentialist theories like utilitarianism tend to focus on agent-neutral reasons - impartially maximizing overall utility or well-being. Deontological theories give more weight to agent-relative duties and permissions.\\n\\nPart 2: Ethical egoism is not a widely accepted moral theory, as most consider impartial, agent-neutral considerations to be central to morality. Commonsense notions of morality generally hold that we have duties to help others and not harm them, even when doing so doesn\\'t benefit ourselves. \\n\\nThat said, one could argue that ethical egoism is at least a coherent view - there\\'s no outright logical contradiction in holding that one ought to maximize their own interest. And rational egoism seems defensible as a theory of practical rationality, even if not as a moral theory. Additionally, evolutionary arguments may suggest that some degree of self-interest has been selected for.\\n\\nUltimately though, I don\\'t find ethical egoism plausible as a moral theory. It fails to capture core moral notions of impartiality, altruism and concern for others inherent to commonsense morality. An ethical egoist would have to reject that we have any unchosen positive duties to help others which conflict with self-interest.\\n\\nPart 3: The demandingness objection poses a serious challenge to maximizing consequentialist theories. Commonsense morality does seem to recognize positive duties of aid/beneficence, but places intuitively plausible limits on the sacrifice demanded of agents. A theory that requires you to donate all your discretionary income to highly effective global poverty charities, for instance, conflicts with widespread considered judgments.  \\n\\nAt the same time, it\\'s hard to deny the force of consequentialist considerations pushing us to expand the circle of moral concern. From an impartial point of view, there\\'s a strong case that relatively affluent people are morally obligated to make substantial sacrifices to help the global poor, even if this exceeds the requirements of commonsense morality.\\n\\nSome have argued for a \"moderate\" consequentialism that places an upper bound on the sacrifices demanded of agents. But this risks being theoretically ad hoc if not grounded in a principled rationale. Alternatively, one could argue that a sophisticated consequentialism would generally produce intuitive results, because an agent\\'s life-long contributions must be considered, extreme self-sacrifice is not sustainable, and act consequentialism must be constrained by rules and rights.\\n\\nThere is no easy resolution, but I\\'m somewhat compelled by the position that commonsense moral intuitions about demandingness should be taken seriously as considered judgments, even if they must be refined and expanded by impartial consequentialist thinking. A plausible view may be that we have robust duties of beneficence, even if not as extreme as a simple act-utilitarianism would require.\\n\\nPart 4: From a straightforward act-consequentialist view, pushing the man onto the track seems not only permitted but required, as it produces the best overall consequences (five lives saved vs. one lost). This aligns with simple utilitarian calculation.\\n\\nIn contrast, deontological theories which place great weight on inviolable individual rights would likely prohibit pushing the man. Doing so uses him merely as a means to save the others, and violates a duty not to harm an innocent person. It treats him as an object rather than an inviolable subject of moral worth.\\n\\nThis case illustrates how deontology sometimes requires that we not violate core duties even when doing so would produce better consequences. Here the prohibition on using someone as a mere means trumps the obligation to save a greater number.\\n\\nThat said, even some who find deontological considerations powerful may support pushing the man here. One could argue that the man\\'s right not to be killed must be weighed against the five\\'s right not to be killed, and that one should minimize rights violations by saving the greater number. \\n\\nAdditionally, some argue for a \"threshold deontology\" on which deontic prohibitions can be overridden if the consequences are sufficiently dire. Saving five lives at the cost of one might meet such an extreme threshold.\\n\\nMy own considered judgment is that pushing the man is very likely wrong, as it crosses a fundamental line of using someone as a mere means and strips them of inviolable dignity. It is difficult to accept the idea that someone\\'s basic rights can be involuntarily overridden and sacrificed for the greater good, even if I feel the pull of the utilitarian arguments.\\n\\nThis reflects my view that both agent-neutral consequentialist considerations and agent-relative deontological constraints are indispensable to a convincing moral framework, and that difficult cases arise when they conflict. In many situations, respect for individual rights/agency and prohibitions on harming can override obligations to help or bring about better outcomes. But these are enormously complex issues that have inspired much debate.\\n\\nI hope this analysis of the key issues in each part is helpful for thinking through these challenging questions! Let me know if you would like me to clarify or expand on any part of the response.',\n",
              " 'input_ids': [2320,\n",
              "  25,\n",
              "  220,\n",
              "  29051,\n",
              "  25,\n",
              "  5692,\n",
              "  374,\n",
              "  264,\n",
              "  11682,\n",
              "  11,\n",
              "  7299,\n",
              "  28037,\n",
              "  3405,\n",
              "  389,\n",
              "  458,\n",
              "  2989,\n",
              "  8544,\n",
              "  304,\n",
              "  7465,\n",
              "  1388,\n",
              "  30908,\n",
              "  1447,\n",
              "  3966,\n",
              "  315,\n",
              "  279,\n",
              "  8622,\n",
              "  11513,\n",
              "  304,\n",
              "  7465,\n",
              "  1388,\n",
              "  30908,\n",
              "  374,\n",
              "  31445,\n",
              "  7979,\n",
              "  279,\n",
              "  23504,\n",
              "  1948,\n",
              "  8315,\n",
              "  91222,\n",
              "  323,\n",
              "  8315,\n",
              "  75706,\n",
              "  15659,\n",
              "  7966,\n",
              "  13,\n",
              "  4710,\n",
              "  5800,\n",
              "  220,\n",
              "  16,\n",
              "  25,\n",
              "  81917,\n",
              "  279,\n",
              "  1376,\n",
              "  11799,\n",
              "  1948,\n",
              "  8315,\n",
              "  91222,\n",
              "  323,\n",
              "  8315,\n",
              "  75706,\n",
              "  15659,\n",
              "  7966,\n",
              "  11,\n",
              "  1667,\n",
              "  3151,\n",
              "  10295,\n",
              "  311,\n",
              "  40368,\n",
              "  1817,\n",
              "  5582,\n",
              "  13,\n",
              "  65279,\n",
              "  1246,\n",
              "  90083,\n",
              "  380,\n",
              "  15659,\n",
              "  24970,\n",
              "  8376,\n",
              "  311,\n",
              "  62552,\n",
              "  8315,\n",
              "  91222,\n",
              "  7966,\n",
              "  11,\n",
              "  1393,\n",
              "  409,\n",
              "  544,\n",
              "  5729,\n",
              "  24970,\n",
              "  2968,\n",
              "  803,\n",
              "  4680,\n",
              "  311,\n",
              "  8315,\n",
              "  75706,\n",
              "  7966,\n",
              "  382,\n",
              "  5800,\n",
              "  220,\n",
              "  17,\n",
              "  25,\n",
              "  33207,\n",
              "  2673,\n",
              "  15442,\n",
              "  279,\n",
              "  625,\n",
              "  11855,\n",
              "  3147,\n",
              "  323,\n",
              "  24154,\n",
              "  315,\n",
              "  30208,\n",
              "  36274,\n",
              "  2142,\n",
              "  11,\n",
              "  279,\n",
              "  1651,\n",
              "  429,\n",
              "  15659,\n",
              "  13009,\n",
              "  21776,\n",
              "  311,\n",
              "  653,\n",
              "  1128,\n",
              "  374,\n",
              "  304,\n",
              "  862,\n",
              "  1828],\n",
              " 'attention_mask': [1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1,\n",
              "  1]}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFX4u0vc0UkS"
      },
      "source": [
        "## Training Set Up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "F-mkiTYw0cZi"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n",
        "#data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wzNdWpCI0c7a"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(p: EvalPrediction):\n",
        "  preds = np.argmax(p.predictions, axis = 1)\n",
        "  precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    p.label_ids,\n",
        "    preds,\n",
        "    average = 'weighted'\n",
        "  )\n",
        "  matrix = {\n",
        "    'accuracy': accuracy_score(p.label_ids, preds),\n",
        "    'f1': f1, 'precision': precision,\n",
        "    'recall': recall\n",
        "  }\n",
        "  return matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ec3lVkPY1BZH"
      },
      "outputs": [],
      "source": [
        "model.config.pad_token_id = tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "tEkgHY4fxFIJ"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVr-LToX1XCl"
      },
      "source": [
        "## Training Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikF6Yfkz1myd",
        "outputId": "1acd8d7f-579d-47c2-a39c-f39d4c83b294"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(151936, 896)\n",
              "    (layers): ModuleList(\n",
              "      (0-23): 24 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2SdpaAttention(\n",
              "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
              "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
              "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
              "          (rotary_emb): Qwen2RotaryEmbedding()\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
              "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhliEMyp1thd",
        "outputId": "497f89dc-9d33-47e8-dc9e-cff716600102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total parameters : 494032768\n",
            "Trainable parameters : 494032768\n",
            "Trainable percentage: 100.00%\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print('Total parameters :', total_params)\n",
        "print('Trainable parameters :', trainable_params)\n",
        "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Xn5zb6xWJtu-"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93ffvb0d4cG6",
        "outputId": "a6c98dd8-3adc-4dff-afa6-6f8b34d26ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainingArguments(\n",
              "_n_gpu=1,\n",
              "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\n",
              "adafactor=False,\n",
              "adam_beta1=0.9,\n",
              "adam_beta2=0.999,\n",
              "adam_epsilon=1e-08,\n",
              "auto_find_batch_size=False,\n",
              "batch_eval_metrics=False,\n",
              "bf16=False,\n",
              "bf16_full_eval=False,\n",
              "data_seed=None,\n",
              "dataloader_drop_last=False,\n",
              "dataloader_num_workers=0,\n",
              "dataloader_persistent_workers=False,\n",
              "dataloader_pin_memory=True,\n",
              "dataloader_prefetch_factor=None,\n",
              "ddp_backend=None,\n",
              "ddp_broadcast_buffers=None,\n",
              "ddp_bucket_cap_mb=None,\n",
              "ddp_find_unused_parameters=None,\n",
              "ddp_timeout=1800,\n",
              "debug=[],\n",
              "deepspeed=None,\n",
              "disable_tqdm=False,\n",
              "dispatch_batches=None,\n",
              "do_eval=True,\n",
              "do_predict=False,\n",
              "do_train=False,\n",
              "eval_accumulation_steps=None,\n",
              "eval_delay=0,\n",
              "eval_do_concat_batches=True,\n",
              "eval_on_start=False,\n",
              "eval_steps=50,\n",
              "eval_strategy=steps,\n",
              "eval_use_gather_object=False,\n",
              "evaluation_strategy=steps,\n",
              "fp16=False,\n",
              "fp16_backend=auto,\n",
              "fp16_full_eval=False,\n",
              "fp16_opt_level=O1,\n",
              "fsdp=[],\n",
              "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
              "fsdp_min_num_params=0,\n",
              "fsdp_transformer_layer_cls_to_wrap=None,\n",
              "full_determinism=False,\n",
              "gradient_accumulation_steps=8,\n",
              "gradient_checkpointing=False,\n",
              "gradient_checkpointing_kwargs=None,\n",
              "greater_is_better=None,\n",
              "group_by_length=False,\n",
              "half_precision_backend=auto,\n",
              "hub_always_push=False,\n",
              "hub_model_id=None,\n",
              "hub_private_repo=False,\n",
              "hub_strategy=every_save,\n",
              "hub_token=<HUB_TOKEN>,\n",
              "ignore_data_skip=False,\n",
              "include_inputs_for_metrics=False,\n",
              "include_num_input_tokens_seen=False,\n",
              "include_tokens_per_second=False,\n",
              "jit_mode_eval=False,\n",
              "label_names=None,\n",
              "label_smoothing_factor=0.0,\n",
              "learning_rate=2e-05,\n",
              "length_column_name=length,\n",
              "load_best_model_at_end=False,\n",
              "local_rank=0,\n",
              "log_level=debug,\n",
              "log_level_replica=warning,\n",
              "log_on_each_node=True,\n",
              "logging_dir=./model/runs/Oct31_06-26-42_89e1e51c1f40,\n",
              "logging_first_step=False,\n",
              "logging_nan_inf_filter=True,\n",
              "logging_steps=50,\n",
              "logging_strategy=steps,\n",
              "lr_scheduler_kwargs={},\n",
              "lr_scheduler_type=cosine,\n",
              "max_grad_norm=1.0,\n",
              "max_steps=1000,\n",
              "metric_for_best_model=None,\n",
              "mp_parameters=,\n",
              "neftune_noise_alpha=None,\n",
              "no_cuda=False,\n",
              "num_train_epochs=3.0,\n",
              "optim=adamw_torch,\n",
              "optim_args=None,\n",
              "optim_target_modules=None,\n",
              "output_dir=./model,\n",
              "overwrite_output_dir=False,\n",
              "past_index=-1,\n",
              "per_device_eval_batch_size=8,\n",
              "per_device_train_batch_size=8,\n",
              "prediction_loss_only=False,\n",
              "push_to_hub=False,\n",
              "push_to_hub_model_id=None,\n",
              "push_to_hub_organization=None,\n",
              "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
              "ray_scope=last,\n",
              "remove_unused_columns=True,\n",
              "report_to=['tensorboard', 'wandb'],\n",
              "restore_callback_states_from_checkpoint=False,\n",
              "resume_from_checkpoint=None,\n",
              "run_name=./model,\n",
              "save_on_each_node=False,\n",
              "save_only_model=False,\n",
              "save_safetensors=False,\n",
              "save_steps=500,\n",
              "save_strategy=no,\n",
              "save_total_limit=2,\n",
              "seed=42,\n",
              "skip_memory_metrics=True,\n",
              "split_batches=None,\n",
              "tf32=None,\n",
              "torch_compile=False,\n",
              "torch_compile_backend=None,\n",
              "torch_compile_mode=None,\n",
              "torch_empty_cache_steps=None,\n",
              "torchdynamo=None,\n",
              "tpu_metrics_debug=False,\n",
              "tpu_num_cores=None,\n",
              "use_cpu=False,\n",
              "use_ipex=False,\n",
              "use_legacy_prediction_loop=False,\n",
              "use_mps_device=False,\n",
              "warmup_ratio=0.0,\n",
              "warmup_steps=30,\n",
              "weight_decay=0.0,\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "save_path = './model'\n",
        "\n",
        "batch_size = 8\n",
        "max_steps = 1000\n",
        "training_args = TrainingArguments(\n",
        "  output_dir = save_path,\n",
        "  gradient_accumulation_steps = batch_size,\n",
        "  evaluation_strategy = 'steps',\n",
        "  do_eval = True,\n",
        "  per_device_train_batch_size = batch_size,\n",
        "  per_device_eval_batch_size = batch_size,\n",
        "  log_level = 'debug',\n",
        "  save_strategy = 'no',\n",
        "  save_total_limit = 2,\n",
        "  save_safetensors = False,\n",
        "  fp16 = False,\n",
        "  logging_steps = 50,\n",
        "  learning_rate = 2e-5,\n",
        "  eval_steps = 50,\n",
        "  max_steps = max_steps,\n",
        "  warmup_steps = 30,\n",
        "  lr_scheduler_type = 'cosine',\n",
        ")\n",
        "training_args"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "  model = model,\n",
        "  train_dataset = train_dataset.select(range(10000)),\n",
        "  eval_dataset = test_dataset.select(range(1000)),\n",
        "  dataset_text_field = 'text',\n",
        "  max_seq_length = max_length,\n",
        "  tokenizer = tokenizer,\n",
        "  args = training_args,\n",
        ")\n",
        "trainer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsKeJE3SMdk7",
        "outputId": "12d5f790-edbd-44a3-c2c0-4098d18cfe03"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
            "\n",
            "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
            "  warnings.warn(message, FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<trl.trainer.sft_trainer.SFTTrainer at 0x7f90bffb3340>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MZVoQX8V1cI3",
        "outputId": "942a7b17-cb6e-4126-fcff-d7dd1494086b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Currently training with a batch size of: 8\n",
            "The following columns in the training set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "***** Running training *****\n",
            "  Num examples = 10,000\n",
            "  Num Epochs = 7\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 1,000\n",
            "  Number of trainable parameters = 494,032,768\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterlupakan100\u001b[0m (\u001b[33mterlupakan100-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.18.5"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241031_062647-k9njsol2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/k9njsol2' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/k9njsol2' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/k9njsol2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1000/1000 33:57, Epoch 6/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>8654190018.559999</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1000, training_loss=432709500.928, metrics={'train_runtime': 2046.6048, 'train_samples_per_second': 31.271, 'train_steps_per_second': 0.489, 'total_flos': 1.7591408001024e+16, 'train_loss': 432709500.928, 'epoch': 6.4})"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5N6fZsU1xiG"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "5d6DT3o0113O",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "a03a8a43-1b65-4fc0-e70a-166354d631a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following columns in the evaluation set don't have a corresponding argument in `Qwen2ForCausalLM.forward` and have been ignored: text. If text are not expected by `Qwen2ForCausalLM.forward`,  you can safely ignore this message.\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 1000\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 00:08]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': nan, 'eval_runtime': 8.9228, 'eval_samples_per_second': 112.072, 'eval_steps_per_second': 14.009, 'epoch': 6.4}\n"
          ]
        }
      ],
      "source": [
        "evaluation_results = trainer.evaluate()\n",
        "print('Evaluation Results:', evaluation_results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjTPWhCj4JQj"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "OKAmko8h2VeV"
      },
      "outputs": [],
      "source": [
        "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
        "#save_model.save_pretrained(save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyN6jf3zsUKfCGhEKB7CTpFV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}