{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-14T01:41:01.381225Z","iopub.execute_input":"2024-12-14T01:41:01.381661Z","iopub.status.idle":"2024-12-14T01:41:57.717372Z","shell.execute_reply.started":"2024-12-14T01:41:01.381617Z","shell.execute_reply":"2024-12-14T01:41:57.716040Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntrl 0.12.2 requires transformers<4.47.0, but you have transformers 4.47.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-12-14T01:41:57.719960Z","iopub.execute_input":"2024-12-14T01:41:57.720398Z","iopub.status.idle":"2024-12-14T01:42:07.138153Z","shell.execute_reply.started":"2024-12-14T01:41:57.720353Z","shell.execute_reply":"2024-12-14T01:42:07.137319Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-12-14T01:42:07.139577Z","iopub.execute_input":"2024-12-14T01:42:07.140460Z","iopub.status.idle":"2024-12-14T01:42:07.147081Z","shell.execute_reply.started":"2024-12-14T01:42:07.140409Z","shell.execute_reply":"2024-12-14T01:42:07.146237Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"model_name = 'unsloth/Hermes-2-Pro-Mistral-7B'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-14T01:42:07.148196Z","iopub.execute_input":"2024-12-14T01:42:07.148541Z","iopub.status.idle":"2024-12-14T01:42:07.167026Z","shell.execute_reply.started":"2024-12-14T01:42:07.148512Z","shell.execute_reply":"2024-12-14T01:42:07.166287Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-12-14T01:42:07.169592Z","iopub.execute_input":"2024-12-14T01:42:07.170347Z","iopub.status.idle":"2024-12-14T01:42:07.178723Z","shell.execute_reply.started":"2024-12-14T01:42:07.170304Z","shell.execute_reply":"2024-12-14T01:42:07.177995Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:07.179769Z","iopub.execute_input":"2024-12-14T01:42:07.180478Z","iopub.status.idle":"2024-12-14T01:42:24.374031Z","shell.execute_reply.started":"2024-12-14T01:42:07.180437Z","shell.execute_reply":"2024-12-14T01:42:24.373180Z"}},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5108fe06f86e451e9cc65c8bc03a014c"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32032, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=4096, out_features=32032, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:24.375015Z","iopub.execute_input":"2024-12-14T01:42:24.375260Z","iopub.status.idle":"2024-12-14T01:42:24.382669Z","shell.execute_reply.started":"2024-12-14T01:42:24.375236Z","shell.execute_reply":"2024-12-14T01:42:24.381845Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 3752333312\nTrainable parameters : 262672384\nTrainable percentage: 7.00%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:24.383693Z","iopub.execute_input":"2024-12-14T01:42:24.383952Z","iopub.status.idle":"2024-12-14T01:42:25.198072Z","shell.execute_reply.started":"2024-12-14T01:42:24.383928Z","shell.execute_reply":"2024-12-14T01:42:25.197019Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"dataset_name = 'microsoft/orca-math-word-problems-200k'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:25.199343Z","iopub.execute_input":"2024-12-14T01:42:25.199646Z","iopub.status.idle":"2024-12-14T01:42:25.204068Z","shell.execute_reply.started":"2024-12-14T01:42:25.199618Z","shell.execute_reply":"2024-12-14T01:42:25.203051Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 384","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:25.205130Z","iopub.execute_input":"2024-12-14T01:42:25.205432Z","iopub.status.idle":"2024-12-14T01:42:25.214534Z","shell.execute_reply.started":"2024-12-14T01:42:25.205403Z","shell.execute_reply":"2024-12-14T01:42:25.213844Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:25.215827Z","iopub.execute_input":"2024-12-14T01:42:25.216358Z","iopub.status.idle":"2024-12-14T01:42:26.749883Z","shell.execute_reply.started":"2024-12-14T01:42:25.216318Z","shell.execute_reply":"2024-12-14T01:42:26.749069Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'answer'],\n    num_rows: 200035\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.750820Z","iopub.execute_input":"2024-12-14T01:42:26.751079Z","iopub.status.idle":"2024-12-14T01:42:26.757092Z","shell.execute_reply.started":"2024-12-14T01:42:26.751054Z","shell.execute_reply":"2024-12-14T01:42:26.756319Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.758252Z","iopub.execute_input":"2024-12-14T01:42:26.758998Z","iopub.status.idle":"2024-12-14T01:42:26.779216Z","shell.execute_reply.started":"2024-12-14T01:42:26.758955Z","shell.execute_reply":"2024-12-14T01:42:26.778515Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.783900Z","iopub.execute_input":"2024-12-14T01:42:26.784194Z","iopub.status.idle":"2024-12-14T01:42:26.789976Z","shell.execute_reply.started":"2024-12-14T01:42:26.784168Z","shell.execute_reply":"2024-12-14T01:42:26.789062Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.790825Z","iopub.execute_input":"2024-12-14T01:42:26.791031Z","iopub.status.idle":"2024-12-14T01:42:26.802583Z","shell.execute_reply.started":"2024-12-14T01:42:26.791010Z","shell.execute_reply":"2024-12-14T01:42:26.801770Z"}},"outputs":[{"name":"stdout","text":"['question', 'answer']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.803633Z","iopub.execute_input":"2024-12-14T01:42:26.804201Z","iopub.status.idle":"2024-12-14T01:42:26.815960Z","shell.execute_reply.started":"2024-12-14T01:42:26.804160Z","shell.execute_reply":"2024-12-14T01:42:26.815107Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  input = examples['question']\n  output = examples['answer']\n  \n  text = prompt_format.format(input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"id":"0wXJNFBWWNYP","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.816749Z","iopub.execute_input":"2024-12-14T01:42:26.816974Z","iopub.status.idle":"2024-12-14T01:42:26.829479Z","shell.execute_reply.started":"2024-12-14T01:42:26.816952Z","shell.execute_reply":"2024-12-14T01:42:26.828678Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.830516Z","iopub.execute_input":"2024-12-14T01:42:26.830770Z","iopub.status.idle":"2024-12-14T01:42:26.846295Z","shell.execute_reply.started":"2024-12-14T01:42:26.830746Z","shell.execute_reply":"2024-12-14T01:42:26.845518Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"Kidf8H5zefDC","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.847308Z","iopub.execute_input":"2024-12-14T01:42:26.847559Z","iopub.status.idle":"2024-12-14T01:42:26.856418Z","shell.execute_reply.started":"2024-12-14T01:42:26.847535Z","shell.execute_reply":"2024-12-14T01:42:26.855585Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.857336Z","iopub.execute_input":"2024-12-14T01:42:26.857586Z","iopub.status.idle":"2024-12-14T01:42:26.866340Z","shell.execute_reply.started":"2024-12-14T01:42:26.857554Z","shell.execute_reply":"2024-12-14T01:42:26.865563Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.867358Z","iopub.execute_input":"2024-12-14T01:42:26.867618Z","iopub.status.idle":"2024-12-14T01:42:26.906927Z","shell.execute_reply.started":"2024-12-14T01:42:26.867594Z","shell.execute_reply":"2024-12-14T01:42:26.906191Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.907892Z","iopub.execute_input":"2024-12-14T01:42:26.908142Z","iopub.status.idle":"2024-12-14T01:42:26.916233Z","shell.execute_reply.started":"2024-12-14T01:42:26.908119Z","shell.execute_reply":"2024-12-14T01:42:26.915377Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.917259Z","iopub.execute_input":"2024-12-14T01:42:26.917521Z","iopub.status.idle":"2024-12-14T01:42:26.931864Z","shell.execute_reply.started":"2024-12-14T01:42:26.917497Z","shell.execute_reply":"2024-12-14T01:42:26.931028Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.932819Z","iopub.execute_input":"2024-12-14T01:42:26.933034Z","iopub.status.idle":"2024-12-14T01:42:26.944123Z","shell.execute_reply.started":"2024-12-14T01:42:26.933012Z","shell.execute_reply":"2024-12-14T01:42:26.943460Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.944979Z","iopub.execute_input":"2024-12-14T01:42:26.945220Z","iopub.status.idle":"2024-12-14T01:42:26.967614Z","shell.execute_reply.started":"2024-12-14T01:42:26.945196Z","shell.execute_reply":"2024-12-14T01:42:26.966803Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  ### Question:\\nThere is a two-digit natural nu...   \n1  ### Question:\\nIn a big box, there are marbles...   \n2  ### Question:\\nAdam goes to a small school, wh...   \n3  ### Question:\\nLisa is looking to attempt a Wo...   \n4  ### Question:\\nThere is a rectangular-shaped p...   \n\n                                           input_ids  \\\n0  [1, 774, 22478, 28747, 13, 5816, 349, 264, 989...   \n1  [1, 774, 22478, 28747, 13, 657, 264, 2032, 389...   \n2  [1, 774, 22478, 28747, 13, 3261, 314, 4859, 29...   \n3  [1, 774, 22478, 28747, 13, 28758, 7682, 349, 2...   \n4  [1, 774, 22478, 28747, 13, 5816, 349, 264, 971...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nThere is a two-digit natural nu...</td>\n      <td>[1, 774, 22478, 28747, 13, 5816, 349, 264, 989...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nIn a big box, there are marbles...</td>\n      <td>[1, 774, 22478, 28747, 13, 657, 264, 2032, 389...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nAdam goes to a small school, wh...</td>\n      <td>[1, 774, 22478, 28747, 13, 3261, 314, 4859, 29...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nLisa is looking to attempt a Wo...</td>\n      <td>[1, 774, 22478, 28747, 13, 28758, 7682, 349, 2...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere is a rectangular-shaped p...</td>\n      <td>[1, 774, 22478, 28747, 13, 5816, 349, 264, 971...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.968587Z","iopub.execute_input":"2024-12-14T01:42:26.968831Z","iopub.status.idle":"2024-12-14T01:42:26.973401Z","shell.execute_reply.started":"2024-12-14T01:42:26.968807Z","shell.execute_reply":"2024-12-14T01:42:26.972628Z"}},"outputs":[{"name":"stdout","text":"### Question:\nThere is a two-digit natural number whose tens place is 3. Let A and B be the quotient of this number by 10 and the remainder of division by 10, respectively. If B multiplied by 10 plus A is 9 less than A multiplied by 10 plus B, what is the first number?\n### Answer:\nLet's denote the two-digit number as \\( XY \\), where \\( X \\) is the digit in the tens place and \\( Y \\) is the digit in the ones place. Since the tens place is 3, we have \\( X = 3 \\).\n\nAccording to the problem, \\( A \\) is the quotient of the number by 10, and \\( B \\) is the remainder of the division by 10. Therefore, \\( A = X = 3 \\) and \\( B = Y \\).\n\nThe problem states that \\( B \\times 10 + A \\) is 9 less than \\( A \\times 10 + B \\). This can be written as an equation:\n\n\\[ B \\times 10 + A = A \\times 10 + B - 9 \\]\n\nSubstituting \\( A \\) and \\( B \\) with \\( 3 \\) and \\( Y \\), respectively, we get:\n\n\\[ Y \\times 10 + 3 = 3 \\times 10 + Y - 9 \\]\n\nSimplifying the equation:\n\n\\[ 10Y + 3 = 30 + Y - 9 \\]\n\n\\[ 10Y + 3 = Y + 21 \\]\n\nSubtract \\( Y \\) from both sides:\n\n\\[ 9Y + 3 = 21 \\]\n\nSubtract 3 from both sides:\n\n\\[ 9Y = 18 \\]\n\nDivide both sides by 9:\n\n\\[ Y = 2 \\]\n\nSo the ones place digit is 2. Since we already know the tens place digit is 3, the two-digit number is \\( 32 \\).<|im_end|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.974490Z","iopub.execute_input":"2024-12-14T01:42:26.975098Z","iopub.status.idle":"2024-12-14T01:42:26.984146Z","shell.execute_reply.started":"2024-12-14T01:42:26.975055Z","shell.execute_reply":"2024-12-14T01:42:26.983310Z"}},"outputs":[{"name":"stdout","text":"[1, 774, 22478, 28747, 13, 5816, 349, 264, 989, 28733, 7845, 279, 4229, 1474, 4636, 19391, 1633, 349, 28705, 28770, 28723, 3169, 330, 304, 365, 347, 272, 17528, 722, 302, 456, 1474, 486, 28705, 28740, 28734, 304, 272, 23317, 302, 9652, 486, 28705, 28740, 28734, 28725, 8628, 28723, 1047, 365, 6079, 3002, 486, 28705, 28740, 28734, 3285, 330, 349, 28705, 28774, 2108, 821, 330, 6079, 3002, 486, 28705, 28740, 28734, 3285, 365, 28725, 767, 349, 272, 907, 1474, 28804, 13, 27332, 26307, 28747, 13, 8779, 28742, 28713, 14543, 272, 989, 28733, 7845, 279, 1474, 390, 18823, 1500, 28802, 414, 557, 970, 18823, 1500, 414, 28731, 349, 272, 21656, 297, 272, 19391, 1633, 304, 18823, 627, 414, 28731, 349, 272, 21656, 297, 272, 4413, 1633, 28723, 4577, 272, 19391, 1633, 349, 28705, 28770, 28725, 478, 506, 18823, 1500, 327, 28705, 28770, 414, 609, 13, 13, 5604, 3059, 298, 272, 2700, 28725, 18823, 330, 414, 28731, 349, 272, 17528, 722, 302, 272, 1474, 486, 28705, 28740, 28734, 28725, 304, 18823, 365, 414, 28731, 349, 272, 23317, 302, 272, 9652, 486, 28705, 28740, 28734, 28723, 8469, 28725, 18823, 330, 327, 1500, 327, 28705, 28770, 414, 28731, 304, 18823, 365, 327, 627, 414, 609, 13, 13, 1014, 2700, 4605, 369, 18823, 365, 414, 4593, 28705, 28740, 28734, 648, 330, 414, 28731, 349, 28705, 28774, 2108, 821, 18823, 330, 414, 4593, 28705, 28740, 28734, 648, 365, 414, 609, 851, 541, 347, 4241, 390, 396, 8777, 28747, 13, 13, 16927, 365, 414, 4593, 28705, 28740, 28734, 648, 330, 327, 330, 414, 4593, 28705, 28740, 28734, 648, 365, 387, 28705, 28774, 8425, 13, 13, 3540, 303, 3288, 288, 18823, 330, 414, 28731, 304, 18823, 365, 414, 28731, 395, 18823, 28705, 28770, 414, 28731, 304, 18823, 627, 414, 557, 8628, 28725, 478, 625, 28747, 13, 13, 16927, 627, 414, 4593, 28705, 28740, 28734, 648, 28705, 28770, 327, 28705, 28770, 414, 4593, 28705, 28740, 28734, 648, 627, 387, 28705, 28774, 8425, 13, 13, 7554, 452, 6219, 272, 8777, 28747, 13, 13, 16927, 28705, 28740, 28734, 28802, 648, 28705, 28770, 327, 28705, 28770, 28734, 648, 627, 387, 28705, 28774, 8425, 13, 13, 16927, 28705, 28740, 28734, 28802, 648, 28705, 28770, 327, 627, 648, 28705, 28750, 28740, 8425, 13, 13, 3540, 2107, 18823, 627, 414, 28731, 477, 1560, 8480, 28747, 13, 13, 16927, 28705, 28774]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.985109Z","iopub.execute_input":"2024-12-14T01:42:26.985388Z","iopub.status.idle":"2024-12-14T01:42:26.996367Z","shell.execute_reply.started":"2024-12-14T01:42:26.985364Z","shell.execute_reply":"2024-12-14T01:42:26.995418Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:26.997369Z","iopub.execute_input":"2024-12-14T01:42:26.997641Z","iopub.status.idle":"2024-12-14T01:42:27.009442Z","shell.execute_reply.started":"2024-12-14T01:42:26.997616Z","shell.execute_reply":"2024-12-14T01:42:27.008713Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:27.010347Z","iopub.execute_input":"2024-12-14T01:42:27.010596Z","iopub.status.idle":"2024-12-14T01:42:27.026135Z","shell.execute_reply.started":"2024-12-14T01:42:27.010572Z","shell.execute_reply":"2024-12-14T01:42:27.025253Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:27.027098Z","iopub.execute_input":"2024-12-14T01:42:27.027460Z","iopub.status.idle":"2024-12-14T01:42:27.039892Z","shell.execute_reply.started":"2024-12-14T01:42:27.027425Z","shell.execute_reply":"2024-12-14T01:42:27.039177Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n\n#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:27.040976Z","iopub.execute_input":"2024-12-14T01:42:27.041680Z","iopub.status.idle":"2024-12-14T01:42:27.050797Z","shell.execute_reply.started":"2024-12-14T01:42:27.041646Z","shell.execute_reply":"2024-12-14T01:42:27.049924Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:27.051872Z","iopub.execute_input":"2024-12-14T01:42:27.052373Z","iopub.status.idle":"2024-12-14T01:42:29.264452Z","shell.execute_reply.started":"2024-12-14T01:42:27.052330Z","shell.execute_reply":"2024-12-14T01:42:29.263635Z"}},"outputs":[{"name":"stdout","text":"trainable params: 167,772,160 || all params: 7,409,766,400 || trainable%: 2.2642\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:29.265480Z","iopub.execute_input":"2024-12-14T01:42:29.265844Z","iopub.status.idle":"2024-12-14T01:42:29.289711Z","shell.execute_reply.started":"2024-12-14T01:42:29.265806Z","shell.execute_reply":"2024-12-14T01:42:29.288946Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 3920105472\nTrainable parameters : 167772160\nTrainable percentage: 4.28%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:29.290645Z","iopub.execute_input":"2024-12-14T01:42:29.291669Z","iopub.status.idle":"2024-12-14T01:42:29.300550Z","shell.execute_reply.started":"2024-12-14T01:42:29.291633Z","shell.execute_reply":"2024-12-14T01:42:29.299835Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 1\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:29.301936Z","iopub.execute_input":"2024-12-14T01:42:29.302194Z","iopub.status.idle":"2024-12-14T01:42:29.347081Z","shell.execute_reply.started":"2024-12-14T01:42:29.302167Z","shell.execute_reply":"2024-12-14T01:42:29.346437Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec14_01-42-29_9bb16ce33c9a,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:29.347916Z","iopub.execute_input":"2024-12-14T01:42:29.348185Z","iopub.status.idle":"2024-12-14T01:42:32.042515Z","shell.execute_reply.started":"2024-12-14T01:42:29.348155Z","shell.execute_reply":"2024-12-14T01:42:32.041724Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x793a41704880>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T01:42:32.043395Z","iopub.execute_input":"2024-12-14T01:42:32.043627Z","iopub.status.idle":"2024-12-14T02:47:20.133185Z","shell.execute_reply.started":"2024-12-14T01:42:32.043604Z","shell.execute_reply":"2024-12-14T02:47:20.132139Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 1\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 167,772,160\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterlupakan100\u001b[0m (\u001b[33mterlupakan100-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241214_014233-n2gh2g1l</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/n2gh2g1l' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/n2gh2g1l' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/n2gh2g1l</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 1:04:35, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.882500</td>\n      <td>0.792491</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.773900</td>\n      <td>0.693048</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.706300</td>\n      <td>0.629357</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.675400</td>\n      <td>0.601854</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.621400</td>\n      <td>0.590919</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.605900</td>\n      <td>0.585206</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.606800</td>\n      <td>0.580539</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.650400</td>\n      <td>0.578557</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.616600</td>\n      <td>0.577829</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.602500</td>\n      <td>0.577608</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=0.6741640615463257, metrics={'train_runtime': 3887.6633, 'train_samples_per_second': 0.206, 'train_steps_per_second': 0.051, 'total_flos': 1.37250855714816e+16, 'train_loss': 0.6741640615463257, 'epoch': 0.08888888888888889})"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:47:20.134594Z","iopub.execute_input":"2024-12-14T02:47:20.134983Z","iopub.status.idle":"2024-12-14T02:50:32.971769Z","shell.execute_reply.started":"2024-12-14T02:47:20.134942Z","shell.execute_reply":"2024-12-14T02:50:32.970936Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 03:08]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.5776081681251526, 'eval_runtime': 192.8235, 'eval_samples_per_second': 1.037, 'eval_steps_per_second': 0.259, 'epoch': 0.08888888888888889}\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:32.972975Z","iopub.execute_input":"2024-12-14T02:50:32.973265Z","iopub.status.idle":"2024-12-14T02:50:36.244098Z","shell.execute_reply.started":"2024-12-14T02:50:32.973237Z","shell.execute_reply":"2024-12-14T02:50:36.243436Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Hermes-2-Pro-Mistral-7B/snapshots/69aa177971081db319c485212ec7f81dafcd6d8a/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"NousResearch/Hermes-2-Pro-Mistral-7B\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": false,\n  \"vocab_size\": 32032\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Hermes-2-Pro-Mistral-7B/snapshots/69aa177971081db319c485212ec7f81dafcd6d8a/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"NousResearch/Hermes-2-Pro-Mistral-7B\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": false,\n  \"vocab_size\": 32032\n}\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:36.249813Z","iopub.execute_input":"2024-12-14T02:50:36.250080Z","iopub.status.idle":"2024-12-14T02:50:36.399698Z","shell.execute_reply.started":"2024-12-14T02:50:36.250053Z","shell.execute_reply":"2024-12-14T02:50:36.398819Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:36.400674Z","iopub.execute_input":"2024-12-14T02:50:36.400964Z","iopub.status.idle":"2024-12-14T02:50:36.413111Z","shell.execute_reply.started":"2024-12-14T02:50:36.400928Z","shell.execute_reply":"2024-12-14T02:50:36.412252Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:36.414025Z","iopub.execute_input":"2024-12-14T02:50:36.414235Z","iopub.status.idle":"2024-12-14T02:50:38.898453Z","shell.execute_reply.started":"2024-12-14T02:50:36.414214Z","shell.execute_reply":"2024-12-14T02:50:38.897769Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:38.899459Z","iopub.execute_input":"2024-12-14T02:50:38.899693Z","iopub.status.idle":"2024-12-14T02:50:55.024537Z","shell.execute_reply.started":"2024-12-14T02:50:38.899669Z","shell.execute_reply":"2024-12-14T02:50:55.023756Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Hermes-2-Pro-Mistral-7B/snapshots/69aa177971081db319c485212ec7f81dafcd6d8a/config.json\nModel config MistralConfig {\n  \"_name_or_path\": \"unsloth/Hermes-2-Pro-Mistral-7B\",\n  \"architectures\": [\n    \"MistralForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 32768,\n  \"model_type\": \"mistral\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_theta\": 10000.0,\n  \"sliding_window\": 4096,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": false,\n  \"vocab_size\": 32032\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--Hermes-2-Pro-Mistral-7B/snapshots/69aa177971081db319c485212ec7f81dafcd6d8a/model.safetensors.index.json\nInstantiating MistralForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"eos_token_id\": 32000,\n  \"use_cache\": false\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea173cfc7e9b41b1b72070845dc735ca"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing MistralForCausalLM.\n\nAll the weights of MistralForCausalLM were initialized from the model checkpoint at unsloth/Hermes-2-Pro-Mistral-7B.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Hermes-2-Pro-Mistral-7B/snapshots/69aa177971081db319c485212ec7f81dafcd6d8a/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 1,\n  \"do_sample\": true,\n  \"eos_token_id\": 32000\n}\n\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"MistralForCausalLM(\n  (model): MistralModel(\n    (embed_tokens): Embedding(32032, 4096)\n    (layers): ModuleList(\n      (0-31): 32 x MistralDecoderLayer(\n        (self_attn): MistralSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): MistralRotaryEmbedding()\n        )\n        (mlp): MistralMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): MistralRMSNorm((4096,), eps=1e-05)\n  )\n  (lm_head): Linear(in_features=4096, out_features=32032, bias=False)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:55.025500Z","iopub.execute_input":"2024-12-14T02:50:55.025725Z","iopub.status.idle":"2024-12-14T02:50:55.034836Z","shell.execute_reply.started":"2024-12-14T02:50:55.025700Z","shell.execute_reply":"2024-12-14T02:50:55.034076Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 3752333312\nTrainable parameters : 262672384\nTrainable percentage: 7.00%\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:55.035829Z","iopub.execute_input":"2024-12-14T02:50:55.036161Z","iopub.status.idle":"2024-12-14T02:50:55.059546Z","shell.execute_reply.started":"2024-12-14T02:50:55.036125Z","shell.execute_reply":"2024-12-14T02:50:55.058633Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): MistralForCausalLM(\n      (model): MistralModel(\n        (embed_tokens): Embedding(32032, 4096)\n        (layers): ModuleList(\n          (0-31): 32 x MistralDecoderLayer(\n            (self_attn): MistralSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=64, bias=False)\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=4096, bias=False)\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=64, bias=False)\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=1024, bias=False)\n                  (default): Linear(in_features=64, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=64, bias=False)\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=1024, bias=False)\n                  (default): Linear(in_features=64, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=64, bias=False)\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=4096, bias=False)\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): MistralRotaryEmbedding()\n            )\n            (mlp): MistralMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=64, bias=False)\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=14336, bias=False)\n                  (default): Linear(in_features=64, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=64, bias=False)\n                  (default): Linear(in_features=4096, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=14336, bias=False)\n                  (default): Linear(in_features=64, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=14336, out_features=64, bias=False)\n                  (default): Linear(in_features=14336, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=4096, bias=False)\n                  (default): Linear(in_features=64, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): MistralRMSNorm((4096,), eps=1e-05)\n      )\n      (lm_head): Linear(in_features=4096, out_features=32032, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:55.060591Z","iopub.execute_input":"2024-12-14T02:50:55.060864Z","iopub.status.idle":"2024-12-14T02:50:55.086326Z","shell.execute_reply.started":"2024-12-14T02:50:55.060840Z","shell.execute_reply":"2024-12-14T02:50:55.085570Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 4087877632\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## 14 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:55.087285Z","iopub.execute_input":"2024-12-14T02:50:55.087605Z","iopub.status.idle":"2024-12-14T02:50:55.097896Z","shell.execute_reply.started":"2024-12-14T02:50:55.087579Z","shell.execute_reply":"2024-12-14T02:50:55.097086Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def post_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:55.098812Z","iopub.execute_input":"2024-12-14T02:50:55.099057Z","iopub.status.idle":"2024-12-14T02:50:55.110437Z","shell.execute_reply.started":"2024-12-14T02:50:55.099032Z","shell.execute_reply":"2024-12-14T02:50:55.109679Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-14T02:50:55.111339Z","iopub.execute_input":"2024-12-14T02:50:55.111609Z","iopub.status.idle":"2024-12-14T02:50:55.120886Z","shell.execute_reply.started":"2024-12-14T02:50:55.111585Z","shell.execute_reply":"2024-12-14T02:50:55.120060Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T04:25:48.602334Z","iopub.execute_input":"2024-12-14T04:25:48.603170Z","iopub.status.idle":"2024-12-14T04:27:05.391764Z","shell.execute_reply.started":"2024-12-14T04:25:48.603131Z","shell.execute_reply":"2024-12-14T04:27:05.390882Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    3752333312                     |                     4087877632                    \n================================================== | ==================================================\n### Question: Nine weights of the same weight       |  ### Question: Nine weights of the same weight     \nweigh a total of 4.5 kilograms (kg). Two of these   |  weigh a total of 4.5 kilograms (kg). Two of these \nweights and several pencil cases that weigh 850     |  weights and several pencil cases that weigh 850   \ngrams (g) each were placed on one side of a pan     |  grams (g) each were placed on one side of a pan   \nbalance scale, and five dictionaries that weigh     |  balance scale, and five dictionaries that weigh   \n1050 grams (g) each were placed on the other side,  |  1050 grams (g) each were placed on the other side,\nand they were level. How many pencil cases are      |  and they were level. How many pencil cases are    \nthere? ### Answer: There are 3 pencil cases.  ###   |  there? ### Answer: There are 3 pencil cases.  ### \nExplanation: Let's break down the information       |  Explanation: Let's break down the information     \ngiven:  1. Nine weights of the same weight weigh a  |  given:  1. Nine weights of the same weight weigh a\ntotal of 4.5 kg. 2. Two of these weights and        |  total of 4.5 kg. 2. Two of these weights and      \nseveral pencil cases that weigh 850 g each were     |  several pencil cases that weigh 850 g each were   \nplaced on one side of the scale. 3. Five            |  placed on one side of the scale. 3. Five          \ndictionaries that weigh 1050 g each were placed on  |  dictionaries that weigh 1050 g each were placed on\nthe other side of the scale.  First, let's convert  |  the other side of the scale.  First, let's convert\neverything to grams to make the problem             |  everything to grams to make the problem           \nconsistent:  1. Nine weights of the same weight     |  consistent:  1. Nine weights of the same weight   \nweigh a total of 4500 g.  Now, let's denote the     |  weigh a total of 4500 g.  Now, let's denote the   \nnumber of pencil cases as 'x'. The total weight on  |  number of pencil cases as 'x'. The total weight on\none side of the scale is:  2 weights + x pencil     |  one side of the scale is:  2 weights + x pencil   \ncases = 2 * 1000 g + x * 850 g  The total weight    |  cases = 2 * 1000 g (since each weight is 1000 g) +\non the other side of the scale is:  5 dictionaries  |  x * 850 g  The total weight on the other side of  \n= 5 * 1050 g  According to the problem, both sides  |  the scale is:  5 dictionaries = 5 * 1050 g        \nof the scale are level, so the total weight on one  |  According to the problem, both sides of the scale \nside should be equal to the total weight on the     |  are level, so the total weight on one side should \nother side:  2 * 1000 g + x * 850 g = 5 * 1050 g    |  be equal to the total weight on the other side:  2\n2000 g + 850gx = 5250 g  Now, we can solve for      |  * 1000 g + x * 850 g = 5 * 1050 g  2000 g + 850gx \n'x':  850gx = 5250 g - 2000 g 850gx = 3250 g x =    |  = 5250 g  Now, we can solve for 'x':  850gx = 5250\n3250 g / 850 g x = 3.888...  Since 'x' must be a    |  g - 2000 g 850gx = 3250 g x = 3250 g / 850 g x =  \nwhole number (as it represents the number of        |  3.84 (approximately)  Since 'x' must be a whole   \npencil cases), we can round down to the nearest     |  number (as it represents the number of pencil     \nwhole number, which is 3.  So, there are 3 pencil   |  cases), we can round down to the nearest whole    \ncases.                                              |  number, which is 3.  So, there are 3 pencil cases.\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T04:29:22.465102Z","iopub.execute_input":"2024-12-14T04:29:22.465929Z","iopub.status.idle":"2024-12-14T04:29:53.760477Z","shell.execute_reply.started":"2024-12-14T04:29:22.465891Z","shell.execute_reply":"2024-12-14T04:29:53.759643Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    3752333312                     |                     4087877632                    \n================================================== | ==================================================\n### Question: James watched 2 episodes of Jeopardy  |  ### Question: James watched 2 episodes of Jeopardy\nand 2 episodes of Wheel of Fortune. Jeopardy is a   |  and 2 episodes of Wheel of Fortune. Jeopardy is a \ncertain length and Wheel of Fortune is twice as     |  certain length and Wheel of Fortune is twice as   \nlong. He watched TV for 2 hours. How many minutes   |  long. He watched TV for 2 hours. How many minutes \nis one episode of Jeopardy? ### Answer: Let's       |  is one episode of Jeopardy? ### Answer: Let's     \nassume the length of one episode of Jeopardy is x   |  assume the length of one episode of Jeopardy is x \nminutes. Since Wheel of Fortune is twice as long,   |  minutes. Since there are 2 episodes of Jeopardy   \nthe length of one episode of Wheel of Fortune is    |  and 2 episodes of Wheel of Fortune, the total time\n2x minutes. James watched 2 episodes of Jeopardy    |  spent watching TV is 2x + 2(2x) = 2x + 4x = 6x    \nand 2 episodes of Wheel of Fortune, so the total    |  minutes. We know that James watched TV for 2      \ntime he spent watching TV is 2x + 2(2x) = 2x + 4x   |  hours, which is equal to 2 * 60 = 120 minutes. So,\n= 6x minutes. We know that James watched TV for 2   |  6x = 120 Dividing both sides of the equation by 6,\nhours, which is equal to 2 * 60 = 120 minutes.      |  we get x = 20. Therefore, one episode of Jeopardy \nTherefore, 6x = 120. Dividing both sides of the     |  is 20 minutes long. #### 20 The answer is: 20     \nequation by 6, we get x = 20. So, one episode of    |                                                    \nJeopardy is 20 minutes long. #### 20 The answer     |                                                    \nis: 20                                              |                                                    \n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T04:37:49.457954Z","iopub.execute_input":"2024-12-14T04:37:49.458733Z","iopub.status.idle":"2024-12-14T04:38:46.268581Z","shell.execute_reply.started":"2024-12-14T04:37:49.458695Z","shell.execute_reply":"2024-12-14T04:38:46.267682Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    3752333312                     |                     4087877632                    \n================================================== | ==================================================\n### Question: There is a six-digit number 25A735    |  ### Question: There is a six-digit number 25A735  \nwhich is 260000 when rounded to the thousandth      |  which is 260000 when rounded to the thousandth    \nplace. If you can write the numbers 0 through 9 in  |  place. If you can write the numbers 0 through 9 in\nA, how many possible numbers are there? ###         |  A, how many possible numbers are there? ###       \nAnswer: There are 10 possible numbers.  To          |  Answer: There are 10 possible numbers.  To        \nunderstand this, let's break down the information   |  understand this, let's break down the information \ngiven:  1. We have a six-digit number (25A735)      |  given:  1. We have a six-digit number (25A735)    \nthat gets rounded to 260000 when rounded to the     |  that gets rounded to 260000 when rounded to the   \nthousandth place. This means that the thousands     |  thousandth place. This means that the thousands   \ndigit (A) can be any number from 0 to 9,            |  digit (A) can be any number from 0 to 9,          \ninclusive, and the other five digits remain the     |  inclusive, and the other five digits remain the   \nsame. 2. When rounding a number to the thousandth   |  same. 2. When rounding a number to the thousandth \nplace, the digit in the thousands place is          |  place, the digit in the thousands place is        \nessentially lost, as it only contributes to the     |  essentially lost, as it doesn't contribute to the \noverall value of the number without affecting the   |  rounded number. In this case, the number 25A735   \nrounding decision. 3. Since the thousands digit     |  would be rounded down to 250000 if A is 0, and    \n(A) can be any number from 0 to 9, and the other    |  rounded up to 260000 if A is 9. 3. Since the other\nfive digits remain the same (25735), there are 10   |  five digits (25735) remain the same for all values\npossible numbers when the thousands digit is        |  of A, we only need to consider the possible values\nvaried from 0 to 9.  Therefore, there are 10        |  for A to determine the number of possible numbers.\npossible numbers when the numbers 0 through 9 can   |  As A can be any number from 0 to 9, inclusive,    \nbe written in the thousands digit of the given      |  there are 10 possible numbers. These are:  0:     \nnumber.                                             |  250000 1: 250001 2: 250002 3: 250003 4: 250004 5: \n                                                    |  250005 6: 250006 7: 250007 8: 250008 9: 260000    \n                                                    |  So, there are 10 possible numbers when the numbers\n                                                    |  0 through 9 are written in the thousands place of \n                                                    |  the number 25A735.                                \n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T05:03:32.502497Z","iopub.execute_input":"2024-12-14T05:03:32.503371Z","iopub.status.idle":"2024-12-14T05:04:38.593195Z","shell.execute_reply.started":"2024-12-14T05:03:32.503335Z","shell.execute_reply":"2024-12-14T05:04:38.592335Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    3752333312                     |                     4087877632                    \n================================================== | ==================================================\n### Question: Andy is a lawyer who's working on     |  ### Question: Andy is a lawyer who's working on   \ntwo lawsuits. The first lawsuit has a certain       |  two lawsuits. The first lawsuit has a certain     \nchance of paying out $5,000,000 upon a win and $0   |  chance of paying out $5,000,000 upon a win and $0 \nif he loses it. The second lawsuit has a 50%        |  if he loses it. The second lawsuit has a 50%      \nchance of paying out $1,000,000 if Andy loses and   |  chance of paying out $1,000,000 if Andy loses and \na 50% chance of paying out $2,000,000 if he wins.   |  a 50% chance of paying out $2,000,000 if he wins. \nIt is 20% more likely that Andy loses both          |  It is 20% more likely that Andy loses both        \nlawsuits compared to winning both of them. What is  |  lawsuits compared to winning both of them. What is\nthe probability of winning the first lawsuit? ###   |  the probability of winning the first lawsuit? ### \nAnswer: To solve this problem, we need to use the   |  Answer: To solve this problem, we need to use the \nconcept of conditional probability and the formula  |  concept of conditional probability and the formula\nfor the probability of independent events: P(A and  |  for the probability of independent events: P(A and\nB) = P(A) * P(B) where A and B are the events we    |  B) = P(A) * P(B) where A and B are the events we  \nare interested in. Let's define the events: -       |  are interested in. Let's define the events: -     \nEvent A: winning the first lawsuit - Event B1:      |  Event A: winning the first lawsuit - Event B1:    \nlosing the first lawsuit - Event B2: winning the    |  losing the first lawsuit - Event B2: winning the  \nsecond lawsuit We are given that it is 20% more     |  second lawsuit We are asked to find the           \nlikely that Andy loses both lawsuits compared to    |  probability of winning the first lawsuit, which is\nwinning both of them. Therefore, we can write:      |  the probability of event A. We know that it is 20%\nP(B1 and B2) = 0.8 * P(B2) P(A and B1) = 0.2 *      |  more likely that Andy loses both lawsuits compared\nP(B2) Now, we can use the formula for the           |  to winning both of them. This means that the      \nprobability of independent events to find the       |  probability of losing both lawsuits is 1.2 times  \nprobability of winning the first lawsuit: P(A) =    |  the probability of winning both lawsuits. Let P(A)\nP(A and B1) + P(A and B2) P(A and B1) = 0.2 *       |  be the probability of winning the first lawsuit.  \nP(B2) P(A and B2) = P(B1 and B2) Substituting the   |  Then, the probability of losing the first lawsuit \ngiven values, we get: P(A) = 0.2 * P(B2) + 0.8 *    |  is 1 - P(A). The probability of losing both       \nP(B2) P(A) = 1.0 * P(B2) Therefore, the             |  lawsuits is the product of the probabilities of   \nprobability of winning the first lawsuit is equal   |  losing each lawsuit: P(B1) * P(B2) = (1 - P(A)) * \nto the probability of winning the second lawsuit.   |  0.5 1.2 * P(A) * 0.5 = 0.5 * (1 - P(A))           \nWe are given that the probability of winning the    |  Simplifying the equation, we get: 0.6P(A) = 0.5 - \nsecond lawsuit is 50%, so the probability of        |  0.5P(A) 1.1P(A) = 0.5 P(A) = 0.5 / 1.1 P(A) ≈     \nwinning the first lawsuit is also 50%. The answer   |  0.4545 Therefore, the probability of winning the  \nis: 50%                                             |  first lawsuit is approximately 45.45%. ### Answer:\n                                                    |  The probability of winning the first lawsuit is   \n                                                    |  approximately 45.45%.                             \n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-12-14T05:07:56.206912Z","iopub.execute_input":"2024-12-14T05:07:56.207777Z","iopub.status.idle":"2024-12-14T05:09:02.036174Z","shell.execute_reply.started":"2024-12-14T05:07:56.207738Z","shell.execute_reply":"2024-12-14T05:09:02.035212Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    3752333312                     |                     4087877632                    \n================================================== | ==================================================\n### Question: Find the sum of all natural numbers   |  ### Question: Find the sum of all natural numbers \nless than or equal to 5. ### Answer: The sum of     |  less than or equal to 5. ### Answer: The sum of   \nall natural numbers less than or equal to 5 is 1 +  |  all natural numbers less than or equal to 5 is 1 +\n2 + 3 + 4 + 5 = 15.  ### Question: What is the      |  2 + 3 + 4 + 5 = 15.  ### Question: What is the    \nvalue of x in the equation 2x + 3 = 11? ###         |  value of x in the equation 2x + 3 = 11? ###       \nAnswer: To find the value of x in the equation 2x   |  Answer: To find the value of x in the equation 2x \n+ 3 = 11, we need to isolate x on one side of the   |  + 3 = 11, we need to isolate x on one side of the \nequation. We can do this by subtracting 3 from      |  equation. We can do this by subtracting 3 from    \nboth sides:  2x + 3 - 3 = 11 - 3 2x = 8  Now, we    |  both sides:  2x + 3 - 3 = 11 - 3 2x = 8  Now, we  \ncan divide both sides by 2 to get the value of x:   |  can divide both sides by 2 to get the value of x: \n2x/2 = 8/2 x = 4  So, the value of x in the         |  2x/2 = 8/2 x = 4  So, the value of x in the       \nequation 2x + 3 = 11 is 4. ### Question: What is    |  equation 2x + 3 = 11 is 4. ### Question: Simplify \nthe area of a circle with a radius of 3 cm? ###     |  the expression (3a^2b^3)(2a^3b^2). ### Answer: To \nAnswer: The area of a circle can be calculated      |  simplify the expression (3a^2b^3)(2a^3b^2), we can\nusing the formula A = πr^2, where A is the area     |  multiply the coefficients and add the exponents of\nand r is the radius. In this case, the radius is 3  |  the like terms:  (3 * 2) * (a^2 * a^3) * (b^3 *   \ncm.  A = π(3)^2 A = π(9) A = 9π  The area of the    |  b^2) 6a^(2+3) * b^(3+2) 6a^5 * b^5  So, the       \ncircle with a radius of 3 cm is 9π square           |  simplified expression is 6a^5b^5.                 \ncentimeters. ### Question: What is the perimeter    |                                                    \nof a rectangle with a length of 8 cm and a width    |                                                    \nof 4 cm? ### Answer: The perimeter of a rectangle   |                                                    \ncan be calculated using the formula P = 2(l + w),   |                                                    \nwhere P is the perimeter, l is the length, and w    |                                                    \nis the width. In this case, the length is 8 cm and  |                                                    \nthe width is 4 cm.  P = 2(8 + 4) P = 2(12) P = 24   |                                                    \nThe perimeter of the rectangle with a length of 8   |                                                    \ncm and a width of 4 cm is 24 cm.                    |                                                    \n","output_type":"stream"}],"execution_count":106}]}