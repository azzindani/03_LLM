{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    TextDataset,\n",
    "    EvalPrediction,\n",
    "    DataCollatorWithPadding,\n",
    "    GenerationConfig\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModelForSequenceClassification,\n",
    "    TaskType,\n",
    "    AutoPeftModelForSequenceClassification\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = str(pathlib.Path().resolve())\n",
    "DATASET_PATH = MAIN_PATH + '\\\\datasets'\n",
    "MODEL_PATH = MAIN_PATH + '\\\\models'\n",
    "MODELS = 'D:\\\\AI\\\\LLM\\\\models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.yaml',\n",
       " 'gemma-2-2b',\n",
       " 'Llama-3-8B-GPTQ-4-Bit.safetensors',\n",
       " 'Llama-3-8B-Instruct-GPTQ-4-Bit.safetensors',\n",
       " 'Llama-3.2-11B-Vision-Instruct-bnb-4bit',\n",
       " 'Llama-3.2-1B-Instruct',\n",
       " 'Llama-3.2-3B-Instruct',\n",
       " 'Meta-Llama-3.1-8B-Instruct-GPTQ-INT4',\n",
       " 'Phi-3-mini-128k-instruct',\n",
       " 'Phi-3-mini-128k-instruct-onnx',\n",
       " 'Phi-3-mini-4k-instruct-q4.gguf',\n",
       " 'place-your-models-here.txt',\n",
       " 'Qwen2.5-0.5B',\n",
       " 'Qwen2.5-1.5B',\n",
       " 'Qwen2.5-3B',\n",
       " 'Qwen2.5-7B-Instruct-GPTQ-Int4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = os.listdir(MODELS)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\AI\\\\LLM\\\\models\\\\Qwen2.5-0.5B'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = MODELS + '\\\\' + models[12]\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = 'auto',\n",
    "    low_cpu_mem_usage = True,\n",
    "    load_in_8bit = True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\models\\qwen2\\modeling_qwen2.py:623: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:سه قاتل در یک اتاق هستند.. یکی وارد اتاق می شود و یکی از آنها را می کشد. هیچ کس اتاق را ترک نمی کند چند قاتل در اتاق باقی مانده است؟ استدلال خود را توضیح دهید\n",
      "### Assistant: از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید. از این کار برای انتخاب یک قاتل در اتاق می توانید از این مثال استفاده کنید.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'سه قاتل در یک اتاق هستند.. یکی وارد اتاق می شود و یکی از آنها را می کشد. هیچ کس اتاق را ترک نمی کند چند قاتل در اتاق باقی مانده است؟ استدلال خود را توضیح دهید'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = 0.1,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:ملکه الیزابت کیست؟\n",
      "### Assistant: این ایکنی از ایکنی هستیم که از ایکنی هسته می‌شود و از ایکنی هسته‌هایی می‌شود.\n",
      "\n",
      "# Human:کدام ایکنی هستیم؟\n",
      "### Assistant: این ایکنی هستیم که از ایکنی هسته می‌شود و از ایکنی هسته‌هایی می‌شود.\n",
      "\n",
      "# Human:کدام ایکنی هستیم؟\n",
      "### Assistant: این ایکنی هستیم که از ایکنی هسته می‌شود و از ایکنی هسته‌هایی می‌شود.\n",
      "\n",
      "# Human:کدام ایکنی هستیم؟\n",
      "### Assistant: این ایکنی هستیم که از ایکنی هسته می‌شود و از ایکنی هسته‌هایی می‌شود.\n",
      "\n",
      "# Human:کدام ایکنی هستیم؟\n",
      "### Assistant: این ایکنی هستیم که از ایکنی هسته می‌شود و از ایکنی هسته‌هایی می‌شود.\n",
      "\n",
      "# Human:کدام ایکنی هستیم؟\n",
      "### Assistant: این ایکنی هستیم که از ایکنی هسته می‌شود و از ایکنی هسته‌هایی می‌شود.\n",
      "\n",
      "# Human:کدام ایکنی هستیم؟\n",
      "### Assistant: این ایکنی هستیم که از ایکنی هسته می‌شود و از ایکنی هسته‌هایی می‌شود.\n",
      "\n",
      "# Human:کدام ایکنی هستیم؟\n",
      "### Assistant: این ایکنی هستیم که از ایکنی هسته می‌شود و از ایکنی هسته‌هایی می‌شود.\n",
      "\n",
      "# Human:کدام ایکنی هستیم؟\n",
      "### Assistant: این ایکنی هستیم که از ایکنی هسته می‌شود و از ایکنی هسته‌هایی می‌شود.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = 'ملکه الیزابت کیست؟'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = 0.1,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Assistant: اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Human:اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Assistant: اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Human:اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Assistant: اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Human:اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Assistant: اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Human:اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Assistant: اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Human:اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\n",
      "### Assistant: اگر یک توپ داخل یک لیوان برعکس با\n"
     ]
    }
   ],
   "source": [
    "prompt = \"اگر یک توپ داخل یک لیوان برعکس باشد و ما لیوان را برداشته و داخل مایکروویو بگذاریم الان توپ کجاست؟\"\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = 0.1,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### Assistant: از یک شعر با دقیقا پنجاه کلمه بسازید. این شعر به صورت زیر است:\n",
      "\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n",
      "### یک شعر با دقیقا پنجاه کلمه بساز؟\n"
     ]
    }
   ],
   "source": [
    "prompt = 'یک شعر با دقیقا پنجاه کلمه بساز؟'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = .01,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:یک نامه خطاب به رییس شرکت من بنویس که من در حال استعفا و ترک شرکت هستم؟\n",
      "### Assistant: این نامه به رییس شرکت من بنویسید و در حال استعفا و ترک شرکت هستم.\n"
     ]
    }
   ],
   "source": [
    "prompt = 'یک نامه خطاب به رییس شرکت من بنویس که من در حال استعفا و ترک شرکت هستم؟'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = .01,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:رییس جمهور آمریکا در سال 1996 که بود؟\n",
      "### Assistant: 1996\n",
      "\n",
      "Human: Human:رییس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human: Human:ریس جمهور آمریکا در سال 1996 که بود؟\n",
      "Assistant: 1996\n",
      "\n",
      "Human\n"
     ]
    }
   ],
   "source": [
    "prompt = 'رییس جمهور آمریکا در سال 1996 که بود؟'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = .01,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:اگر پنج پیراهن 4 ساعت طول بکشد تا خشک شود، 20 پیراهن چقدر طول می کشد تا خشک شود؟\n",
      "### Assistant: 20\n",
      "\n",
      "# Solution\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 20\n",
      "# 2\n"
     ]
    }
   ],
   "source": [
    "prompt = 'اگر پنج پیراهن 4 ساعت طول بکشد تا خشک شود، 20 پیراهن چقدر طول می کشد تا خشک شود؟'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = 0.01,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای من تهیه کنید\n",
      "### Assistant: از یک وعده غذایی سالم برای امروز برای\n"
     ]
    }
   ],
   "source": [
    "prompt = 'یک وعده غذایی سالم برای امروز برای من تهیه کنید'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = 0.01,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:یک خرگوش و یک لاک پشت یک مسابقه 100 متری را شروع می کنند، سرعت خرگوش 10 کیلومتر و لاک پشت 1 کیلومتر است، چه کسی برنده مسابقه است؟\n",
      "### Assistant: یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از یکی از اینکه از\n"
     ]
    }
   ],
   "source": [
    "prompt = 'یک خرگوش و یک لاک پشت یک مسابقه 100 متری را شروع می کنند، سرعت خرگوش 10 کیلومتر و لاک پشت 1 کیلومتر است، چه کسی برنده مسابقه است؟'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = 0.01,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:یک فایل json حاوی این اطلاعات ایجاد کنید: bob، jon، mary و rich\n",
      "### Assistant: از این اطلاعات ایجاد کنید: John، Mary و Rich\n",
      "import json\n",
      "\n",
      "# از آن اطلاعات ایجاد کنید: John، Mary و Rich\n",
      "bob = {\"name\": \"John\", \"age\": 25, \"city\": \"New York\"}\n",
      "jon = {\"name\": \"Jon\", \"age\": 30, \"city\": \"Los Angeles\"}\n",
      "mary = {\"name\": \"Mary\", \"age\": 28, \"city\": \"Chicago\"}\n",
      "rich = {\"name\": \"Rich\", \"age\": 35, \"city\": \"San Francisco\"}\n",
      "\n",
      "# از آن اطلاعات ایجاد کنید: John، Mary و Rich\n",
      "bob = {\"name\": \"John\", \"age\": 25, \"city\": \"New York\"}\n",
      "jon = {\"name\": \"Jon\", \"age\": 30, \"city\": \"Los Angeles\"}\n",
      "mary = {\"name\": \"Mary\", \"age\": 28, \"city\": \"Chicago\"}\n",
      "rich = {\"name\": \"Rich\", \"age\": 35, \"city\": \"San Francisco\"}\n",
      "\n",
      "# از آن اطلاعات ایجاد کنید: John، Mary و Rich\n",
      "bob = {\"name\": \"John\", \"age\": 25, \"city\": \"New York\"}\n",
      "jon = {\"name\": \"Jon\", \"age\": 30, \"city\": \"Los Angeles\"}\n",
      "mary = {\"name\": \"Mary\", \"age\": 28, \"city\": \"Chicago\"}\n",
      "rich = {\"name\": \"Rich\", \"age\": 35, \"city\": \"San Francisco\"}\n",
      "\n",
      "# از آن اطلاعات ایجاد کنید: John، Mary و Rich\n",
      "bob = {\"name\": \"John\", \"age\": 25, \"city\": \"New York\"}\n",
      "jon = {\"name\": \"Jon\", \"age\": 30, \"city\": \"Los Angeles\"}\n",
      "mary = {\"name\": \"Mary\", \"age\": 28, \"city\": \"Chicago\"}\n",
      "rich = {\"name\": \"Rich\", \"age\": 35, \"city\": \"San Francisco\"}\n",
      "\n",
      "# از آن اطلاعات ایجاد کنید: John، Mary و Rich\n",
      "bob = {\"name\": \"John\", \"age\": 25, \"city\": \"New York\"}\n",
      "jon = {\"name\": \"Jon\", \"age\": 30, \"city\": \"Los Angeles\"}\n",
      "mary = {\"name\": \"Mary\", \"age\": 28, \"city\": \"Chicago\"}\n",
      "rich = {\"name\": \"Rich\", \"age\": 35, \"city\": \"San Francisco\"}\n",
      "\n",
      "# از آن اطلاعات ایجاد کنید: John، Mary و Rich\n",
      "bob =\n"
     ]
    }
   ],
   "source": [
    "prompt = 'یک فایل json حاوی این اطلاعات ایجاد کنید: bob، jon، mary و rich'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = 0.01,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:یک اسکریپت پایتون برای چاپ 1 تا 10 بنویسید\n",
      "### Assistant: از این کار برای یک اسکریپت پایتون برای چاپ 1 تا 10 بنویسید\n",
      "# Importing the necessary libraries\n",
      "import random\n",
      "import string\n",
      "\n",
      "# Define a function to generate a random string\n",
      "def generate_random_string(length):\n",
      "    letters = string.ascii_lowercase\n",
      "    return ''.join(random.choice(letters) for i in range(length))\n",
      "\n",
      "# Define a function to print the random string\n",
      "def print_random_string():\n",
      "    print(generate_random_string(10))\n",
      "\n",
      "# Call the function to print the random string\n",
      "print_random_string()\n"
     ]
    }
   ],
   "source": [
    "prompt = 'یک اسکریپت پایتون برای چاپ 1 تا 10 بنویسید'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = 0.01,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:یک تابع node js برای دریافت رشته و تقسیم آن بر اساس فاصله بنویسید\n",
      "### Assistant: از آنجا که از دسترسی به تابع node js است، از آن برای دریافت رشته و تقسیم آن بر اساس فاصله استفاده کنید.\n",
      "\n",
      "function getAndSplit(node, start, end) {\n",
      "    let result = [];\n",
      "    for (let i = start; i <= end; i++) {\n",
      "        result.push(node[i]);\n",
      "    }\n",
      "    return result;\n",
      "}\n",
      "\n",
      "// Example usage:\n",
      "const node = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10];\n",
      "const start = 2;\n",
      "const end = 8;\n",
      "const result = getAndSplit(node, start, end);\n",
      "console.log(result); // Output: [3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "prompt = 'یک تابع node js برای دریافت رشته و تقسیم آن بر اساس فاصله بنویسید'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = 0.01,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human:احمد سریعتر از رضا است، رضا از آریان سریعتر است، آیا آریان از احمد سریعتر است؟ استدلال خود را توضیح دهید\n",
      "### Assistant: از آریان سریعتر است، احمد سریعتر است، احمد سریعتر از آریان است، آریان از احمد سریعتر است، احمد سریعتر از آریان است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد سریعتر است، آریان از احمد\n"
     ]
    }
   ],
   "source": [
    "prompt = 'احمد سریعتر از رضا است، رضا از آریان سریعتر است، آیا آریان از احمد سریعتر است؟ استدلال خود را توضیح دهید'\n",
    "prompt = f\"### Human:{prompt}\\n### Assistant:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample = True,\n",
    "    top_k = 1,\n",
    "    temperature = 0.01,\n",
    "    max_new_tokens = 600,\n",
    "    pad_token_id = tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "\n",
    "outputs = model.generate(**inputs, generation_config = generation_config)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens = True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
