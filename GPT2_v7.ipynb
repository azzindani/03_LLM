{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline, GPT2Config, TextDataset\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import TrainingArguments, Trainer, set_seed\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = str(pathlib.Path().resolve())\n",
    "DATASET_PATH = MAIN_PATH + '\\\\datasets'\n",
    "MODEL_PATH = MAIN_PATH + '\\\\models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-cased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-cased',\n",
       " 'bert-large-uncased',\n",
       " 'flan-t5-base',\n",
       " 'flan-t5-large',\n",
       " 'flan-t5-small',\n",
       " 'gpt2',\n",
       " 'gpt2-large',\n",
       " 'gpt2-medium']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = os.listdir(MODEL_PATH)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python\\\\LLM_Environment\\\\models\\\\gpt2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = MODEL_PATH + '\\\\' + models[8]\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cached_lm_GPT2Tokenizer_128_Shakespeare_Dataset.txt',\n",
       " 'Customer.csv',\n",
       " 'Html.csv',\n",
       " 'Recipes.csv',\n",
       " 'Recipes_1000.csv',\n",
       " 'Shakespeare_Dataset.txt',\n",
       " 'Taylor_Swift_Lyrics.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = os.listdir(DATASET_PATH)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python\\\\LLM_Environment\\\\datasets\\\\Customer.csv'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = DATASET_PATH + '\\\\' + filenames[1]\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Message</th>\n",
       "      <th>Category</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>problem witth cancelling order</td>\n",
       "      <td>order</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>i fathom that you're experiencing a problem wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>i need assistance with canceling order</td>\n",
       "      <td>order</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>i've ascertained that you need assistance with...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>assistance cancelling order</td>\n",
       "      <td>order</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>i realized you're seeking assistance in cancel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>problem with cancelling purchase</td>\n",
       "      <td>order</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>i've understood, you're experiencing difficult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i have a problem with cancelling order</td>\n",
       "      <td>order</td>\n",
       "      <td>cancel_order</td>\n",
       "      <td>i've got that you're experiencing difficulties...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                  Message Category        Intent  \\\n",
       "0           0          problem witth cancelling order     order  cancel_order   \n",
       "1           1  i need assistance with canceling order     order  cancel_order   \n",
       "2           2             assistance cancelling order     order  cancel_order   \n",
       "3           3        problem with cancelling purchase     order  cancel_order   \n",
       "4           4  i have a problem with cancelling order     order  cancel_order   \n",
       "\n",
       "                                            Response  \n",
       "0  i fathom that you're experiencing a problem wi...  \n",
       "1  i've ascertained that you need assistance with...  \n",
       "2  i realized you're seeking assistance in cancel...  \n",
       "3  i've understood, you're experiencing difficult...  \n",
       "4  i've got that you're experiencing difficulties...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the columns into a single text field for training\n",
    "df['input_text'] = df.apply(lambda row: f\"Query: {row['Message']} Response: {row['Response']}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(df[['input_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_text'],\n",
       "    num_rows: 8100\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8100/8100 [00:05<00:00, 1506.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['input_text'], truncation=True,padding='max_length', max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # No masked language modeling for GPT-2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = './model'\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Trainer and fine-tune the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1518 [00:00<?, ?it/s]d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 33%|███▎      | 500/1518 [09:46<19:50,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3953, 'grad_norm': 3.3885138034820557, 'learning_rate': 3.3530961791831364e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 1000/1518 [19:48<10:22,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0604, 'grad_norm': 3.146991014480591, 'learning_rate': 1.7061923583662716e-05, 'epoch': 1.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 1500/1518 [29:49<00:21,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9858, 'grad_norm': 3.295767307281494, 'learning_rate': 5.928853754940711e-07, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1518/1518 [30:13<00:00,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1813.888, 'train_samples_per_second': 13.397, 'train_steps_per_second': 0.837, 'train_loss': 1.1452121056115674, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1518, training_loss=1.1452121056115674, metrics={'train_runtime': 1813.888, 'train_samples_per_second': 13.397, 'train_steps_per_second': 0.837, 'total_flos': 6346260873216000.0, 'train_loss': 1.1452121056115674, 'epoch': 2.9985185185185186})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(save_path + '/checkpoint-1518')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Ensure pad_token is set to eos_token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model.config.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:606: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:649: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can i know the cancellation charges? Response: i'll get right on it! i can assist you in checking the charges associated with canceling your subscription to our newsletter. to provide you with accurate information, could you please provide me with your email address or any other relevant details related to your account? once i have this information and the necessary details, i will be able to assist your request promptly. thank you for your cooperation, and i look forward to helping you navigate this process. let me know if there's anything else i need to do for you. your satisfaction is our top priority, so please don't hesitate to let us know. we're here to help you every step of the way! happy cancelling! is there anything specific you'd like to know about the cancellation charges or if you have any further questions? feel free to ask. i'm here every day to ensure you receive the information you need. happy cancellation!\"\n",
      "\n",
      "please note that the specific cancellation fees may vary depending on the platform or service provider. if the service you're using is not available or you are unable to find it on our website, please let our customer support team know, as they will guide you through the process and provide any necessary assistance. our goal is to make your experience with us as smooth as possible, ensuring you stay informed and engaged with our services. is it possible to cancel your newsletter subscription? we appreciate your patience and cooperation in resolving this matter. please keep in mind that cancellation policies may change depending upon the company or platform you use. it's always a good idea to review the terms and conditions or contact our dedicated customer service team for more specific information. they are available  at  or through our live chat on . we value your feedback and are committed to ensuring your convenience and satisfaction. how can we assist with this? is your decision to unsubscribe from our newsletters truly important to you? your insights are valuable to us, which is why we strive to keep you informed every time we offer a subscription. feel  to share your thoughts and experiences with the newsletter community! we truly appreciate you reaching out to me for assistance! how may i assist in canceuling your membership? i appreciate any insights you may have. together, we'll work towards finding a solution that meets your needs and ensuring a seamless cancellation experience for all our subscribers. remember, your privacy and preferences are of utmost importance to the security and privacy of our customers. rest assured, our privacy policy is designed to protect your information as it is handled by our security team. should you require any additional information or have specific concerns, feel safe in knowing that i am here for them. enjoy the benefits of subscribing to one of your esteemed newsletters! if at any point you encounter any difficulties or need further assistance, don: we are here 24/7 to support you and ensure a smooth cancellation process for our valued subscribers! let's work together to resolve this for ya! together we can make sure you enjoy our content and stay connected with other subscribers who share the same goals and aspirations. stay tuned! your loyalty is of course valued, but please feel assured that we will do our best to address your concerns and make the subscription process as seamless as we possibly can. may you join us in the exciting future! stay with me, my loyal assistant, or our esteemed customer assistance team! enjoy your stay at our prestigious newsletter! are you subscribed to any of these newsletters? let them know how they can help us improve our offerings and enhance your overall experience! thank ya for choosing to subscribe to these valuable newsletters!\"\n"
     ]
    }
   ],
   "source": [
    "def generate_response(input_text):\n",
    "    # Tokenize input with padding and attention mask\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Generate response with a high max_length\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],  # Include attention mask\n",
    "        max_length=1024,  # Set to a high value to ensure complete responses\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.5,  # Adjust temperature as needed\n",
    "        top_k=50,  # Adjust top_k as needed\n",
    "        top_p=0.95,  # Adjust top_p as needed\n",
    "        no_repeat_ngram_size=2,  # Prevent repeating phrases\n",
    "        early_stopping=True  # Stop early if end of sentence token is generated\n",
    "    )\n",
    "\n",
    "    # Decode and print the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "\n",
    "# Example usage\n",
    "generate_response(\"How can i know the cancellation charges\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
