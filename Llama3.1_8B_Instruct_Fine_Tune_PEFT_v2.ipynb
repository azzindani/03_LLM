{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"#!pip install --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-11-25T01:35:52.948435Z","iopub.execute_input":"2024-11-25T01:35:52.948790Z","iopub.status.idle":"2024-11-25T01:36:41.570330Z","shell.execute_reply.started":"2024-11-25T01:35:52.948752Z","shell.execute_reply":"2024-11-25T01:36:41.569186Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-11-25T01:36:41.572275Z","iopub.execute_input":"2024-11-25T01:36:41.573133Z","iopub.status.idle":"2024-11-25T01:37:00.709545Z","shell.execute_reply.started":"2024-11-25T01:36:41.573068Z","shell.execute_reply":"2024-11-25T01:37:00.708632Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-11-25T01:37:00.710688Z","iopub.execute_input":"2024-11-25T01:37:00.710956Z","iopub.status.idle":"2024-11-25T01:37:00.717287Z","shell.execute_reply.started":"2024-11-25T01:37:00.710931Z","shell.execute_reply":"2024-11-25T01:37:00.716451Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"#url = 'https://huggingface.co/Qwen/Qwen2.5-0.5B'\n#model_name = url.split('.co/')[-1]\n\nmodel_name = 'unsloth/Meta-Llama-3.1-8B-Instruct'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-11-25T01:37:00.718964Z","iopub.execute_input":"2024-11-25T01:37:00.719294Z","iopub.status.idle":"2024-11-25T01:37:00.748977Z","shell.execute_reply.started":"2024-11-25T01:37:00.719242Z","shell.execute_reply":"2024-11-25T01:37:00.748141Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-11-25T01:37:00.749909Z","iopub.execute_input":"2024-11-25T01:37:00.750185Z","iopub.status.idle":"2024-11-25T01:37:00.761506Z","shell.execute_reply.started":"2024-11-25T01:37:00.750162Z","shell.execute_reply":"2024-11-25T01:37:00.760470Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:37:00.762893Z","iopub.execute_input":"2024-11-25T01:37:00.763304Z","iopub.status.idle":"2024-11-25T01:44:20.560259Z","shell.execute_reply.started":"2024-11-25T01:37:00.763250Z","shell.execute_reply":"2024-11-25T01:44:20.559501Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/969 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59589f0e107248fd9be19b18a011a111"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f1b9110a78049aca30892b0b85e6ee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cfd6d90c5064f92acf2421d3bc812fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"791b14a238b14c62b2d00ed85ea67f5b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32ba00ca567b4cadb73de55c787bfca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8543a2792a124ed0b6e9d5c588938ee8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11437dad899b46329df91ed4ea7f76b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b42276460fd64556853c0bdb2e610dc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/234 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de13162c11a4d1db55993cc2602176e"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:20.561364Z","iopub.execute_input":"2024-11-25T01:44:20.561697Z","iopub.status.idle":"2024-11-25T01:44:20.569948Z","shell.execute_reply.started":"2024-11-25T01:44:20.561658Z","shell.execute_reply":"2024-11-25T01:44:20.569048Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 4540600320\nTrainable parameters : 1050939392\nTrainable percentage: 23.15%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:20.571106Z","iopub.execute_input":"2024-11-25T01:44:20.571369Z","iopub.status.idle":"2024-11-25T01:44:22.285851Z","shell.execute_reply.started":"2024-11-25T01:44:20.571345Z","shell.execute_reply":"2024-11-25T01:44:22.285138Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7307cb35fc46472eadbfdfe1ed590fed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2be1bcc8a104d96ad71a4f47d32adbb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/340 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"372f3d8ec3b84891b66a92837d2c2673"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"#url = 'https://huggingface.co/datasets/KingNish/reasoning-base-20k'\n#dataset_name = url.split('datasets/')[-1]\n\ndataset_name = 'yahma/alpaca-cleaned'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:22.286961Z","iopub.execute_input":"2024-11-25T01:44:22.287261Z","iopub.status.idle":"2024-11-25T01:44:22.291103Z","shell.execute_reply.started":"2024-11-25T01:44:22.287226Z","shell.execute_reply":"2024-11-25T01:44:22.290161Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 384","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:22.293702Z","iopub.execute_input":"2024-11-25T01:44:22.293946Z","iopub.status.idle":"2024-11-25T01:44:22.304521Z","shell.execute_reply.started":"2024-11-25T01:44:22.293921Z","shell.execute_reply":"2024-11-25T01:44:22.303709Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:22.305310Z","iopub.execute_input":"2024-11-25T01:44:22.305550Z","iopub.status.idle":"2024-11-25T01:44:25.536308Z","shell.execute_reply.started":"2024-11-25T01:44:22.305526Z","shell.execute_reply":"2024-11-25T01:44:25.535429Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/11.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ac9a2a0eec489bafae52e1cab66ff7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"alpaca_data_cleaned.json:   0%|          | 0.00/44.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d42ab6e488af42c89bb195d4f2ac3432"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/51760 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57120d521c434e71864b0752ab895bee"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['output', 'input', 'instruction'],\n    num_rows: 51760\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:25.537336Z","iopub.execute_input":"2024-11-25T01:44:25.537598Z","iopub.status.idle":"2024-11-25T01:44:25.543846Z","shell.execute_reply.started":"2024-11-25T01:44:25.537572Z","shell.execute_reply":"2024-11-25T01:44:25.543166Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:25.544785Z","iopub.execute_input":"2024-11-25T01:44:25.545014Z","iopub.status.idle":"2024-11-25T01:44:25.567926Z","shell.execute_reply.started":"2024-11-25T01:44:25.544991Z","shell.execute_reply":"2024-11-25T01:44:25.567030Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                              output input  \\\n0  1. Eat a balanced and nutritious diet: Make su...         \n1  The three primary colors are red, blue, and ye...         \n2  An atom is the basic building block of all mat...         \n3  There are several ways to reduce air pollution...         \n4  I had to make a difficult decision when I was ...         \n\n                                         instruction  \n0               Give three tips for staying healthy.  \n1                 What are the three primary colors?  \n2                 Describe the structure of an atom.  \n3                   How can we reduce air pollution?  \n4  Pretend you are a project manager of a constru...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>output</th>\n      <th>input</th>\n      <th>instruction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1. Eat a balanced and nutritious diet: Make su...</td>\n      <td></td>\n      <td>Give three tips for staying healthy.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>The three primary colors are red, blue, and ye...</td>\n      <td></td>\n      <td>What are the three primary colors?</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>An atom is the basic building block of all mat...</td>\n      <td></td>\n      <td>Describe the structure of an atom.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>There are several ways to reduce air pollution...</td>\n      <td></td>\n      <td>How can we reduce air pollution?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I had to make a difficult decision when I was ...</td>\n      <td></td>\n      <td>Pretend you are a project manager of a constru...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:25.568752Z","iopub.execute_input":"2024-11-25T01:44:25.568975Z","iopub.status.idle":"2024-11-25T01:44:25.576214Z","shell.execute_reply.started":"2024-11-25T01:44:25.568952Z","shell.execute_reply":"2024-11-25T01:44:25.575337Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'output': '1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\\n\\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\\n\\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.',\n 'input': '',\n 'instruction': 'Give three tips for staying healthy.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:25.577309Z","iopub.execute_input":"2024-11-25T01:44:25.577626Z","iopub.status.idle":"2024-11-25T01:44:25.586302Z","shell.execute_reply.started":"2024-11-25T01:44:25.577591Z","shell.execute_reply":"2024-11-25T01:44:25.585492Z"}},"outputs":[{"name":"stdout","text":"['output', 'input', 'instruction']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:25.587330Z","iopub.execute_input":"2024-11-25T01:44:25.587954Z","iopub.status.idle":"2024-11-25T01:44:25.596704Z","shell.execute_reply.started":"2024-11-25T01:44:25.587928Z","shell.execute_reply":"2024-11-25T01:44:25.596100Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  instruction = examples['instruction']\n  input = examples['input']\n  output = examples['output']\n  \n  text = prompt_format.format(instruction, input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:25.597599Z","iopub.execute_input":"2024-11-25T01:44:25.597907Z","iopub.status.idle":"2024-11-25T01:44:25.607997Z","shell.execute_reply.started":"2024-11-25T01:44:25.597868Z","shell.execute_reply":"2024-11-25T01:44:25.607141Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:25.609156Z","iopub.execute_input":"2024-11-25T01:44:25.609485Z","iopub.status.idle":"2024-11-25T01:44:26.173907Z","shell.execute_reply.started":"2024-11-25T01:44:25.609451Z","shell.execute_reply":"2024-11-25T01:44:26.172666Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef0e323508504c30bdd0e29441e8c9f5"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"cZya4tPEWUc9","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:26.175437Z","iopub.execute_input":"2024-11-25T01:44:26.176164Z","iopub.status.idle":"2024-11-25T01:44:26.181713Z","shell.execute_reply.started":"2024-11-25T01:44:26.176118Z","shell.execute_reply":"2024-11-25T01:44:26.180755Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Input:\n\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|eot_id|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:26.183836Z","iopub.execute_input":"2024-11-25T01:44:26.184153Z","iopub.status.idle":"2024-11-25T01:44:26.212794Z","shell.execute_reply.started":"2024-11-25T01:44:26.184114Z","shell.execute_reply":"2024-11-25T01:44:26.211909Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:26.213921Z","iopub.execute_input":"2024-11-25T01:44:26.214245Z","iopub.status.idle":"2024-11-25T01:44:34.038253Z","shell.execute_reply.started":"2024-11-25T01:44:26.214201Z","shell.execute_reply":"2024-11-25T01:44:34.037371Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d5078d9ec8647d0bf7e49f5207a5366"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.039346Z","iopub.execute_input":"2024-11-25T01:44:34.039619Z","iopub.status.idle":"2024-11-25T01:44:34.046668Z","shell.execute_reply.started":"2024-11-25T01:44:34.039593Z","shell.execute_reply":"2024-11-25T01:44:34.045730Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nGive three tips for staying healthy.\n\n### Input:\n\n\n### Response:\n1. Eat a balanced and nutritious diet: Make sure your meals are inclusive of a variety of fruits and vegetables, lean protein, whole grains, and healthy fats. This helps to provide your body with the essential nutrients to function at its best and can help prevent chronic diseases.\n\n2. Engage in regular physical activity: Exercise is crucial for maintaining strong bones, muscles, and cardiovascular health. Aim for at least 150 minutes of moderate aerobic exercise or 75 minutes of vigorous exercise each week.\n\n3. Get enough sleep: Getting enough quality sleep is crucial for physical and mental well-being. It helps to regulate mood, improve cognitive function, and supports healthy growth and immune function. Aim for 7-9 hours of sleep each night.<|eot_id|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.048047Z","iopub.execute_input":"2024-11-25T01:44:34.048465Z","iopub.status.idle":"2024-11-25T01:44:34.115287Z","shell.execute_reply.started":"2024-11-25T01:44:34.048420Z","shell.execute_reply":"2024-11-25T01:44:34.114487Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.116485Z","iopub.execute_input":"2024-11-25T01:44:34.116847Z","iopub.status.idle":"2024-11-25T01:44:34.127056Z","shell.execute_reply.started":"2024-11-25T01:44:34.116811Z","shell.execute_reply":"2024-11-25T01:44:34.126292Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.128117Z","iopub.execute_input":"2024-11-25T01:44:34.128366Z","iopub.status.idle":"2024-11-25T01:44:34.151304Z","shell.execute_reply.started":"2024-11-25T01:44:34.128343Z","shell.execute_reply":"2024-11-25T01:44:34.150523Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  Below is an instruction that describes a task,...   \n1  Below is an instruction that describes a task,...   \n2  Below is an instruction that describes a task,...   \n3  Below is an instruction that describes a task,...   \n4  Below is an instruction that describes a task,...   \n\n                                           input_ids  \\\n0  [128009, 128009, 128009, 128009, 128009, 12800...   \n1  [128009, 128009, 128009, 128009, 128009, 12800...   \n2  [128009, 128009, 128009, 128009, 128009, 12800...   \n3  [128009, 128009, 128009, 128009, 128009, 12800...   \n4  [128009, 128009, 128009, 128009, 128009, 12800...   \n\n                                      attention_mask  \n0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[128009, 128009, 128009, 128009, 128009, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[128009, 128009, 128009, 128009, 128009, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[128009, 128009, 128009, 128009, 128009, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[128009, 128009, 128009, 128009, 128009, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Below is an instruction that describes a task,...</td>\n      <td>[128009, 128009, 128009, 128009, 128009, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.152235Z","iopub.execute_input":"2024-11-25T01:44:34.152564Z","iopub.status.idle":"2024-11-25T01:44:34.157931Z","shell.execute_reply.started":"2024-11-25T01:44:34.152525Z","shell.execute_reply":"2024-11-25T01:44:34.156947Z"}},"outputs":[{"name":"stdout","text":"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nAnalyze the given poem and identify its main theme.\n\n### Input:\nTwo roads diverged in a yellow wood,\\nAnd sorry I could not travel both\\nAnd be one traveler, long I stood\\nAnd looked down one as far as I could\\nTo where it bent in the undergrowth;\\n\\nThen took the other, as just as fair,\\nAnd having perhaps the better claim,\\nBecause it was grassy and wanted wear;\\nThough as for that the passing there\\nHad worn them really about the same,\\nThe roads that morning equally lay\\nIn leaves no step had trodden black.\\nOh, I left the first for another day!\\nYet knowing how way leads on to way,\\nI doubted if I should ever come back.\\n\\nI shall be telling this with a sigh\\nSomewhere ages and ages hence:\\nTwo roads diverged in a wood, and I—\\nI took the one less traveled by,\\nAnd that has made all the difference.\n\n### Response:\nThe main theme of the poem is the importance of making choices and the impact of those choices on one's life. The speaker is faced with a decision between two paths and ultimately chooses the one less traveled, which ultimately shapes their life experience.<|eot_id|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.158776Z","iopub.execute_input":"2024-11-25T01:44:34.158984Z","iopub.status.idle":"2024-11-25T01:44:34.168015Z","shell.execute_reply.started":"2024-11-25T01:44:34.158964Z","shell.execute_reply":"2024-11-25T01:44:34.167224Z"}},"outputs":[{"name":"stdout","text":"[128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128000, 39314, 374, 459, 7754, 430, 16964, 264, 3465, 11, 35526, 449, 459, 1988, 430, 5825, 4726, 2317, 13, 9842, 264, 2077, 430, 36001, 45695, 279, 1715, 382, 14711, 30151, 512, 2127, 56956, 279, 2728, 33894, 323, 10765, 1202, 1925, 7057, 382, 14711, 5688, 512, 11874, 19795, 37441, 3640, 304, 264, 14071, 7732, 27362, 77, 3112, 14931, 358, 1436, 539, 5944, 2225, 1734, 3112, 387, 832, 63865, 11, 1317, 358, 14980, 1734, 3112, 7111, 1523, 832, 439, 3117, 439, 358, 1436, 1734, 1271, 1405, 433, 30280, 304, 279, 1234, 74189, 18364, 77, 1734, 12487, 3952, 279, 1023, 11, 439, 1120, 439, 6762, 27362, 77, 3112, 3515, 8530, 279, 2731, 3802, 27362, 77, 18433, 433, 574, 16763, 88, 323, 4934, 10051, 18364, 77, 27831, 439, 369, 430, 279, 12579, 1070, 1734, 56568, 24634, 1124, 2216, 922, 279, 1890, 27362, 89330, 19795, 430, 6693, 18813, 11203, 1734, 644, 11141, 912, 3094, 1047, 8348, 11025, 3776, 7255, 77, 12174, 11, 358, 2163, 279, 1176, 369, 2500, 1938, 15114, 77, 29174, 14392, 1268, 1648, 11767, 389, 311, 1648, 27362, 77, 40, 93762, 422, 358, 1288, 3596, 2586, 1203, 7255, 77, 1734, 40, 4985, 387, 11890, 420, 449, 264, 31238, 1734, 50982, 61752, 17051, 323, 17051, 16472, 7338, 77, 11874, 19795, 37441, 3640, 304, 264, 7732, 11, 323, 358, 2345, 59, 77, 40, 3952, 279, 832, 2753, 31796, 555, 27362, 77, 3112, 430, 706, 1903, 682, 279, 6811, 382, 14711, 6075, 512, 791, 1925, 7057, 315, 279, 33894, 374, 279, 12939, 315, 3339, 11709, 323, 279, 5536, 315, 1884, 11709, 389, 832, 596, 2324, 13, 578, 19114, 374, 17011, 449, 264, 5597, 1990, 1403, 13006, 323, 13967, 41011, 279, 832, 2753, 31796, 11, 902, 13967, 21483, 872, 2324, 3217, 13, 128009]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.173476Z","iopub.execute_input":"2024-11-25T01:44:34.173945Z","iopub.status.idle":"2024-11-25T01:44:34.182348Z","shell.execute_reply.started":"2024-11-25T01:44:34.173920Z","shell.execute_reply":"2024-11-25T01:44:34.181500Z"}},"outputs":[{"name":"stdout","text":"[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.183362Z","iopub.execute_input":"2024-11-25T01:44:34.183634Z","iopub.status.idle":"2024-11-25T01:44:34.192821Z","shell.execute_reply.started":"2024-11-25T01:44:34.183611Z","shell.execute_reply":"2024-11-25T01:44:34.191982Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.193766Z","iopub.execute_input":"2024-11-25T01:44:34.194101Z","iopub.status.idle":"2024-11-25T01:44:34.201803Z","shell.execute_reply.started":"2024-11-25T01:44:34.194046Z","shell.execute_reply":"2024-11-25T01:44:34.200964Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.202787Z","iopub.execute_input":"2024-11-25T01:44:34.203023Z","iopub.status.idle":"2024-11-25T01:44:34.210884Z","shell.execute_reply.started":"2024-11-25T01:44:34.203001Z","shell.execute_reply":"2024-11-25T01:44:34.210119Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 32\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.211965Z","iopub.execute_input":"2024-11-25T01:44:34.212246Z","iopub.status.idle":"2024-11-25T01:44:34.222574Z","shell.execute_reply.started":"2024-11-25T01:44:34.212221Z","shell.execute_reply":"2024-11-25T01:44:34.221770Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:34.223517Z","iopub.execute_input":"2024-11-25T01:44:34.223786Z","iopub.status.idle":"2024-11-25T01:44:35.346046Z","shell.execute_reply.started":"2024-11-25T01:44:34.223761Z","shell.execute_reply":"2024-11-25T01:44:35.345160Z"}},"outputs":[{"name":"stdout","text":"trainable params: 83,886,080 || all params: 8,114,147,328 || trainable%: 1.0338\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"model","metadata":{"id":"ikF6Yfkz1myd","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:35.347258Z","iopub.execute_input":"2024-11-25T01:44:35.347630Z","iopub.status.idle":"2024-11-25T01:44:35.363309Z","shell.execute_reply.started":"2024-11-25T01:44:35.347591Z","shell.execute_reply":"2024-11-25T01:44:35.362483Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=14336, out_features=32, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:35.364393Z","iopub.execute_input":"2024-11-25T01:44:35.364689Z","iopub.status.idle":"2024-11-25T01:44:35.387959Z","shell.execute_reply.started":"2024-11-25T01:44:35.364660Z","shell.execute_reply":"2024-11-25T01:44:35.387018Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 4624486400\nTrainable parameters : 83886080\nTrainable percentage: 1.81%\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:35.388970Z","iopub.execute_input":"2024-11-25T01:44:35.389257Z","iopub.status.idle":"2024-11-25T01:44:35.396352Z","shell.execute_reply.started":"2024-11-25T01:44:35.389227Z","shell.execute_reply":"2024-11-25T01:44:35.395627Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 1\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:35.397315Z","iopub.execute_input":"2024-11-25T01:44:35.397651Z","iopub.status.idle":"2024-11-25T01:44:35.445672Z","shell.execute_reply.started":"2024-11-25T01:44:35.397609Z","shell.execute_reply":"2024-11-25T01:44:35.444772Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Nov25_01-44-35_8e1e0daec8e5,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:35.446844Z","iopub.execute_input":"2024-11-25T01:44:35.447209Z","iopub.status.idle":"2024-11-25T01:44:37.313618Z","shell.execute_reply.started":"2024-11-25T01:44:35.447169Z","shell.execute_reply":"2024-11-25T01:44:37.312930Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7a07c05e54e0>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T01:44:37.314593Z","iopub.execute_input":"2024-11-25T01:44:37.314837Z","iopub.status.idle":"2024-11-25T02:58:03.100904Z","shell.execute_reply.started":"2024-11-25T01:44:37.314812Z","shell.execute_reply":"2024-11-25T02:58:03.100199Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 1\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 83,886,080\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113012444444495, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13a2d474d6bb4fb19f310502cc972fed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241125_015048-e0y1b07e</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/e0y1b07e' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/e0y1b07e' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/e0y1b07e</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 1:07:00, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.945500</td>\n      <td>1.666909</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.513400</td>\n      <td>1.264067</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.148300</td>\n      <td>1.091832</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.054700</td>\n      <td>1.068342</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.018200</td>\n      <td>1.058547</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.021500</td>\n      <td>1.051959</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.007500</td>\n      <td>1.048871</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.990400</td>\n      <td>1.044964</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.018000</td>\n      <td>1.043976</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.984400</td>\n      <td>1.043816</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=1.1701847648620605, metrics={'train_runtime': 4405.3455, 'train_samples_per_second': 0.182, 'train_steps_per_second': 0.045, 'total_flos': 1.41423148007424e+16, 'train_loss': 1.1701847648620605, 'epoch': 0.08888888888888889})"},"metadata":{}}],"execution_count":39},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T02:58:03.101886Z","iopub.execute_input":"2024-11-25T02:58:03.102128Z","iopub.status.idle":"2024-11-25T03:01:23.693181Z","shell.execute_reply.started":"2024-11-25T02:58:03.102104Z","shell.execute_reply":"2024-11-25T03:01:23.692310Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 03:16]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 1.0438162088394165, 'eval_runtime': 200.579, 'eval_samples_per_second': 0.997, 'eval_steps_per_second': 0.249, 'epoch': 0.08888888888888889}\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:01:23.694267Z","iopub.execute_input":"2024-11-25T03:01:23.694546Z","iopub.status.idle":"2024-11-25T03:01:25.603663Z","shell.execute_reply.started":"2024-11-25T03:01:23.694505Z","shell.execute_reply":"2024-11-25T03:01:25.602648Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct/snapshots/da09a334d51a646967eec17cb412575702b3d767/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct/snapshots/da09a334d51a646967eec17cb412575702b3d767/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:01:25.604901Z","iopub.execute_input":"2024-11-25T03:01:25.605623Z","iopub.status.idle":"2024-11-25T03:01:25.746709Z","shell.execute_reply.started":"2024-11-25T03:01:25.605582Z","shell.execute_reply":"2024-11-25T03:01:25.745733Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:01:25.748003Z","iopub.execute_input":"2024-11-25T03:01:25.748445Z","iopub.status.idle":"2024-11-25T03:01:25.761705Z","shell.execute_reply.started":"2024-11-25T03:01:25.748409Z","shell.execute_reply":"2024-11-25T03:01:25.760880Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:01:25.762945Z","iopub.execute_input":"2024-11-25T03:01:25.763369Z","iopub.status.idle":"2024-11-25T03:01:27.283553Z","shell.execute_reply.started":"2024-11-25T03:01:25.763335Z","shell.execute_reply":"2024-11-25T03:01:27.282589Z"}},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:01:27.284679Z","iopub.execute_input":"2024-11-25T03:01:27.285015Z","iopub.status.idle":"2024-11-25T03:02:53.821130Z","shell.execute_reply.started":"2024-11-25T03:01:27.284977Z","shell.execute_reply":"2024-11-25T03:02:53.820289Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct/snapshots/da09a334d51a646967eec17cb412575702b3d767/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 128,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 4096,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 14336,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 32,\n  \"num_key_value_heads\": 8,\n  \"pad_token_id\": 128004,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 8.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": false,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"unsloth_version\": \"2024.9\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct/snapshots/da09a334d51a646967eec17cb412575702b3d767/model.safetensors.index.json\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"pad_token_id\": 128004\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61701fa9b6dd48ee85b2f57eacad5463"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/Meta-Llama-3.1-8B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct/snapshots/da09a334d51a646967eec17cb412575702b3d767/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"max_length\": 131072,\n  \"pad_token_id\": 128004,\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\n","output_type":"stream"},{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n    (layers): ModuleList(\n      (0-31): 32 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:02:53.822362Z","iopub.execute_input":"2024-11-25T03:02:53.823013Z","iopub.status.idle":"2024-11-25T03:02:53.832178Z","shell.execute_reply.started":"2024-11-25T03:02:53.822965Z","shell.execute_reply":"2024-11-25T03:02:53.831429Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 4540600320\nTrainable parameters : 1050939392\nTrainable percentage: 23.15%\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:02:53.833293Z","iopub.execute_input":"2024-11-25T03:02:53.833540Z","iopub.status.idle":"2024-11-25T03:02:53.858831Z","shell.execute_reply.started":"2024-11-25T03:02:53.833516Z","shell.execute_reply":"2024-11-25T03:02:53.858124Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n        (layers): ModuleList(\n          (0-31): 32 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=1024, bias=False)\n                  (default): Linear(in_features=32, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=4096, out_features=32, bias=False)\n                  (default): Linear(in_features=4096, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=14336, bias=False)\n                  (default): Linear(in_features=32, out_features=14336, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=14336, out_features=32, bias=False)\n                  (default): Linear(in_features=14336, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=32, out_features=4096, bias=False)\n                  (default): Linear(in_features=32, out_features=4096, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((4096,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:02:53.859913Z","iopub.execute_input":"2024-11-25T03:02:53.860157Z","iopub.status.idle":"2024-11-25T03:02:53.886186Z","shell.execute_reply.started":"2024-11-25T03:02:53.860128Z","shell.execute_reply":"2024-11-25T03:02:53.885387Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 4708372480\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":48},{"cell_type":"markdown","source":"## 14 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt, inputs):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      inputs,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:02:53.887295Z","iopub.execute_input":"2024-11-25T03:02:53.887971Z","iopub.status.idle":"2024-11-25T03:02:53.893434Z","shell.execute_reply.started":"2024-11-25T03:02:53.887933Z","shell.execute_reply":"2024-11-25T03:02:53.892547Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def post_assistant(prompt, inputs):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      inputs,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:16:51.963447Z","iopub.execute_input":"2024-11-25T03:16:51.963806Z","iopub.status.idle":"2024-11-25T03:16:51.970017Z","shell.execute_reply.started":"2024-11-25T03:16:51.963773Z","shell.execute_reply":"2024-11-25T03:16:51.969253Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:02:53.906772Z","iopub.execute_input":"2024-11-25T03:02:53.907435Z","iopub.status.idle":"2024-11-25T03:02:53.917168Z","shell.execute_reply.started":"2024-11-25T03:02:53.907398Z","shell.execute_reply":"2024-11-25T03:02:53.916424Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:16:58.734516Z","iopub.execute_input":"2024-11-25T03:16:58.734855Z","iopub.status.idle":"2024-11-25T03:18:42.397141Z","shell.execute_reply.started":"2024-11-25T03:16:58.734826Z","shell.execute_reply":"2024-11-25T03:18:42.396357Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    4540600320                     |                     4708372480                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Describe   |  completes the request.  ### Instruction: Describe \nthe implications of Artificial Intelligence  ###    |  the implications of Artificial Intelligence  ###  \nInput:   ### Response: The implications of          |  Input:   ### Response: The implications of        \nArtificial Intelligence (AI) are far-reaching and   |  Artificial Intelligence (AI) are far-reaching and \nmultifaceted, impacting various aspects of          |  multifaceted, impacting various aspects of        \nsociety, economy, and individual lives. Some of     |  society, economy, and individual lives. Some of   \nthe key implications include:  1. **Job Market      |  the key implications include:  1. **Job Market    \nDisruption**: AI has the potential to automate      |  Disruption**: AI has the potential to automate    \nmany jobs, potentially leading to significant       |  many jobs, potentially leading to significant     \nunemployment, especially in sectors where tasks     |  unemployment, especially in sectors where tasks   \nare repetitive or can be easily automated. This     |  are repetitive or can be easily automated. This   \ncould exacerbate income inequality and require      |  could exacerbate income inequality and require    \nsignificant social and economic adjustments.  2.    |  significant social and economic adjustments.  2.  \n**Enhanced Productivity**: On the other hand, AI    |  **Enhanced Productivity**: On the other hand, AI  \ncan significantly boost productivity by automating  |  can significantly boost productivity by automating\ntasks, freeing up human resources for more          |  tasks, freeing up human resources for more        \nstrategic and creative work. This could lead to     |  strategic and creative work. This could lead to   \neconomic growth and improved standards of living.   |  economic growth and improved standards of living. \n3. **Improved Decision Making**: AI can analyze     |  3. **Improved Decision Making**: AI can analyze   \nvast amounts of data, providing insights that       |  vast amounts of data, providing insights that     \nhumans might miss. This can lead to better          |  humans might miss. This can lead to better        \ndecision-making in various fields, including        |  decision-making in various fields, including      \nhealthcare, finance, and education.  4. **Ethical   |  healthcare, finance, and education.  4. **Ethical \nConcerns**: The development and deployment of AI    |  Concerns**: The development and deployment of AI  \nraise ethical concerns, such as privacy, bias, and  |  raise ethical concerns, such as privacy, bias, and\naccountability. Ensuring that AI systems are        |  accountability. Ensuring that AI systems are      \ntransparent, fair, and accountable is crucial to    |  transparent, fair, and aligned with human values  \nmaintaining public trust.  5. **Security Risks**:   |  is crucial.  5. **Security Risks**: AI can be used\nAI can be used to create sophisticated cyber        |  to create sophisticated cyber threats, making     \nthreats, making cybersecurity a significant         |  cybersecurity a significant challenge. AI-powered \nchallenge. The potential for AI to be used in       |  systems can also be vulnerable to attacks,        \nmalicious ways, such as in autonomous weapons,      |  highlighting the need for robust security         \nalso raises ethical and legal questions.  6.        |  measures.  6. **Social Impact**: AI has the       \n**Social Impact**: AI has the potential to enhance  |  potential to enhance social services, such as     \nsocial services, such as healthcare and education,  |  healthcare and education, by providing            \nby providing personalized support and improving     |  personalized support and improving access to      \naccess to information. However, it also raises      |  information. However, it also raises concerns     \nconcerns about social isolation and the potential   |  about social isolation and the potential for AI to\nfor AI to exacerbate existing social inequalities.  |  exacerbate existing social inequalities.  7.      \n7. **Economic Impact**: The economic implications   |  **Economic Impact**: The economic implications of \nof AI are significant, with the potential for both  |  AI are profound, with the potential for           \neconomic growth and disruption. The distribution    |  significant economic growth through increased     \nof benefits and the management of potential         |  productivity and the creation of new industries.  \nnegative impacts will be crucial.  8. **Legal and   |  However, there are also concerns about the        \nRegulatory Frameworks**: The development and        |  distribution of benefits and the potential for    \ndeployment of AI require the creation of legal and  |  economic disruption.  8. **Environmental Impact**:\nregulatory frameworks that can address its          |  The development and deployment of AI can have     \nimplications. This includes issues related to       |  environmental implications, such as the energy    \nliability, privacy, and intellectual property.  9.  |  consumption required to power AI systems and the  \n**Education and Training**: As AI transforms the    |  potential for AI to contribute to environmental   \njob market, there will be a need for significant    |  degradation through its applications.  9. **Legal \ninvestments in education and training to ensure     |  and Regulatory Frameworks**: The rapid development\nthat the workforce has the skills required to work  |  of AI necessitates the creation of legal and      \nalongside AI.  10. **Existential Risks**: Some      |  regulatory frameworks that can address its        \nexperts worry about the potential existential       |  implications. This includes issues related to     \nrisks associated with superintelligent AI, which    |  liability, intellectual property, and data        \ncould potentially surpass human intelligence and    |  privacy.  10. **Human-AI Collaboration**: The     \npose a threat to human existence.  In conclusion,   |  future of work and society will likely involve    \nthe implications of AI are profound and             |  humans and AI systems collaborating closely. This \nmultifaceted, requiring careful consideration and   |  requires a focus on education and training that   \nplanning to ensure that its benefits are maximized  |  prepares individuals for a world where AI is a    \nwhile minimizing its risks. Addressing these        |  significant partner.  In conclusion, the          \nimplications will be crucial for creating a future  |  implications of AI are complex and multifaceted,  \nwhere AI enhances human life without compromising   |  requiring careful consideration and planning to   \nit.                                                 |  ensure that its benefits are maximized while its  \n                                                    |  risks are mitigated. By understanding these       \n                                                    |  implications, we can work towards a future where  \n                                                    |  AI enhances human life without compromising our   \n                                                    |  values or exacerbating existing challenges.       \n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:18:42.398947Z","iopub.execute_input":"2024-11-25T03:18:42.399519Z","iopub.status.idle":"2024-11-25T03:19:21.826781Z","shell.execute_reply.started":"2024-11-25T03:18:42.399470Z","shell.execute_reply":"2024-11-25T03:19:21.825892Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    4540600320                     |                     4708372480                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Three      |  completes the request.  ### Instruction: Three    \npositive integers have a sum of 72 and are in the   |  positive integers have a sum of 72 and are in the \nratio 1:3:4. What is the least of these three       |  ratio 1:3:4. What is the least of these three     \nintegers?  ### Input:   ### Response: ## Step 1:    |  integers?  ### Input:   ### Response: ## Step 1:  \nUnderstand the given information We are given       |  Understand the given ratio and sum of the three   \nthree positive integers that are in the ratio       |  integers. The three integers are in the ratio     \n1:3:4 and have a sum of 72.  ## Step 2: Express     |  1:3:4, which means they can be represented as x,  \nthe integers using the given ratio Let's express    |  3x, and 4x for some positive integer x.  ## Step  \nthe three integers as x, 3x, and 4x, where x is     |  2: Express the sum of the three integers using the\nthe least of the three integers.  ## Step 3: Write  |  ratio. Given that the sum of the three integers is\nan equation based on the given information Since    |  72, we can write the equation x + 3x + 4x = 72.   \nthe sum of the three integers is 72, we can write   |  ## Step 3: Solve the equation for x. Combine like \nthe equation x + 3x + 4x = 72.  ## Step 4: Solve    |  terms to get 8x = 72, then divide both sides by 8 \nthe equation Combine like terms to get 8x = 72.     |  to find x.  ## Step 4: Calculate the value of x. x\n## Step 5: Find the value of x Divide both sides    |  = 72 / 8 = 9.  ## Step 5: Find the least of the   \nof the equation by 8 to get x = 9.  ## Step 6:      |  three integers. The least of the three integers is\nFind the least of the three integers Since x is     |  represented by x, which we found to be 9.  The    \nthe least of the three integers, the least of the   |  final answer is: $\\boxed{9}$                      \nthree integers is 9.  The final answer is:          |                                                    \n$\\boxed{9}$                                         |                                                    \n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:06:53.606025Z","iopub.execute_input":"2024-11-25T03:06:53.606300Z","iopub.status.idle":"2024-11-25T03:09:17.993071Z","shell.execute_reply.started":"2024-11-25T03:06:53.606273Z","shell.execute_reply":"2024-11-25T03:09:17.992222Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    4540600320                     |                     4708372480                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  <|begin_of_text|>Below is an instruction that     \npaired with an input that provides further          |  describes a task, paired with an input that       \ncontext. Write a response that appropriately        |  provides further context. Write a response that   \ncompletes the request.  ### Instruction: Construct  |  appropriately completes the request.  ###         \na story of love and loss.  ### Input:   ###         |  Instruction: Construct a story of love and loss.  \nResponse: Once upon a time, in a quaint village     |  ### Input:   ### Response: Once upon a time, in a \nnestled between rolling hills and vast fields of    |  quaint village nestled between rolling hills and  \ngolden wheat, there lived a young couple named      |  vast fields of golden wheat, there lived a young  \nSophia and Alexander. Their love story was one for  |  couple named Sophia and Alexander. Their love     \nthe ages, filled with laughter, adventure, and a    |  story was one for the ages, filled with laughter, \ndeep, unbreakable bond.  Sophia, with her long,     |  adventure, and a deep, unbreakable bond.  Sophia, \ncurly brown hair and bright smile, was the village  |  with her long, curly brown hair and bright smile, \nbaker. She spent her days crafting the most         |  was the village baker. She spent her days crafting\ndelicious pastries and bread, filling the village   |  the most delicious pastries and bread, filling the\nwith the sweet scent of freshly baked goods.        |  village with the sweet scent of freshly baked     \nAlexander, with his rugged good looks and kind      |  goods. Alexander, with his rugged good looks and  \nheart, was a farmer. He spent his days tending to   |  kind heart, was a farmer. He spent his days       \nthe land, ensuring the village had a bounty of      |  tending to the land, ensuring the village had a   \nfresh produce.  Their love blossomed in the         |  bounty of fresh produce.  Their love blossomed    \nvillage square, where Sophia would often set up     |  under the warm sun of summer, amidst the vibrant  \nher bakery stand. Alexander would stop by every     |  colors of spring, and through the cozy warmth of  \nmorning, buying a loaf of bread and a smile from    |  autumn. They would often meet at the village      \nSophia. As the days turned into weeks, and the      |  square, where Sophia would sell her baked goods   \nweeks into months, their friendship deepened into   |  and Alexander would display his fresh produce.    \nsomething more.  One fateful day, Alexander         |  Their eyes would meet, and with a smile, they     \ndecided to take Sophia on a surprise picnic to the  |  would exchange stories of their day.  As the      \ntop of the hill overlooking the village. He packed  |  seasons passed, their love grew stronger. They    \na basket with her favorite foods and a bottle of    |  would take long walks through the fields, hand in \nwine. As they sat together, watching the sunset     |  hand, watching the sunset paint the sky with hues \npaint the sky with hues of pink and orange,         |  of pink and orange. They would talk about their   \nAlexander realized he couldn't live without         |  dreams, their aspirations, and their future       \nSophia. He took her hand, and with his heart full   |  together.  But fate, it seemed, had other plans.  \nof emotion, he proposed.  Sophia, overwhelmed with  |  One fateful day, a severe storm swept through the \njoy, said yes. They spent the rest of the evening   |  village, bringing with it heavy rains and strong  \nunder the stars, their love shining brighter than   |  winds. The fields were flooded, and the crops were\nany star in the sky.  But fate, it seemed, had      |  destroyed. Alexander, determined to save their    \nother plans. A few months later, a severe storm     |  livelihood, ventured out into the storm to salvage\nswept through the village, destroying homes and     |  what he could. He was caught in a flash flood and \ncrops. Alexander, in a desperate attempt to save    |  swept away by the rushing waters.  Sophia, frantic\nthe village, ventured into the heart of the storm.  |  with worry, searched for Alexander, but he was    \nHe was caught in a flash flood and swept away,      |  nowhere to be found. The village was in chaos, and\nnever to be seen again.  Sophia, heartbroken,       |  the storm had left its mark. Days turned into     \nspent her days searching for Alexander, scouring    |  weeks, and weeks turned into months. Sophia's     \nthe riverbanks and fields, hoping against hope      |  heart was heavy with grief, but she refused to    \nthat he would return. But as the days turned into   |  give up. She continued to bake, her pastries and  \nweeks, and the weeks into months, the reality of    |  bread a testament to her strength and resilience. \nher loss began to sink in.  The village, too, was   |  As the seasons passed, Sophia's bakery became a   \naffected by the loss. The once-thriving fields lay  |  beacon of hope for the village. People would come \nbarren, and the bakery, once filled with the        |  from all around to taste her delicious creations  \nwarmth of Sophia's love, now stood empty. The       |  and to hear the story of her love and loss.       \nvillagers, though they tried to be strong,          |  Sophia's heart, though broken, remained full of   \ncouldn't help but feel the absence of the young     |  love and kindness. She would often look up at the \ncouple who had brought so much joy to their lives.  |  sky, remembering the days she and Alexander spent \nYears passed, and Sophia, though she never forgot   |  together, and the love they shared.  Years went   \nAlexander, learned to live with her loss. She       |  by, and the village began to heal. Sophia's bakery\ncontinued to bake, but the pastries and bread no    |  remained a symbol of hope and love. One day, a    \nlonger held the same magic. The village, too,       |  young traveler stumbled upon the village, seeking \nslowly began to heal. New families moved in, and    |  refuge from the storm. As he entered the bakery,  \nthe fields were replanted. But the memory of        |  he was struck by the warmth and kindness of       \nSophia and Alexander's love story remained, a       |  Sophia. He learned of her love story and the loss \ntestament to the power of love and the resilience   |  she had endured.  Moved by her tale, the traveler \nof the human spirit.  One day, as Sophia was        |  decided to stay in the village. He helped Sophia  \nbaking a loaf of bread, she noticed a small,        |  rebuild her bakery and her life. Together, they   \ndelicate flower blooming in the garden. It was a    |  worked tirelessly, their hands moving in harmony  \nrare species, one that only bloomed once a year,    |  as they crafted a new future. Sophia's heart,     \non the anniversary of Alexander's disappearance.    |  though still bearing the scars of her loss, began \nSophia smiled, knowing that Alexander's spirit was  |  to heal.  Years later, as the sun set over the    \nstill with her, watching over her, and guiding her  |  village, Sophia looked out at the fields, now lush\nthrough the darkest of times.  And so, Sophia       |  and green. She smiled, knowing that Alexander's   \ncontinued to bake, her pastries and bread now       |  spirit lived on, guiding her through the ups and  \ninfused with the love and the loss she had          |  downs of life. Her bakery remained a testament to \nexperienced. The villagers, too, learned to         |  their love, a reminder that even in the darkest of\ncherish the memories of the young couple who had    |  times, there is always hope.  And so, Sophia's    \nbrought so much joy to their lives. For in the      |  story became a legend, a tale of love and loss, of\nend, it was not the absence of love that defined    |  resilience and hope. It served as a reminder to   \nthem, but the presence of love that had once been,  |  the villagers and to all who heard it, that even  \nand the hope that one day, they would be reunited.  |  in the face of adversity, love can conquer all.   \n                                                    |  The story of Sophia and Alexander lived on, a     \n                                                    |  beacon of hope in the hearts of those who knew    \n                                                    |  them, and a reminder that love never truly dies.  \n                                                    |  It only transforms, much like the seasons that    \n                                                    |  change the world around us.<|eot_id|>             \n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:48:33.442868Z","iopub.execute_input":"2024-11-25T03:48:33.443639Z","iopub.status.idle":"2024-11-25T03:51:47.320844Z","shell.execute_reply.started":"2024-11-25T03:48:33.443605Z","shell.execute_reply":"2024-11-25T03:51:47.319903Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    4540600320                     |                     4708372480                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Create an  |  completes the request.  ### Instruction: Create an\nanalogy using the following word  ### Input:        |  analogy using the following word  ### Input:      \ncheetah  ### Response: The cheetah is to the        |  cheetah  ### Response: The cheetah is to the      \nsavannah as the cheetah is to the grasslands.       |  savannah as the cheetah is to the grasslands.     \n### Explanation: The analogy is comparing the       |  ### Explanation: The analogy is comparing the     \ncheetah's habitat. The savannah and the grasslands  |  cheetah's habitat. The savannah and the grasslands\nare both environments where cheetahs can be found.  |  are both environments where cheetahs can be found.\nThe analogy is highlighting the similarity in the   |  The analogy is highlighting the similarity in the \ncheetah's habitat in these two different            |  cheetah's habitat in these two different          \necosystems.   ### Note: This analogy is a           |  ecosystems.   ### Note: This analogy is a         \ncomparison between two things that are similar in   |  comparison between two things that are similar in \nsome way. In this case, the comparison is between   |  some way. In this case, the comparison is between \nthe cheetah's habitat in the savannah and the       |  the cheetah's habitat in the savannah and the     \ncheetah's habitat in the grasslands. The analogy    |  cheetah's habitat in the grasslands. The analogy  \nis not comparing the cheetah itself, but rather     |  is not comparing the cheetah itself, but rather   \nits environment in these two different ecosystems.  |  its environment in these two different ecosystems.\n### Alternative Analogy: The cheetah is to the      |  ### Alternative Analogy: The cheetah is to the    \ndesert as the cheetah is to the grasslands.   ###   |  desert as the cheetah is to the grasslands.   ### \nExplanation: This analogy is comparing the          |  Explanation: This analogy is comparing the        \ncheetah's habitat in the desert and the cheetah's   |  cheetah's habitat in the desert and the cheetah's \nhabitat in the grasslands. The desert and the       |  habitat in the grasslands. The desert and the     \ngrasslands are both environments where cheetahs     |  grasslands are both environments where cheetahs   \ncan be found. The analogy is highlighting the       |  can be found. The analogy is highlighting the     \nsimilarity in the cheetah's habitat in these two    |  similarity in the cheetah's habitat in these two  \ndifferent ecosystems.   ### Note: This analogy is   |  different ecosystems.   ### Note: This analogy is \na comparison between two things that are similar    |  a comparison between two things that are similar  \nin some way. In this case, the comparison is        |  in some way. In this case, the comparison is      \nbetween the cheetah's habitat in the desert and     |  between the cheetah's habitat in the desert and   \nthe cheetah's habitat in the grasslands. The        |  the cheetah's habitat in the grasslands. The      \nanalogy is not comparing the cheetah itself, but    |  analogy is not comparing the cheetah itself, but  \nrather its environment in these two different       |  rather its environment in these two different     \necosystems.   ### Alternative Analogy: The cheetah  |  ecosystems.   ### Alternative Analogy: The cheetah\nis to the forest as the cheetah is to the           |  is to the forest as the cheetah is to the         \ngrasslands.   ### Explanation: This analogy is      |  grasslands.   ### Explanation: This analogy is    \ncomparing the cheetah's habitat in the forest and   |  comparing the cheetah's habitat in the forest and \nthe cheetah's habitat in the grasslands. The        |  the cheetah's habitat in the grasslands. The      \nforest and the grasslands are both environments     |  forest and the grasslands are both environments   \nwhere cheetahs can be found. The analogy is         |  where cheetahs can be found. The analogy is       \nhighlighting the similarity in the cheetah's        |  highlighting the similarity in the cheetah's      \nhabitat in these two different ecosystems.   ###    |  habitat in these two different ecosystems.   ###  \nNote: This analogy is a comparison between two      |  Note: This analogy is a comparison between two    \nthings that are similar in some way. In this case,  |  things that are similar in some way. In this case,\nthe comparison is between the cheetah's habitat in  |  the comparison is between the cheetah's habitat in\nthe forest and the cheetah's habitat in the         |  the forest and the cheetah's habitat in the       \ngrasslands. The analogy is not comparing the        |  grasslands. The analogy is not comparing the      \ncheetah itself, but rather its environment in       |  cheetah itself, but rather its environment in     \nthese two different ecosystems.   ### Alternative   |  these two different ecosystems.   ### Alternative \nAnalogy: The cheetah is to the tundra as the        |  Analogy: The cheetah is to the tundra as the      \ncheetah is to the grasslands.   ### Explanation:    |  cheetah is to the grasslands.   ### Explanation:  \nThis analogy is comparing the cheetah's habitat in  |  This analogy is comparing the cheetah's habitat in\nthe tundra and the cheetah's habitat in the         |  the tundra and the cheetah's habitat in the       \ngrasslands. The tundra and the grasslands are both  |  grasslands. The tundra and the grasslands are both\nenvironments where cheetahs can be found. The       |  environments where cheetahs can be found. The     \nanalogy is highlighting the similarity in the       |  analogy is highlighting the similarity in the     \ncheetah's habitat in these two different            |  cheetah's habitat in these two different          \necosystems.   ### Note: This analogy is a           |  ecosystems.   ### Note: This analogy is a         \ncomparison between two things that are similar in   |  comparison between two things that are similar in \nsome way. In this case, the comparison is between   |  some way. In this case, the comparison is between \nthe cheetah's habitat in the tundra and the         |  the cheetah's habitat in the tundra and the       \ncheetah's habitat in the grasslands. The analogy    |  cheetah's habitat in the grasslands. The analogy  \nis not comparing the cheetah itself, but rather     |  is not comparing the cheetah itself, but rather   \nits environment in these two different ecosystems.  |  its environment in these two different ecosystems.\n### Alternative Analogy: The cheetah is to the      |  ### Alternative Analogy: The cheetah is to the    \nmountains as the cheetah is to the grasslands.      |  mountains as the cheetah is to the grasslands.    \n### Explanation: This analogy is comparing the      |  ### Explanation: This analogy is comparing the    \ncheetah's habitat in the mountains and the          |  cheetah's habitat in the mountains and the        \ncheetah's habitat in the grasslands. The mountains  |  cheetah's habitat in the grasslands. The mountains\nand the grasslands are both environments where      |  and the grasslands are both environments where    \ncheetahs can be found. The analogy is highlighting  |  cheetahs can be found. The analogy is highlighting\nthe similarity in the cheetah's habitat in these    |  the similarity in the cheetah's habitat in these  \ntwo different ecosystems.   ### Note: This analogy  |  two different ecosystems.   ### Note: This analogy\nis a comparison between two things that are         |  is a comparison between two things that are       \nsimilar in some way. In this case, the comparison   |  similar in some way. In this case, the comparison \nis between the cheetah's habitat in the mountains   |  is between the cheetah's habitat in the mountains \nand the cheetah's habitat in the grasslands. The    |  and the cheetah's habitat in the grasslands. The  \nanalogy is not comparing the cheetah itself, but    |  analogy is not comparing the cheetah itself, but  \nrather its environment in these two different       |  rather its environment in these two different     \necosystems.   ### Alternative Analogy: The cheetah  |  ecosystems.   ### Alternative Analogy: The cheetah\nis to the wetlands as the cheetah is to the         |  is to the wetlands as the cheetah is to the       \ngrasslands.   ### Explanation: This analogy is      |  grasslands.   ### Explanation: This analogy is    \ncomparing the cheetah's habitat in the wetlands     |  comparing the cheetah's habitat in the wetlands   \nand the cheetah's habitat in the grasslands. The    |  and the cheetah's habitat in the grasslands. The  \nwetlands and the grasslands are both environments   |  wetlands and the grasslands are both environments \nwhere cheetahs can be found. The analogy is         |  where cheetahs can be found. The analogy is       \nhighlighting the similarity in the cheetah's        |  highlighting the similarity in the cheetah's      \nhabitat in these two different ecosystems.   ###    |  habitat in these two different ecosystems.   ###  \nNote: This analogy is a comparison between two      |  Note: This analogy is a comparison between two    \nthings that are similar in some way. In this case,  |  things that are similar in some way. In this case,\nthe comparison is between the cheetah's habitat in  |  the comparison is between the cheetah's habitat in\nthe wetlands and the cheetah's habitat in the       |  the wetlands and the cheetah's habitat in the     \ngrasslands. The analogy is not comparing the        |  grasslands. The analogy is not comparing the      \ncheetah itself, but rather its environment in       |  cheetah itself, but rather its environment in     \nthese two different ecosystems.   ### Alternative   |  these two different ecosystems.   ### Alternative \nAnalogy: The cheetah is to the coral reef as the    |  Analogy: The cheetah is to the coral reef as the  \ncheetah is to the grasslands.   ### Explanation:    |  cheetah is to the grasslands.   ### Explanation:  \nThis analogy is comparing the cheetah's habitat in  |  This analogy is comparing the cheetah's habitat in\nthe coral reef and the cheetah's habitat in the     |  the coral reef and the cheetah's habitat in the   \ngrasslands. The coral reef and the grasslands are   |  grasslands. The coral reef and the grasslands are \nboth environments where cheetahs can be             |  both environments where cheetahs can be           \n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['instruction']\ninputs = dataset[loc]['input']\npre_text = pre_assistant(prompt, inputs)\npost_text = post_assistant(prompt, inputs)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-11-25T03:37:41.907316Z","iopub.execute_input":"2024-11-25T03:37:41.907582Z","iopub.status.idle":"2024-11-25T03:40:55.802956Z","shell.execute_reply.started":"2024-11-25T03:37:41.907556Z","shell.execute_reply":"2024-11-25T03:40:55.802128Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    4540600320                     |                     4708372480                    \n================================================== | ==================================================\nBelow is an instruction that describes a task,      |  Below is an instruction that describes a task,    \npaired with an input that provides further          |  paired with an input that provides further        \ncontext. Write a response that appropriately        |  context. Write a response that appropriately      \ncompletes the request.  ### Instruction: Summarize  |  completes the request.  ### Instruction: Summarize\nthe importance of sleep  ### Input:   ###           |  the importance of sleep  ### Input:   ###         \nResponse: Sleep is crucial for our overall health   |  Response: Sleep is crucial for our overall health \nand well-being. During sleep, our body repairs and  |  and well-being. During sleep, our body repairs and\nregenerates tissues, builds bone and muscle, and    |  regenerates tissues, builds bone and muscle, and  \nstrengthens our immune system. It also helps to     |  strengthens our immune system. It also helps to   \nclear waste from the brain, which can reduce the    |  clear waste from the brain, which can reduce the  \nrisk of neurodegenerative diseases like             |  risk of neurodegenerative diseases like           \nAlzheimer's. Furthermore, sleep plays a             |  Alzheimer's. Furthermore, sleep plays a           \nsignificant role in the regulation of emotions and  |  significant role in regulating emotions and       \nbehaviors, with adequate sleep helping to prevent   |  behaviors, which can impact our mental health.    \nmood disorders like depression. Additionally,       |  Adequate sleep has been linked to improved        \nsleep deprivation has been linked to an increased   |  cognitive function, better concentration, and     \nrisk of chronic diseases, such as diabetes,         |  enhanced creativity. It also helps to maintain a  \ncardiovascular disease, and obesity. Therefore,     |  healthy weight, as sleep deprivation can increase \ngetting sufficient sleep is essential for           |  the production of ghrelin, a hormone that         \nmaintaining physical and mental health.   ###       |  stimulates appetite. Overall, getting sufficient  \nNote: The response is written in a formal tone,     |  sleep is essential for maintaining physical and   \nsuitable for academic or professional purposes. If  |  mental health, and can have a significant impact  \nyou need a response in a different tone, please     |  on our quality of life.   ### Note: The response  \nlet me know.   ### Additional Information: If you   |  is a summary of the importance of sleep,          \nhave any specific requirements or need further      |  highlighting its role in physical and mental      \nclarification, feel free to ask. I'll be happy to   |  health, cognitive function, and overall well-     \nassist you.  Please let me know if there's          |  being. It is written in a clear and concise       \nanything else I can help you with.  I'm here to     |  manner, making it easy to understand. The tone is \nhelp.  If you have any other questions or need      |  informative and neutral, providing a balanced view\nfurther clarification, please don't hesitate to     |  of the topic.  The response is approximately      \nask.  I'll be happy to help.  If you have any       |  150-200 words, which is suitable for a summary.   \nother questions or need further clarification,      |  It is free of grammatical errors and is written in\nplease don't hesitate to ask.  I'll be happy to     |  a formal tone, making it suitable for academic or \nhelp.  If you have any other questions or need      |  professional purposes.  The response is also free \nfurther clarification, please don't hesitate to     |  of any bias or personal opinions, providing a     \nask.  I'll be happy to help.  If you have any       |  neutral and informative view of the topic.  The   \nother questions or need further clarification,      |  language used is simple and clear, making it      \nplease don't hesitate to ask.  I'll be happy to     |  accessible to a wide range of readers.  The       \nhelp.  If you have any other questions or need      |  response is also well-structured, with a clear    \nfurther clarification, please don't hesitate to     |  introduction, body, and conclusion.  The          \nask.  I'll be happy to help.  If you have any       |  conclusion summarizes the main points and         \nother questions or need further clarification,      |  reiterates the importance of sleep.  The response \nplease don't hesitate to ask.  I'll be happy to     |  is also well-supported by evidence, as it cites   \nhelp.  If you have any other questions or need      |  the benefits of sleep on physical and mental      \nfurther clarification, please don't hesitate to     |  health, cognitive function, and overall well-     \nask.  I'll be happy to help.  If you have any       |  being.  The response is also free of any jargon or\nother questions or need further clarification,      |  technical terms that may be unfamiliar to non-    \nplease don't hesitate to ask.  I'll be happy to     |  experts.  The response is also well-organized,    \nhelp.  If you have any other questions or need      |  with a clear and logical flow of ideas.  The      \nfurther clarification, please don't hesitate to     |  response is also free of any errors in            \nask.  I'll be happy to help.  If you have any       |  punctuation, capitalization, or spelling.  The    \nother questions or need further clarification,      |  response is also well-supported by evidence, as it\nplease don't hesitate to ask.  I'll be happy to     |  cites the benefits of sleep on physical and mental\nhelp.  If you have any other questions or need      |  health, cognitive function, and overall well-     \nfurther clarification, please don't hesitate to     |  being.  The response is also free of any bias or  \nask.  I'll be happy to help.  If you have any       |  personal opinions, providing a neutral and        \nother questions or need further clarification,      |  informative view of the topic.  The response is   \nplease don't hesitate to ask.  I'll be happy to     |  also well-structured, with a clear introduction,  \nhelp.  If you have any other questions or need      |  body, and conclusion.  The conclusion summarizes  \nfurther clarification, please don't hesitate to     |  the main points and reiterates the importance of  \nask.  I'll be happy to help.  If you have any       |  sleep.  The response is also well-supported by    \nother questions or need further clarification,      |  evidence, as it cites the benefits of sleep on    \nplease don't hesitate to ask.  I'll be happy to     |  physical and mental health, cognitive function,   \nhelp.  If you have any other questions or need      |  and overall well-being.  The response is also free\nfurther clarification, please don't hesitate to     |  of any jargon or technical terms that may be      \nask.  I'll be happy to help.  If you have any       |  unfamiliar to non-experts.  The response is also  \nother questions or need further clarification,      |  well-organized, with a clear and logical flow of  \nplease don't hesitate to ask.  I'll be happy to     |  ideas.  The response is also free of any errors in\nhelp.  If you have any other questions or need      |  punctuation, capitalization, or spelling.  The    \nfurther clarification, please don't hesitate to     |  response is also well-supported by evidence, as it\nask.  I'll be happy to help.  If you have any       |  cites the benefits of sleep on physical and mental\nother questions or need further clarification,      |  health, cognitive function, and overall well-     \nplease don't hesitate to ask.  I'll be happy to     |  being.  The response is also free of any bias or  \nhelp.  If you have any other questions or need      |  personal opinions, providing a neutral and        \nfurther clarification, please don't hesitate to     |  informative view of the topic.  The response is   \nask.  I'll be happy to help.  If you have any       |  also well-structured, with a clear introduction,  \nother questions or need further clarification,      |  body, and conclusion.  The conclusion summarizes  \nplease don't hesitate to ask.  I'll be happy to     |  the main points and reiterates the importance of  \nhelp.  If you have any other questions or need      |  sleep.  The response is also well-supported by    \nfurther clarification, please don't hesitate to     |  evidence, as it cites the benefits of sleep on    \nask.  I'll be happy to help.  If you have any       |  physical and mental health, cognitive function,   \nother questions or need further clarification,      |  and overall well-being.  The response is also free\nplease don't hesitate to ask.  I'll be happy to     |  of any jargon or technical terms that may be      \nhelp.  If you have any other questions or need      |  unfamiliar to non-experts.  The response is also  \nfurther clarification, please don't hesitate to     |  well-organized, with a clear and logical flow of  \nask.  I'll be happy to help.  If you have any       |  ideas.  The response is also free of any errors in\nother questions or need further clarification,      |  punctuation, capitalization, or spelling.  The    \nplease don't hesitate to ask.  I'll be happy to     |  response is also well-supported by evidence, as it\nhelp.  If you have any other questions or need      |  cites the benefits of sleep on physical and mental\nfurther clarification, please don't hesitate to     |  health, cognitive function, and overall well-     \nask.  I'll be happy to help.  If you have any       |  being.  The response is also free of any bias or  \nother questions or need further clarification,      |  personal opinions, providing a neutral and        \nplease don't hesitate to ask.  I'll be happy to     |  informative view of the topic.  The response is   \nhelp.  If you have any other questions or need      |  also well-structured, with a clear introduction,  \nfurther clarification, please don't hesitate to     |  body, and conclusion.  The conclusion summarizes  \nask.  I'll be happy to help.  If you have any       |  the main points and reiterates the importance of  \nother questions or need further clarification,      |  sleep.  The response is also well-supported by    \nplease don't hesitate to ask.  I'll be happy to     |  evidence, as it cites the benefits of sleep on    \nhelp.  If you have any other questions or need      |  physical and mental health, cognitive function,   \nfurther clarification, please don't hesitate to     |  and overall well-being.  The response is also free\nask.  I'll be happy to help.  If you have any       |  of any jargon or technical terms that may be      \nother questions or need further clarification,      |  unfamiliar to non-experts.  The response is also  \nplease don't hesitate to ask.  I'll be happy to     |  well-organized, with a clear and logical flow of  \nhelp.  If you                                       |  ideas.  The response is also free of any errors in\n                                                    |  punctuation, capitalization, or spelling.  The    \n                                                    |  response is also well-supported by evidence, as it\n                                                    |  cites the benefits of sleep on physical and mental\n                                                    |  health, cognitive function, and overall well-     \n                                                    |  being.  The response is also free of any bias or  \n                                                    |  personal opinions, providing a neutral and        \n                                                    |  informative view of the topic.  The response is   \n                                                    |  also well-structured, with a clear introduction,  \n","output_type":"stream"}],"execution_count":62}]}