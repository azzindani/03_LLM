{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-15T02:41:50.749602Z","iopub.execute_input":"2024-12-15T02:41:50.749967Z","iopub.status.idle":"2024-12-15T02:42:47.362256Z","shell.execute_reply.started":"2024-12-15T02:41:50.749936Z","shell.execute_reply":"2024-12-15T02:42:47.361301Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntrl 0.12.2 requires transformers<4.47.0, but you have transformers 4.47.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-12-15T02:42:47.364353Z","iopub.execute_input":"2024-12-15T02:42:47.364660Z","iopub.status.idle":"2024-12-15T02:42:58.023122Z","shell.execute_reply.started":"2024-12-15T02:42:47.364631Z","shell.execute_reply":"2024-12-15T02:42:58.022222Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-12-15T02:42:58.025031Z","iopub.execute_input":"2024-12-15T02:42:58.025425Z","iopub.status.idle":"2024-12-15T02:42:58.031692Z","shell.execute_reply.started":"2024-12-15T02:42:58.025385Z","shell.execute_reply":"2024-12-15T02:42:58.030908Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"model_name = 'unsloth/Llama-3.2-1B-Instruct'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-15T02:42:58.033288Z","iopub.execute_input":"2024-12-15T02:42:58.033531Z","iopub.status.idle":"2024-12-15T02:42:58.128600Z","shell.execute_reply.started":"2024-12-15T02:42:58.033507Z","shell.execute_reply":"2024-12-15T02:42:58.127776Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-12-15T02:42:58.129633Z","iopub.execute_input":"2024-12-15T02:42:58.129897Z","iopub.status.idle":"2024-12-15T02:42:58.138154Z","shell.execute_reply.started":"2024-12-15T02:42:58.129872Z","shell.execute_reply":"2024-12-15T02:42:58.137365Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:42:58.139339Z","iopub.execute_input":"2024-12-15T02:42:58.139935Z","iopub.status.idle":"2024-12-15T02:44:00.567601Z","shell.execute_reply.started":"2024-12-15T02:42:58.139896Z","shell.execute_reply":"2024-12-15T02:44:00.566778Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/927 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b33799d2d17442e2bbcdc77025c9bae6"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"641bb72caa314b4d87c5ded7206d5e60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"759ba68c69c14634bf9c689cee09f049"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:00.568818Z","iopub.execute_input":"2024-12-15T02:44:00.569103Z","iopub.status.idle":"2024-12-15T02:44:00.575926Z","shell.execute_reply.started":"2024-12-15T02:44:00.569077Z","shell.execute_reply":"2024-12-15T02:44:00.574977Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 749275136\nTrainable parameters : 262735872\nTrainable percentage: 35.07%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:00.577077Z","iopub.execute_input":"2024-12-15T02:44:00.577334Z","iopub.status.idle":"2024-12-15T02:44:02.356851Z","shell.execute_reply.started":"2024-12-15T02:44:00.577309Z","shell.execute_reply":"2024-12-15T02:44:02.355855Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6e29cd8bbac4ef6a2ed2c3b54f9f671"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e956b44775004e128d4f448c5111294d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b340fd9fe944de5b5d5909f4bd574bd"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"dataset_name = 'microsoft/orca-math-word-problems-200k'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:02.358051Z","iopub.execute_input":"2024-12-15T02:44:02.358341Z","iopub.status.idle":"2024-12-15T02:44:02.362561Z","shell.execute_reply.started":"2024-12-15T02:44:02.358315Z","shell.execute_reply":"2024-12-15T02:44:02.361613Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 384","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:02.366108Z","iopub.execute_input":"2024-12-15T02:44:02.366366Z","iopub.status.idle":"2024-12-15T02:44:02.578691Z","shell.execute_reply.started":"2024-12-15T02:44:02.366331Z","shell.execute_reply":"2024-12-15T02:44:02.577828Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:02.579774Z","iopub.execute_input":"2024-12-15T02:44:02.580086Z","iopub.status.idle":"2024-12-15T02:44:04.851153Z","shell.execute_reply.started":"2024-12-15T02:44:02.580060Z","shell.execute_reply":"2024-12-15T02:44:04.850129Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'answer'],\n    num_rows: 200035\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:04.852168Z","iopub.execute_input":"2024-12-15T02:44:04.852423Z","iopub.status.idle":"2024-12-15T02:44:04.858970Z","shell.execute_reply.started":"2024-12-15T02:44:04.852394Z","shell.execute_reply":"2024-12-15T02:44:04.858109Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:04.860297Z","iopub.execute_input":"2024-12-15T02:44:04.860548Z","iopub.status.idle":"2024-12-15T02:44:05.246984Z","shell.execute_reply.started":"2024-12-15T02:44:04.860524Z","shell.execute_reply":"2024-12-15T02:44:05.246130Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:05.248238Z","iopub.execute_input":"2024-12-15T02:44:05.248657Z","iopub.status.idle":"2024-12-15T02:44:05.254521Z","shell.execute_reply.started":"2024-12-15T02:44:05.248615Z","shell.execute_reply":"2024-12-15T02:44:05.253701Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:05.255749Z","iopub.execute_input":"2024-12-15T02:44:05.256372Z","iopub.status.idle":"2024-12-15T02:44:05.267002Z","shell.execute_reply.started":"2024-12-15T02:44:05.256332Z","shell.execute_reply":"2024-12-15T02:44:05.266206Z"}},"outputs":[{"name":"stdout","text":"['question', 'answer']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:05.268072Z","iopub.execute_input":"2024-12-15T02:44:05.268321Z","iopub.status.idle":"2024-12-15T02:44:05.278688Z","shell.execute_reply.started":"2024-12-15T02:44:05.268297Z","shell.execute_reply":"2024-12-15T02:44:05.277851Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  input = examples['question']\n  output = examples['answer']\n  \n  text = prompt_format.format(input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"id":"0wXJNFBWWNYP","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:05.279643Z","iopub.execute_input":"2024-12-15T02:44:05.279920Z","iopub.status.idle":"2024-12-15T02:44:05.288761Z","shell.execute_reply.started":"2024-12-15T02:44:05.279895Z","shell.execute_reply":"2024-12-15T02:44:05.287962Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:05.289762Z","iopub.execute_input":"2024-12-15T02:44:05.290034Z","iopub.status.idle":"2024-12-15T02:44:05.739744Z","shell.execute_reply.started":"2024-12-15T02:44:05.290010Z","shell.execute_reply":"2024-12-15T02:44:05.738912Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6b8efabb8774cc9b9288413532f4dfb"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"Kidf8H5zefDC","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:05.740834Z","iopub.execute_input":"2024-12-15T02:44:05.741107Z","iopub.status.idle":"2024-12-15T02:44:05.745957Z","shell.execute_reply.started":"2024-12-15T02:44:05.741082Z","shell.execute_reply":"2024-12-15T02:44:05.745117Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|eot_id|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:05.746867Z","iopub.execute_input":"2024-12-15T02:44:05.747107Z","iopub.status.idle":"2024-12-15T02:44:05.770563Z","shell.execute_reply.started":"2024-12-15T02:44:05.747084Z","shell.execute_reply":"2024-12-15T02:44:05.769791Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:05.771609Z","iopub.execute_input":"2024-12-15T02:44:05.771906Z","iopub.status.idle":"2024-12-15T02:44:14.836608Z","shell.execute_reply.started":"2024-12-15T02:44:05.771881Z","shell.execute_reply":"2024-12-15T02:44:14.835762Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fda971e2af445458be077a99ffe2468"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.837735Z","iopub.execute_input":"2024-12-15T02:44:14.838023Z","iopub.status.idle":"2024-12-15T02:44:14.843183Z","shell.execute_reply.started":"2024-12-15T02:44:14.837996Z","shell.execute_reply":"2024-12-15T02:44:14.842359Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|eot_id|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.844288Z","iopub.execute_input":"2024-12-15T02:44:14.844617Z","iopub.status.idle":"2024-12-15T02:44:14.894446Z","shell.execute_reply.started":"2024-12-15T02:44:14.844578Z","shell.execute_reply":"2024-12-15T02:44:14.893630Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.895430Z","iopub.execute_input":"2024-12-15T02:44:14.895690Z","iopub.status.idle":"2024-12-15T02:44:14.901772Z","shell.execute_reply.started":"2024-12-15T02:44:14.895658Z","shell.execute_reply":"2024-12-15T02:44:14.901000Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.902593Z","iopub.execute_input":"2024-12-15T02:44:14.902845Z","iopub.status.idle":"2024-12-15T02:44:14.927533Z","shell.execute_reply.started":"2024-12-15T02:44:14.902812Z","shell.execute_reply":"2024-12-15T02:44:14.926822Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  ### Question:\\nThere is a two-digit natural nu...   \n1  ### Question:\\nIn a big box, there are marbles...   \n2  ### Question:\\nAdam goes to a small school, wh...   \n3  ### Question:\\nLisa is looking to attempt a Wo...   \n4  ### Question:\\nThere is a rectangular-shaped p...   \n\n                                           input_ids  \\\n0  [128000, 14711, 16225, 512, 3947, 374, 264, 14...   \n1  [128000, 14711, 16225, 512, 644, 264, 2466, 38...   \n2  [128004, 128004, 128004, 128004, 128004, 12800...   \n3  [128004, 128004, 128004, 128004, 128004, 12800...   \n4  [128000, 14711, 16225, 512, 3947, 374, 264, 52...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nThere is a two-digit natural nu...</td>\n      <td>[128000, 14711, 16225, 512, 3947, 374, 264, 14...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nIn a big box, there are marbles...</td>\n      <td>[128000, 14711, 16225, 512, 644, 264, 2466, 38...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nAdam goes to a small school, wh...</td>\n      <td>[128004, 128004, 128004, 128004, 128004, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nLisa is looking to attempt a Wo...</td>\n      <td>[128004, 128004, 128004, 128004, 128004, 12800...</td>\n      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere is a rectangular-shaped p...</td>\n      <td>[128000, 14711, 16225, 512, 3947, 374, 264, 52...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.928436Z","iopub.execute_input":"2024-12-15T02:44:14.928694Z","iopub.status.idle":"2024-12-15T02:44:14.933419Z","shell.execute_reply.started":"2024-12-15T02:44:14.928669Z","shell.execute_reply":"2024-12-15T02:44:14.932713Z"}},"outputs":[{"name":"stdout","text":"### Question:\nThere is a two-digit natural number whose tens place is 3. Let A and B be the quotient of this number by 10 and the remainder of division by 10, respectively. If B multiplied by 10 plus A is 9 less than A multiplied by 10 plus B, what is the first number?\n### Answer:\nLet's denote the two-digit number as \\( XY \\), where \\( X \\) is the digit in the tens place and \\( Y \\) is the digit in the ones place. Since the tens place is 3, we have \\( X = 3 \\).\n\nAccording to the problem, \\( A \\) is the quotient of the number by 10, and \\( B \\) is the remainder of the division by 10. Therefore, \\( A = X = 3 \\) and \\( B = Y \\).\n\nThe problem states that \\( B \\times 10 + A \\) is 9 less than \\( A \\times 10 + B \\). This can be written as an equation:\n\n\\[ B \\times 10 + A = A \\times 10 + B - 9 \\]\n\nSubstituting \\( A \\) and \\( B \\) with \\( 3 \\) and \\( Y \\), respectively, we get:\n\n\\[ Y \\times 10 + 3 = 3 \\times 10 + Y - 9 \\]\n\nSimplifying the equation:\n\n\\[ 10Y + 3 = 30 + Y - 9 \\]\n\n\\[ 10Y + 3 = Y + 21 \\]\n\nSubtract \\( Y \\) from both sides:\n\n\\[ 9Y + 3 = 21 \\]\n\nSubtract 3 from both sides:\n\n\\[ 9Y = 18 \\]\n\nDivide both sides by 9:\n\n\\[ Y = 2 \\]\n\nSo the ones place digit is 2. Since we already know the tens place digit is 3, the two-digit number is \\( 32 \\).<|eot_id|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.934439Z","iopub.execute_input":"2024-12-15T02:44:14.934679Z","iopub.status.idle":"2024-12-15T02:44:14.946627Z","shell.execute_reply.started":"2024-12-15T02:44:14.934653Z","shell.execute_reply":"2024-12-15T02:44:14.945918Z"}},"outputs":[{"name":"stdout","text":"[128000, 14711, 16225, 512, 3947, 374, 264, 1403, 49442, 5933, 1396, 6832, 22781, 2035, 374, 220, 18, 13, 6914, 362, 323, 426, 387, 279, 75862, 315, 420, 1396, 555, 220, 605, 323, 279, 27410, 315, 13096, 555, 220, 605, 11, 15947, 13, 1442, 426, 56016, 555, 220, 605, 5636, 362, 374, 220, 24, 2753, 1109, 362, 56016, 555, 220, 605, 5636, 426, 11, 1148, 374, 279, 1176, 1396, 5380, 14711, 22559, 512, 10267, 596, 79164, 279, 1403, 49442, 1396, 439, 18240, 58419, 1144, 705, 1405, 18240, 1630, 1144, 8, 374, 279, 16099, 304, 279, 22781, 2035, 323, 18240, 816, 1144, 8, 374, 279, 16099, 304, 279, 6305, 2035, 13, 8876, 279, 22781, 2035, 374, 220, 18, 11, 584, 617, 18240, 1630, 284, 220, 18, 1144, 3677, 11439, 311, 279, 3575, 11, 18240, 362, 1144, 8, 374, 279, 75862, 315, 279, 1396, 555, 220, 605, 11, 323, 18240, 426, 1144, 8, 374, 279, 27410, 315, 279, 13096, 555, 220, 605, 13, 15636, 11, 18240, 362, 284, 1630, 284, 220, 18, 1144, 8, 323, 18240, 426, 284, 816, 1144, 3677, 791, 3575, 5415, 430, 18240, 426, 1144, 15487, 220, 605, 489, 362, 1144, 8, 374, 220, 24, 2753, 1109, 18240, 362, 1144, 15487, 220, 605, 489, 426, 1144, 570, 1115, 649, 387, 5439, 439, 459, 24524, 1473, 79145, 426, 1144, 15487, 220, 605, 489, 362, 284, 362, 1144, 15487, 220, 605, 489, 426, 482, 220, 24, 1144, 2595, 3214, 3781, 10831, 18240, 362, 1144, 8, 323, 18240, 426, 1144, 8, 449, 18240, 220, 18, 1144, 8, 323, 18240, 816, 1144, 705, 15947, 11, 584, 636, 1473, 79145, 816, 1144, 15487, 220, 605, 489, 220, 18, 284, 220, 18, 1144, 15487, 220, 605, 489, 816, 482, 220, 24, 1144, 2595, 50, 6517, 7922, 279, 24524, 1473, 79145, 220, 605, 56, 489, 220, 18, 284, 220, 966, 489, 816, 482, 220, 24, 1144, 2595, 79145, 220, 605, 56, 489, 220, 18, 284, 816, 489, 220, 1691, 1144, 2595, 3214, 2193, 18240, 816, 1144, 8, 505, 2225, 11314, 1473, 79145, 220, 24, 56, 489, 220, 18, 284, 220, 1691, 1144, 2595, 3214, 2193, 220, 18, 505, 2225, 11314, 1473, 79145, 220, 24, 56, 284, 220, 972, 1144, 2595, 12792, 579, 2225, 11314, 555, 220, 24, 1473, 79145, 816, 284, 220, 17, 1144, 2595, 4516, 279, 6305, 2035, 16099, 374]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.951312Z","iopub.execute_input":"2024-12-15T02:44:14.951558Z","iopub.status.idle":"2024-12-15T02:44:14.957210Z","shell.execute_reply.started":"2024-12-15T02:44:14.951535Z","shell.execute_reply":"2024-12-15T02:44:14.956353Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.958392Z","iopub.execute_input":"2024-12-15T02:44:14.959021Z","iopub.status.idle":"2024-12-15T02:44:14.967480Z","shell.execute_reply.started":"2024-12-15T02:44:14.958982Z","shell.execute_reply":"2024-12-15T02:44:14.966663Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.968574Z","iopub.execute_input":"2024-12-15T02:44:14.969256Z","iopub.status.idle":"2024-12-15T02:44:14.978584Z","shell.execute_reply.started":"2024-12-15T02:44:14.969218Z","shell.execute_reply":"2024-12-15T02:44:14.978007Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.979662Z","iopub.execute_input":"2024-12-15T02:44:14.980015Z","iopub.status.idle":"2024-12-15T02:44:14.989023Z","shell.execute_reply.started":"2024-12-15T02:44:14.979979Z","shell.execute_reply":"2024-12-15T02:44:14.988244Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n\n#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.989776Z","iopub.execute_input":"2024-12-15T02:44:14.990060Z","iopub.status.idle":"2024-12-15T02:44:14.998295Z","shell.execute_reply.started":"2024-12-15T02:44:14.990035Z","shell.execute_reply":"2024-12-15T02:44:14.997479Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:14.999206Z","iopub.execute_input":"2024-12-15T02:44:14.999495Z","iopub.status.idle":"2024-12-15T02:44:15.616499Z","shell.execute_reply.started":"2024-12-15T02:44:14.999468Z","shell.execute_reply":"2024-12-15T02:44:15.615569Z"}},"outputs":[{"name":"stdout","text":"trainable params: 45,088,768 || all params: 1,280,903,168 || trainable%: 3.5201\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:15.618030Z","iopub.execute_input":"2024-12-15T02:44:15.618405Z","iopub.status.idle":"2024-12-15T02:44:15.628075Z","shell.execute_reply.started":"2024-12-15T02:44:15.618352Z","shell.execute_reply":"2024-12-15T02:44:15.627461Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 794363904\nTrainable parameters : 45088768\nTrainable percentage: 5.68%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:15.628770Z","iopub.execute_input":"2024-12-15T02:44:15.629049Z","iopub.status.idle":"2024-12-15T02:44:15.642031Z","shell.execute_reply.started":"2024-12-15T02:44:15.629020Z","shell.execute_reply":"2024-12-15T02:44:15.641234Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 2\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:15.643177Z","iopub.execute_input":"2024-12-15T02:44:15.643454Z","iopub.status.idle":"2024-12-15T02:44:15.689069Z","shell.execute_reply.started":"2024-12-15T02:44:15.643429Z","shell.execute_reply":"2024-12-15T02:44:15.688244Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec15_02-44-15_38248fae1b86,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:15.690233Z","iopub.execute_input":"2024-12-15T02:44:15.690529Z","iopub.status.idle":"2024-12-15T02:44:16.884633Z","shell.execute_reply.started":"2024-12-15T02:44:15.690499Z","shell.execute_reply":"2024-12-15T02:44:16.883780Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:403: UserWarning: You passed a processing_class with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `processing_class.padding_side = 'right'` to your code.\n  warnings.warn(\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7888bd6f6ef0>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T02:44:16.885852Z","iopub.execute_input":"2024-12-15T02:44:16.886209Z","iopub.status.idle":"2024-12-15T03:01:38.212905Z","shell.execute_reply.started":"2024-12-15T02:44:16.886170Z","shell.execute_reply":"2024-12-15T03:01:38.212017Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 2\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 45,088,768\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterlupakan100\u001b[0m (\u001b[33mterlupakan100-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.7"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241215_024418-mwukl6pe</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/mwukl6pe' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/mwukl6pe' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/mwukl6pe</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 17:14, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.071700</td>\n      <td>1.040411</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.002300</td>\n      <td>0.918602</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.897500</td>\n      <td>0.863425</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.877100</td>\n      <td>0.834719</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.835500</td>\n      <td>0.819666</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.796200</td>\n      <td>0.811225</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.797000</td>\n      <td>0.806124</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.818700</td>\n      <td>0.803805</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.796300</td>\n      <td>0.802748</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.809200</td>\n      <td>0.802619</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=0.8701309537887574, metrics={'train_runtime': 1040.9382, 'train_samples_per_second': 1.537, 'train_steps_per_second': 0.192, 'total_flos': 3919836295987200.0, 'train_loss': 0.8701309537887574, 'epoch': 0.17777777777777778})"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:01:38.214254Z","iopub.execute_input":"2024-12-15T03:01:38.214624Z","iopub.status.idle":"2024-12-15T03:02:14.714406Z","shell.execute_reply.started":"2024-12-15T03:01:38.214585Z","shell.execute_reply":"2024-12-15T03:02:14.713528Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:35]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.8026185631752014, 'eval_runtime': 36.4868, 'eval_samples_per_second': 5.481, 'eval_steps_per_second': 1.37, 'epoch': 0.17777777777777778}\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:14.715605Z","iopub.execute_input":"2024-12-15T03:02:14.715890Z","iopub.status.idle":"2024-12-15T03:02:16.308059Z","shell.execute_reply.started":"2024-12-15T03:02:14.715864Z","shell.execute_reply":"2024-12-15T03:02:16.307107Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/22e9fdbe3473c7affd8596fdc248e67e3a004da1/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/22e9fdbe3473c7affd8596fdc248e67e3a004da1/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"meta-llama/Llama-3.2-1B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:16.309309Z","iopub.execute_input":"2024-12-15T03:02:16.309688Z","iopub.status.idle":"2024-12-15T03:02:16.536064Z","shell.execute_reply.started":"2024-12-15T03:02:16.309647Z","shell.execute_reply":"2024-12-15T03:02:16.535069Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:16.537244Z","iopub.execute_input":"2024-12-15T03:02:16.537518Z","iopub.status.idle":"2024-12-15T03:02:16.548232Z","shell.execute_reply.started":"2024-12-15T03:02:16.537492Z","shell.execute_reply":"2024-12-15T03:02:16.547565Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:16.549225Z","iopub.execute_input":"2024-12-15T03:02:16.549456Z","iopub.status.idle":"2024-12-15T03:02:17.304809Z","shell.execute_reply.started":"2024-12-15T03:02:16.549433Z","shell.execute_reply":"2024-12-15T03:02:17.304004Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:17.306160Z","iopub.execute_input":"2024-12-15T03:02:17.306515Z","iopub.status.idle":"2024-12-15T03:02:20.790814Z","shell.execute_reply.started":"2024-12-15T03:02:17.306476Z","shell.execute_reply":"2024-12-15T03:02:20.789883Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/22e9fdbe3473c7affd8596fdc248e67e3a004da1/config.json\nModel config LlamaConfig {\n  \"_name_or_path\": \"unsloth/Llama-3.2-1B-Instruct\",\n  \"architectures\": [\n    \"LlamaForCausalLM\"\n  ],\n  \"attention_bias\": false,\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"head_dim\": 64,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8192,\n  \"max_position_embeddings\": 131072,\n  \"mlp_bias\": false,\n  \"model_type\": \"llama\",\n  \"num_attention_heads\": 32,\n  \"num_hidden_layers\": 16,\n  \"num_key_value_heads\": 8,\n  \"pretraining_tp\": 1,\n  \"rms_norm_eps\": 1e-05,\n  \"rope_scaling\": {\n    \"factor\": 32.0,\n    \"high_freq_factor\": 4.0,\n    \"low_freq_factor\": 1.0,\n    \"original_max_position_embeddings\": 8192,\n    \"rope_type\": \"llama3\"\n  },\n  \"rope_theta\": 500000.0,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 128256\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/22e9fdbe3473c7affd8596fdc248e67e3a004da1/model.safetensors\nInstantiating LlamaForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ]\n}\n\nAll model checkpoint weights were used when initializing LlamaForCausalLM.\n\nAll the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/Llama-3.2-1B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--unsloth--Llama-3.2-1B-Instruct/snapshots/22e9fdbe3473c7affd8596fdc248e67e3a004da1/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 128000,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    128001,\n    128008,\n    128009\n  ],\n  \"temperature\": 0.6,\n  \"top_p\": 0.9\n}\n\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"LlamaForCausalLM(\n  (model): LlamaModel(\n    (embed_tokens): Embedding(128256, 2048)\n    (layers): ModuleList(\n      (0-15): 16 x LlamaDecoderLayer(\n        (self_attn): LlamaSdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): LlamaMLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n      )\n    )\n    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:20.792115Z","iopub.execute_input":"2024-12-15T03:02:20.792483Z","iopub.status.idle":"2024-12-15T03:02:20.802213Z","shell.execute_reply.started":"2024-12-15T03:02:20.792443Z","shell.execute_reply":"2024-12-15T03:02:20.801381Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 749275136\nTrainable parameters : 262735872\nTrainable percentage: 35.07%\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:20.803305Z","iopub.execute_input":"2024-12-15T03:02:20.803573Z","iopub.status.idle":"2024-12-15T03:02:21.665773Z","shell.execute_reply.started":"2024-12-15T03:02:20.803548Z","shell.execute_reply":"2024-12-15T03:02:21.664779Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): LlamaForCausalLM(\n      (model): LlamaModel(\n        (embed_tokens): Embedding(128256, 2048)\n        (layers): ModuleList(\n          (0-15): 16 x LlamaDecoderLayer(\n            (self_attn): LlamaSdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=512, bias=False)\n                  (default): Linear(in_features=64, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=512, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=512, bias=False)\n                  (default): Linear(in_features=64, out_features=512, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): LlamaMLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n                  (default): Linear(in_features=64, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=8192, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8192, bias=False)\n                  (default): Linear(in_features=64, out_features=8192, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8192, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=8192, out_features=64, bias=False)\n                  (default): Linear(in_features=8192, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n          )\n        )\n        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:21.666936Z","iopub.execute_input":"2024-12-15T03:02:21.667249Z","iopub.status.idle":"2024-12-15T03:02:21.686776Z","shell.execute_reply.started":"2024-12-15T03:02:21.667223Z","shell.execute_reply":"2024-12-15T03:02:21.685828Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 839452672\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## 14 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:21.688065Z","iopub.execute_input":"2024-12-15T03:02:21.688420Z","iopub.status.idle":"2024-12-15T03:02:21.694711Z","shell.execute_reply.started":"2024-12-15T03:02:21.688372Z","shell.execute_reply":"2024-12-15T03:02:21.694005Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def post_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:21.695773Z","iopub.execute_input":"2024-12-15T03:02:21.696092Z","iopub.status.idle":"2024-12-15T03:02:21.705648Z","shell.execute_reply.started":"2024-12-15T03:02:21.696033Z","shell.execute_reply":"2024-12-15T03:02:21.704850Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:02:21.706675Z","iopub.execute_input":"2024-12-15T03:02:21.706954Z","iopub.status.idle":"2024-12-15T03:02:21.715546Z","shell.execute_reply.started":"2024-12-15T03:02:21.706928Z","shell.execute_reply":"2024-12-15T03:02:21.714661Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:08:52.954533Z","iopub.execute_input":"2024-12-15T03:08:52.955446Z","iopub.status.idle":"2024-12-15T03:10:20.423496Z","shell.execute_reply.started":"2024-12-15T03:08:52.955410Z","shell.execute_reply":"2024-12-15T03:10:20.422525Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    749275136                      |                     839452672                     \n================================================== | ==================================================\n### Question: There is the expression 691-6A7=4.    |  ### Question: There is the expression 691-6A7=4.  \nFind the number that goes into A ### Answer: To     |  Find the number that goes into A ### Answer: To   \nfind the number that goes into A, we need to find   |  find the number that goes into A, we need to find \nthe value of A that makes the equation true.  ###   |  the value of A that makes the equation true.  ### \nStep 1:  Understand the equation The equation is    |  Step 1:  Understand the equation The equation is  \n691 - 6A7 = 4. We need to find the value of A that  |  691 - 6A7 = 4. We need to find the value of A that\nmakes this equation true.  ### Step 2:  Isolate     |  makes this equation true.  ### Step 2:  Isolate   \nthe variable A To isolate A, we need to get rid of  |  the variable A To isolate A, we need to get rid of\nthe negative sign in front of 6A7. We can do this   |  the negative sign in front of 6A7. We can do this \nby adding 6A7 to 691 and then subtracting 4 from    |  by adding 6A7 to 691 and then subtracting 4 from  \nboth sides of the equation.  ### Step 3:  Add 6A7   |  both sides of the equation.  ### Step 3:  Add 6A7 \nto 691 Add 6A7 to 691: 691 + 6A7 = 691 + 6(7) =     |  to 691 Add 6A7 to 691: 691 + 6A7 = 691 + 6*7 = 691\n691 + 42 = 733.  ### Step 4:  Subtract 4 from both  |  + 42 = 733.  ### Step 4:  Subtract 4 from both    \nsides Subtract 4 from both sides: 733 - 4 = 729.    |  sides Subtract 4 from both sides: 733 - 4 = 729.  \n### Step 5:  Subtract 6A7 from 729 Subtract 6A7     |  ### Step 5:  Subtract 6A7 from 729 Subtract 6A7   \nfrom 729: 729 - 6A7 = 729 - 6(7) = 729 - 42 = 687.  |  from 729: 729 - 6A7 = 729 - 6*7 = 729 - 42 = 687. \n### Step 6:  Subtract 6A7 from 687 Subtract 6A7     |  ### Step 6:  Subtract 4 from 687 Subtract 4 from  \nfrom 687: 687 - 6A7 = 687 - 6(7) = 687 - 42 = 645.  |  687: 687 - 4 = 683.  ### Step 7:  Subtract 6A7    \n### Step 7:  Subtract 6A7 from 645 Subtract 6A7     |  from 683 Subtract 6A7 from 683: 683 - 6A7 = 683 - \nfrom 645: 645 - 6A7 = 645 - 6(7) = 645 - 42 = 603.  |  6*7 = 683 - 42 = 641.  ### Step 8:  Subtract 4    \n### Step 8:  Subtract 6A7 from 603 Subtract 6A7     |  from 641 Subtract 4 from 641: 641 - 4 = 637.  ### \nfrom 603: 603 - 6A7 = 603 - 6(7) = 603 - 42 = 561.  |  Step 9:  Subtract 6A7 from 637 Subtract 6A7 from  \n### Step 9:  Subtract 6A7 from 561 Subtract 6A7     |  637: 637 - 6A7 = 637 - 6*7 = 637 - 42 = 595.  ### \nfrom 561: 561 - 6A7 = 561 - 6(7) = 561 - 42 = 519.  |  Step 10:  Subtract 4 from 595 Subtract 4 from 595:\n### Step 10:  Subtract 6A7 from 519 Subtract 6A7    |  595 - 4 = 591.  ### Step 11:  Subtract 6A7 from   \nfrom 519: 519 - 6A7 = 519 - 6(7) = 519 - 42 = 477.  |  591 Subtract 6A7 from 591: 591 - 6A7 = 591 - 6*7 =\n### Step 11:  Subtract 6A7 from 477 Subtract 6A7    |  591 - 42 = 549.  ### Step 12:  Subtract 4 from 549\nfrom 477: 477 - 6A7 = 477 - 6(7) = 477 - 42 = 435.  |  Subtract 4 from 549: 549 - 4 = 545.  ### Step 13: \n### Step 12:  Subtract 6A7 from 435 Subtract 6A7    |  Subtract 6A7 from 545 Subtract 6A7 from 545: 545 -\nfrom 435: 435 - 6A7 = 435 - 6(7) = 435 - 42 = 393.  |  6A7 = 545 - 6*7 = 545 - 42 = 503.  ### Step 14:   \n### Step 13:  Subtract 6A7 from 393 Subtract 6A7    |  Subtract 4 from 503 Subtract 4 from 503: 503 - 4 =\nfrom 393: 393 - 6A7 = 393 - 6(7) = 393 - 42 = 351.  |  499.  ### Step 15:  Subtract 6A7 from 499 Subtract\n### Step 14:  Subtract 6A7 from 351 Subtract 6A7    |  6A7 from 499: 499 - 6A7 = 499 - 6*7 = 499 - 42 =  \nfrom 351: 351 - 6A7 = 351 - 6(7) = 351 - 42 = 309.  |  457.  ### Step 16:  Subtract 4 from 457 Subtract 4\n### Step 15:  Subtract 6A7 from 309 Subtract 6A7    |  from 457: 457 - 4 = 453.  ### Step 17:  Subtract  \nfrom 309: 309 - 6A7 = 309 - 6(7) = 309 - 42 = 267.  |  6A7 from 453 Subtract 6A7 from 453: 453 - 6A7 =   \n### Step 16:  Subtract 6A7 from 267 Subtract 6A7    |  453 - 6*7 = 453 - 42 = 411.  ### Step 18:         \nfrom 267: 267 - 6A7 = 267 - 6(7) = 267 - 42 = 225.  |  Subtract 4 from 411 Subtract 4 from 411: 411 - 4 =\n### Step 17:  Subtract 6A7 from 225 Subtract 6A7    |  407.  ### Step 19:  Subtract 6A7 from 407 Subtract\nfrom 225: 225 - 6A7 = 225 - 6(7) = 225 - 42 = 183.  |  6A7 from 407: 407 - 6A7 = 407 - 6*7 = 407 - 42 =  \n### Step 18:  Subtract 6A7 from 183 Subtract 6A7    |  365.  ### Step 20:  Subtract 4 from 365 Subtract 4\nfrom 183: 183 - 6A7 = 183 - 6(7) = 183 - 42 = 141.  |  from 365: 365 - 4 = 361.  ### Step 21:  Subtract  \n### Step 19:  Subtract 6A7 from 141 Subtract 6A7    |  6A7 from 361 Subtract 6A7 from 361: 361 - 6A7 =   \nfrom 141: 141 - 6A7 = 141 - 6(7) = 141 - 42 = 99.   |  361 - 6*7 = 361 - 42 = 319.  ### Step 22:         \n### Step 20:  Subtract 6A7 from 99 Subtract 6A7     |  Subtract 4 from 319 Subtract 4 from 319: 319 - 4 =\nfrom 99: 99 - 6A7 = 99 - 6(7) = 99 - 42 = 57.  ###  |  315.  ### Step 23:  Subtract 6A7 from 315 Subtract\nStep 21:  Subtract                                  |  6A7 from 315: 315 - 6A7 = 315 - 6*7 = 315 - 42 =  \n                                                    |  273.  ### Step 24:  Subtract 4 from 273 Subtract 4\n                                                    |  from 273: 273 - 4 = 269.  ### Step 25:  Subtract  \n                                                    |  6A7 from 269 Subtract 6A7 from 269:               \n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:15:05.544754Z","iopub.execute_input":"2024-12-15T03:15:05.545142Z","iopub.status.idle":"2024-12-15T03:15:29.709304Z","shell.execute_reply.started":"2024-12-15T03:15:05.545111Z","shell.execute_reply":"2024-12-15T03:15:29.708327Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    749275136                      |                     839452672                     \n================================================== | ==================================================\n### Question: If A is the number of diagonals in a  |  ### Question: If A is the number of diagonals in a\nheptagon and B is the number of diagonals in an     |  heptagon and B is the number of diagonals in an   \noctagon, find the value of B-A. ### Answer: B-A =   |  octagon, find the value of B-A. ### Answer: B-A = \n7 - 5 = 2  ### Step-by-Step Solution:  1.  First,   |  7 - 5 = 2  ### Step-by-Step Solution:  1.  We are \nwe need to find the number of diagonals in a        |  given that A is the number of diagonals in a      \nheptagon (A) and an octagon (B). 2.  The formula    |  heptagon and B is the number of diagonals in an   \nto find the number of diagonals in a polygon is:    |  octagon. 2.  To find the value of B-A, we need to \n-   Diagonals = (n * (n - 1)) / 2 3.  For a         |  find the values of A and B. 3.  We know that a    \nheptagon (7 sides), we can plug in the value of n   |  heptagon has 7 sides and a octagon has 8 sides. 4.\ninto the formula:     -   Diagonals in a heptagon   |  The formula to find the number of diagonals in a  \n(A) = (7 * (7 - 1)) / 2     -   A = (7 * 6) / 2     |  polygon is n(n-3)/2, where n is the number of     \n-   A = 42 / 2     -   A = 21 4.  For an octagon    |  sides. 5.  For a heptagon, the number of diagonals\n(8 sides), we can plug in the value of n into the   |  is 7(7-3)/2 = 7(4)/2 = 14. 6.  For an octagon, the\nformula:     -   Diagonals in an octagon (B) = (8   |  number of diagonals is 8(8-3)/2 = 8(5)/2 = 20. 7. \n* (8 - 1)) / 2     -   B = (8 * 7) / 2     -   B =  |  Therefore, the value of B-A is 20 - 14 = 6. 8.    \n56 / 2     -   B = 28 5.  Now, we can find the      |  However, we are asked to find the value of B-A,   \nvalue of B-A by subtracting the number of           |  which is 2.  ### Answer: B-A = 2  The final answer\ndiagonals in a heptagon from the number of          |  is: $\\boxed{2}$                                   \ndiagonals in an octagon:     -   B - A = 28 - 21    |                                                    \n-   B - A = 7  The final answer is: $\\boxed{7}$     |                                                    \n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:16:07.918354Z","iopub.execute_input":"2024-12-15T03:16:07.918727Z","iopub.status.idle":"2024-12-15T03:17:35.539002Z","shell.execute_reply.started":"2024-12-15T03:16:07.918694Z","shell.execute_reply":"2024-12-15T03:17:35.538002Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    749275136                      |                     839452672                     \n================================================== | ==================================================\n### Question: There are 5 numbers, 3.4, 7/2, 1.7,   |  ### Question: There are 5 numbers, 3.4, 7/2, 1.7, \n27/10, 2.9. What is the smallest number including   |  27/10, 2.9. What is the smallest number including \nthe decimal point? ### Answer: The smallest number  |  the decimal point? ### Answer: The smallest number\nincluding the decimal point is 1.7.  ### Step-by-   |  including the decimal point is 1.7.  ### Step-by- \nStep Solution:  1.  We are given 5 numbers: 3.4,    |  Step Solution:  1.  We are given 5 numbers: 3.4,  \n7/2, 1.7, 27/10, and 2.9. 2.  To find the smallest  |  7/2, 1.7, 27/10, and 2.9. 2.  To find the smallest\nnumber including the decimal point, we need to      |  number including the decimal point, we need to    \nfind the smallest number that includes all the      |  find the smallest number that includes all the    \ngiven decimal points. 3.  We can start by finding   |  given decimal points. 3.  We can start by finding \nthe decimal points of the given numbers. 4.  The    |  the decimal points of the given numbers. 4.  The  \ndecimal point of 3.4 is 3.4. 5.  The decimal point  |  decimal point of 3.4 is 3.4. 5.  The decimal point\nof 7/2 is 0.35. 6.  The decimal point of 1.7 is     |  of 7/2 is 0.35. 6.  The decimal point of 1.7 is   \n1.7. 7.  The decimal point of 27/10 is 2.7. 8.      |  1.7. 7.  The decimal point of 27/10 is 2.7. 8.    \nThe decimal point of 2.9 is 0.9. 9.  Now, we need   |  The decimal point of 2.9 is 0.9. 9.  Now, we need \nto find the smallest number that includes all the   |  to find the smallest number that includes all the \ndecimal points. 10. We can do this by finding the   |  decimal points. 10. We can do this by finding the \nsmallest number that includes all the decimal       |  smallest number that includes all the decimal     \npoints. 11. The smallest number that includes all   |  points. 11. The smallest number that includes all \nthe decimal points is 1.7.  ### Step-by-Step        |  the decimal points is 1.7.  ### Step-by-Step      \nSolution Continued:  12. To find the smallest       |  Solution Continued:  12. To find the smallest     \nnumber that includes all the decimal points, we     |  number that includes all the decimal points, we   \ncan start by finding the decimal points of the      |  can start by finding the decimal point of the     \ngiven numbers. 13. The decimal point of 3.4 is      |  smallest number. 13. The decimal point of 1.7 is  \n3.4. 14. The decimal point of 7/2 is 0.35. 15. The  |  1.7. 14. We can add the decimal points of the     \ndecimal point of 1.7 is 1.7. 16. The decimal point  |  given numbers to the decimal point of 1.7. 15. The\nof 27/10 is 2.7. 17. The decimal point of 2.9 is    |  decimal point of 1.7 + 3.4 is 1.7 + 3.4 = 5.1. 16.\n0.9. 18. Now, we need to find the smallest number   |  The decimal point of 1.7 + 3.4 + 0.35 is 1.7 + 3.4\nthat includes all the decimal points. 19. We can    |  + 0.35 = 5.6. 17. The decimal point of 1.7 + 3.4 +\ndo this by finding the smallest number that         |  0.35 + 2.7 is 1.7 + 3.4 + 0.35 + 2.7 = 7.9. 18.   \nincludes all the decimal points. 20. The smallest   |  The decimal point of 1.7 + 3.4 + 0.35 + 2.7 + 0.9 \nnumber that includes all the decimal points is      |  is 1.7 + 3.4 + 0.35 + 2.7 + 0.9 = 9.9. 19. The    \n1.7.  ### Step-by-Step Solution Continued           |  decimal point of 1.7 + 3.4 + 0.35 + 2.7 + 0.9 +   \n(continued):  21. To find the smallest number that  |  0.1 is 1.7 + 3.4 + 0.35 + 2.7 + 0.9 + 0.1 = 10.1. \nincludes all the decimal points, we can start by    |  20. The decimal point of 1.7 + 3.4 + 0.35 + 2.7 + \nfinding the decimal points of the given numbers.    |  0.9 + 0.1 + 0.3 is 1.7 + 3.4 + 0.35 + 2.7 + 0.9 + \n22. The decimal point of 3.4 is 3.4. 23. The        |  0.1 + 0.3 = 10.6. 21. The decimal point of 1.7 +  \ndecimal point of 7/2 is 0.35. 24. The decimal       |  3.4 + 0.35 + 2.7 + 0.9 + 0.1 + 0.3 + 0.2 is 1.7 + \npoint of 1.7 is 1.7. 25. The decimal point of       |  3.4 + 0.35 + 2.7 + 0.9 + 0.1 + 0.3 + 0.2 = 10.8.  \n27/10 is 2.7. 26. The decimal point of 2.9 is 0.9.  |  22. The decimal point of 1.7 + 3.4 + 0.35 + 2.7 + \n27. Now, we need to find the smallest number that   |  0.9 + 0.1 + 0.3 + 0.2 + 0.1 is 1.7 + 3.4 + 0.35 + \nincludes all the decimal points. 28. We can do      |  2.7 + 0.9 + 0.1 + 0.3 + 0.2 + 0.1 = 10.9. 23. The \nthis by finding the smallest number that includes   |  decimal point of 1.7 + 3.4 + 0.35 + 2.7 + 0.9 +   \nall the decimal points. 29. The smallest number     |  0.1 + 0.3 + 0.2 + 0.1 + 0.1 is 1.7 + 3.4 + 0.35 + \nthat includes all the decimal points is 1.7.  ###   |  2.7 + 0.9 + 0.1 + 0.3 + 0.2 + 0.1 + 0.1 = 11. 24. \nStep-by-Step Solution Continued (continued):  30.   |  The decimal point of 1.7 + 3.4 + 0.35 + 2.7 + 0.9 \nTo find the smallest number that includes all the   |  + 0.1 + 0.3 + 0.2 + 0.1 + 0.1 + 0.1 is 1.7 + 3.4 +\ndecimal points, we can start by finding the         |  0.35                                              \ndecimal points of the given numbers. 31. The        |                                                    \ndecimal point of 3.4 is 3.4. 32. The decimal point  |                                                    \nof 7/2 is 0.35. 33. The decimal point of 1.7 is     |                                                    \n1.7. 34. The decimal point of 27/10 is 2.7. 35.     |                                                    \nThe decimal point of 2.9 is 0.9. 36. Now, we need   |                                                    \nto find the smallest number that includes all the   |                                                    \ndecimal points. 37. We can do this by finding the   |                                                    \nsmallest number that includes all the decimal       |                                                    \npoints. 38. The smallest number that includes all   |                                                    \nthe decimal points is 1.7.  ### Step-by-Step        |                                                    \nSolution Continued (continued):  39. To find the    |                                                    \nsmallest number that includes all the decimal       |                                                    \npoints, we can start by finding the decimal points  |                                                    \nof the given numbers. 40. The decimal point of 3.4  |                                                    \nis 3.4. 41. The decimal point of 7/2 is 0.35. 42.   |                                                    \nThe decimal point of 1.7 is 1.7. 43. The decimal    |                                                    \npoint of 27/10 is 2.7. 44. The decimal point of     |                                                    \n2.9 is 0.9. 45. Now, we need to find the smallest   |                                                    \nnumber that includes all the decimal points. 46.    |                                                    \nWe can do this by finding the smallest number that  |                                                    \nincludes all the decimal points. 47. The smallest   |                                                    \nnumber that includes all the decimal points is      |                                                    \n1.7.  ### Step-by-Step Solution Continued           |                                                    \n(continued):  48. To find the smallest number that  |                                                    \nincludes all the decimal points, we can start by    |                                                    \nfinding the decimal points of the given numbers.    |                                                    \n49. The decimal point of 3.4 is 3.4. 50. The        |                                                    \ndecimal point of 7/2 is 0.35. 51. The decimal       |                                                    \npoint of 1.7 is 1.7. 52. The decimal point of       |                                                    \n27/10 is 2.7. 53. The decimal point of              |                                                    \n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:03:13.941910Z","iopub.execute_input":"2024-12-15T03:03:13.942177Z","iopub.status.idle":"2024-12-15T03:04:15.640959Z","shell.execute_reply.started":"2024-12-15T03:03:13.942152Z","shell.execute_reply":"2024-12-15T03:04:15.639989Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    749275136                      |                     839452672                     \n================================================== | ==================================================\n### Question: How many numbers greater than 1.1     |  ### Question: How many numbers greater than 1.1   \nare there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###    |  are there in 1.4, 9/10, 1.2, 0.5, and 13/10? ###  \nAnswer: To find the answer, we need to find the     |  Answer: To find the answer, we need to find the   \nnumber of integers in each of the given ranges and  |  number of integers in each of the given intervals.\nthen add them up.  ### Step 1: Convert the given    |  ### Step 1: Convert the given fractions to        \nfractions to decimals 1.4 = 1.4 9/10 = 0.9 1.2 =    |  decimals 1.4 = 1.4 9/10 = 0.9 1.2 = 1.2 0.5 = 0.5 \n1.2 0.5 = 0.5 13/10 = 1.3  ### Step 2: Find the     |  13/10 = 1.3  ### Step 2: Find the number of       \nnumber of integers in each range 1.4: 1 to 1.3 = 3  |  integers in each interval 1.1 < x < 1.4 1.4 < x < \nintegers 9/10: 1 to 0.9 = 9 integers 1.2: 1 to 1.1  |  1.2 1.2 < x < 1.5 1.5 < x < 1.6 1.6 < x < 1.7 1.7 \n= 1 integer 0.5: 1 to 0.4 = 5 integers 1.3: 1 to    |  < x < 1.8 1.8 < x < 1.9 1.9 < x < 2.0 1.0 < x <   \n1.2 = 1 integer  ### Step 3: Add up the number of   |  1.1 1.1 < x < 1.2 1.2 < x < 1.3 1.3 < x < 1.4 1.4 \nintegers in each range 3 + 9 + 1 + 5 + 1 = 19  The  |  < x < 1.5 1.5 < x < 1.6 1.6 < x < 1.7 1.7 < x <   \nfinal answer is: $\\boxed{19}$                       |  1.8 1.8 < x < 1.9 1.9 < x < 2.0 1.0 < x < 1.1 1.1 \n                                                    |  < x < 1.2 1.2 < x < 1.3 1.3 < x < 1.4 1.4 < x <   \n                                                    |  1.5 1.5 < x < 1.6 1.6 < x < 1.7 1.7 < x < 1.8 1.8 \n                                                    |  < x < 1.9 1.9 < x < 2.0 1.0 < x < 1.1 1.1 < x <   \n                                                    |  1.2 1.2 < x < 1.3 1.3 < x < 1.4 1.4 < x < 1.5 1.5 \n                                                    |  < x < 1.6 1.6 < x < 1.7 1.7 < x < 1.8 1.8 < x <   \n                                                    |  1.9 1.9 < x < 2.0 1.0 < x < 1.1 1.1 < x < 1.2 1.2 \n                                                    |  < x < 1.3 1.3 < x < 1.4 1.4 < x < 1.5 1.5 < x <   \n                                                    |  1.6 1.6 < x < 1.7 1.7 < x < 1.8 1.8 < x < 1.9 1.9 \n                                                    |  < x < 2.0 1.0 < x < 1.1 1.1 < x < 1.2 1.2 < x <   \n                                                    |  1.3 1.3 < x < 1.4 1.4 < x < 1.5 1.5 < x < 1.6 1.6 \n                                                    |  < x < 1.7 1.7 < x < 1.8 1.8 < x < 1.9 1.9 < x <   \n                                                    |  2.0 1.0 < x < 1.1 1.1 < x < 1.2 1.2 < x < 1.3 1.3 \n                                                    |  < x < 1.4 1.4 < x < 1.5 1.5 < x < 1.6 1.6 < x <   \n                                                    |  1.7 1.7 < x < 1.8 1.8 < x < 1.9 1.9 < x < 2.0 1.0 \n                                                    |  < x < 1.1 1.1 < x < 1.2 1.2 < x < 1.3 1.3 < x <   \n                                                    |  1.4 1.4 < x < 1.5 1.5 < x < 1.6 1.6 < x < 1.7 1.7 \n                                                    |  < x < 1.8 1.8 < x < 1.9 1.9 < x < 2.0 1.0 < x <   \n                                                    |  1.1 1.1 < x < 1.2 1.2 < x < 1.3 1.3 < x < 1.4 1.4 \n                                                    |  < x < 1.5 1.5 < x < 1.6 1.6 < x < 1.              \n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-12-15T03:18:51.262172Z","iopub.execute_input":"2024-12-15T03:18:51.262907Z","iopub.status.idle":"2024-12-15T03:19:56.436809Z","shell.execute_reply.started":"2024-12-15T03:18:51.262873Z","shell.execute_reply":"2024-12-15T03:19:56.435823Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    749275136                      |                     839452672                     \n================================================== | ==================================================\n### Question: Sehun wants to make a three-digit     |  ### Question: Sehun wants to make a three-digit   \nnumber by using the single-digit natural numbers    |  number by using the single-digit natural numbers  \n0, 2, and 9 once each. How many total can he make?  |  0, 2, and 9 once each. How many total can he make?\n### Answer: There are 3! = 6 ways to arrange the    |  ### Answer: There are 3! = 6 ways to arrange the  \ndigits 0, 2, and 9. Therefore, there are 6 total    |  digits 0, 2, and 9. Therefore, there are 6 total  \nways to make a three-digit number.  ### Step-by-    |  ways to make a three-digit number.  ### Step-by-  \nStep Solution:  1.  **Identify the digits**: The    |  Step Solution:  1.  **Identify the digits**: The  \nsingle-digit natural numbers 0, 2, and 9 are        |  single-digit natural numbers 0, 2, and 9 are      \navailable for use. 2.  **Determine the number of    |  available for use. 2.  **Determine the number of  \npositions**: There are three positions in the       |  positions**: There are three positions in the     \nthree-digit number. 3.  **Calculate the             |  three-digit number. 3.  **Calculate the           \npermutations**: The number of permutations of the   |  permutations**: The number of permutations of the \ndigits is given by 3! (3 factorial), which is       |  digits is given by 3! (3 factorial), which is     \nequal to 6. 4.  **Consider the arrangement of       |  equal to 6. 4.  **Consider the arrangement of     \ndigits**: Since the digits are distinct, the order  |  digits**: Since the digits are distinct and the   \nmatters. Therefore, we need to consider all         |  order matters, we need to consider all possible   \npossible arrangements of the digits. 5.             |  arrangements of the digits. 5.  **Calculate the   \n**Calculate the total number of arrangements**:     |  total number of arrangements**: There are 3! = 6  \nThere are 3! = 6 ways to arrange the digits 0, 2,   |  ways to arrange the digits 0, 2, and 9. Therefore,\nand 9. Therefore, there are 6 total ways to make a  |  there are 6 total ways to make a three-digit      \nthree-digit number.  ### Example Walkthrough:  For  |  number.  ### Step-by-Step Solution Continued:  6. \nexample, let's say Sehun wants to make the number   |  **Consider the arrangement of digits**: Since the \n902. Here's how he can arrange the digits:  -   0   |  digits are distinct and the order matters, we need\ncan go in the hundreds place. -   9 can go in the   |  to consider all possible arrangements of the      \ntens place. -   2 can go in the ones place.  After  |  digits. 7. **Calculate the total number of        \narranging the digits, we get the number 902. This   |  arrangements**: There are 3! = 6 ways to arrange  \nis one of the 6 possible arrangements of the        |  the digits 0, 2, and 9. Therefore, there are 6    \ndigits 0, 2, and 9. Therefore, Sehun can make the   |  total ways to make a three-digit number.  ###     \nnumber 902.                                         |  Step-by-Step Solution Continued (continued):  8.  \n                                                    |  **Consider the arrangement of digits**: Since the \n                                                    |  digits are distinct and the order matters, we need\n                                                    |  to consider all possible arrangements of the      \n                                                    |  digits. 9. **Calculate the total number of        \n                                                    |  arrangements**: There are 3! = 6 ways to arrange  \n                                                    |  the digits 0, 2, and 9. Therefore, there are 6    \n                                                    |  total ways to make a three-digit number.  ###     \n                                                    |  Step-by-Step Solution Continued (continued):  10. \n                                                    |  **Consider the arrangement of digits**: Since the \n                                                    |  digits are distinct and the order matters, we need\n                                                    |  to consider all possible arrangements of the      \n                                                    |  digits. 11. **Calculate the total number of       \n                                                    |  arrangements**: There are 3! = 6 ways to arrange  \n                                                    |  the digits 0, 2, and 9. Therefore, there are 6    \n                                                    |  total ways to make a three-digit number.  ###     \n                                                    |  Step-by-Step Solution Continued (continued):  12. \n                                                    |  **Consider the arrangement of digits**: Since the \n                                                    |  digits are distinct and the order matters, we need\n                                                    |  to consider all possible arrangements of the      \n                                                    |  digits. 13. **Calculate the total number of       \n                                                    |  arrangements**: There are 3! = 6 ways to arrange  \n                                                    |  the digits 0, 2, and 9. Therefore, there are 6    \n                                                    |  total ways to make a three-digit number.  ###     \n                                                    |  Step-by-Step Solution Continued (continued):  14. \n                                                    |  **Consider the arrangement of digits**: Since the \n                                                    |  digits are distinct and the order matters, we need\n                                                    |  to consider all possible arrangements of the      \n                                                    |  digits. 15. **Calculate the total number of       \n                                                    |  arrangements**: There are 3! = 6 ways to arrange  \n                                                    |  the digits 0, 2, and 9. Therefore, there are 6    \n                                                    |  total ways to make a three-digit number.  ###     \n                                                    |  Step-by-Step Solution Continued (continued):  16. \n                                                    |  **Consider the arrangement of digits**: Since the \n                                                    |  digits are distinct and the order matters, we need\n                                                    |  to consider all possible arrangements of the      \n                                                    |  digits. 17. **Calculate the total number of       \n                                                    |  arrangements**: There are 3! = 6 ways to arrange  \n                                                    |  the digits 0, 2, and 9. Therefore, there are 6    \n                                                    |  total ways to make a three-digit number.  ###     \n                                                    |  Step-by-Step Solution Continued (continued):  18. \n                                                    |  **Consider the arrangement of digits**: Since the \n                                                    |  digits are distinct and the order matters, we need\n                                                    |  to consider all possible arrangements of the      \n                                                    |  digits. 19. **Calculate the total number of       \n                                                    |  arrangements**: There are 3! = 6 ways to arrange  \n                                                    |  the digits 0, 2, and 9. Therefore, there are 6    \n                                                    |  total ways to make a three-digit number.  ###     \n                                                    |  Step-by-Step Solution Continued (continued):  20. \n                                                    |  **Consider the arrangement of digits**: Since the \n                                                    |  digits are distinct and the order matters, we need\n                                                    |  to consider all possible arrangements of the      \n                                                    |  digits. 21. **Calculate the total number of       \n                                                    |  arrangements**: There are 3! = 6 ways to arrange  \n                                                    |  the digits 0, 2, and 9. Therefore, there are 6    \n                                                    |  total ways to make a three-digit number.  ###     \n                                                    |  Step-by-Step Solution Continued (continued):  22. \n                                                    |  **Consider the arrangement of digits**: Since the \n                                                    |  digits are distinct and the order matters, we need\n                                                    |  to consider all possible arrangements of the      \n                                                    |  digits. 23. **Calculate the total number of       \n                                                    |  arrangements**: There are 3! = 6 ways to arrange  \n                                                    |  the digits 0, 2, and 9. Therefore, there are 6    \n                                                    |  total ways to make a three-digit number.  ###     \n                                                    |  Step-by-Step Solution Continued (continued):  24. \n                                                    |  **Consider the arrangement of digits**: Since the \n                                                    |  digits are distinct and the order matters         \n","output_type":"stream"}],"execution_count":65}]}