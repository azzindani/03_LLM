{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    TextDataset,\n",
    "    EvalPrediction,\n",
    "    DataCollatorWithPadding,\n",
    "    GenerationConfig,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModelForSequenceClassification,\n",
    "    TaskType,\n",
    "    AutoPeftModelForSequenceClassification,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = str(pathlib.Path().resolve())\n",
    "DATASET_PATH = MAIN_PATH + '\\\\datasets'\n",
    "MODEL_PATH = MAIN_PATH + '\\\\models'\n",
    "SAVE_PATH = MAIN_PATH + '\\\\fine_tuned_models'\n",
    "MODELS = 'D:\\\\AI\\\\LLM\\\\models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.yaml',\n",
       " 'gemma-2-2b',\n",
       " 'Llama-3-8B-GPTQ-4-Bit.safetensors',\n",
       " 'Llama-3-8B-Instruct-GPTQ-4-Bit.safetensors',\n",
       " 'Llama-3.2-11B-Vision-Instruct-bnb-4bit',\n",
       " 'Llama-3.2-1B-Instruct',\n",
       " 'Llama-3.2-3B-Instruct',\n",
       " 'Meta-Llama-3.1-8B-Instruct-GPTQ-INT4',\n",
       " 'Phi-3-mini-128k-instruct',\n",
       " 'Phi-3-mini-128k-instruct-onnx',\n",
       " 'Phi-3-mini-4k-instruct-q4.gguf',\n",
       " 'place-your-models-here.txt',\n",
       " 'Qwen2.5-0.5B',\n",
       " 'Qwen2.5-1.5B',\n",
       " 'Qwen2.5-3B',\n",
       " 'Qwen2.5-7B-Instruct-GPTQ-Int4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = os.listdir(MODELS)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\AI\\\\LLM\\\\models\\\\Llama-3.2-1B-Instruct'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = MODELS + '\\\\' + models[5]\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    trust_remote_code = True\n",
    ")#.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters : 749275136\n",
      "Trainable parameters : 262735872\n",
      "Trainable percentage: 35.07%\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "trainable_percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print('Total parameters :', total_params)\n",
    "print('Trainable parameters :', trainable_params)\n",
    "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side = 'left')\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token' : '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:655: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|eot_id|><|begin_of_text|>hello, I am using a 2015 2nd gen Mustang GT 3.2L V6 engine, with a 6-speed manual transmission. I'm looking for a reliable and affordable solution to upgrade the exhaust system. I've considered purchasing a used or new exhaust system from a reputable aftermarket manufacturer. I've researched and found several options, including the following:\n",
      "\n",
      "* 4th Gen Mustang GT 4.0L V6 exhaust system with a 4-speed automatic transmission\n",
      "* 4th Gen Mustang GT 4.0L V6 exhaust system with a 6-speed automatic transmission\n",
      "* 4th Gen Mustang GT 4.0L V6 exhaust system with a 4-speed automatic transmission and a catalytic converter\n",
      "* 4th Gen Mustang GT 4.0L V6 exhaust system with a 6-speed automatic transmission and a catalytic converter\n",
      "\n",
      "I've also considered purchasing a used exhaust system from a reputable seller, such as:\n",
      "\n",
      "* Ford Performance\n",
      "* Edelbrock\n",
      "* Garrett\n",
      "* JE Motorsports\n",
      "\n",
      "I'm looking for a reliable and affordable solution to upgrade the exhaust system. I'd like to know what is the best way to go about this project. Here are the details of my vehicle:\n",
      "\n",
      "* Year: 2015\n",
      "* Model: 2nd Gen Mustang GT\n",
      "* Engine: 3.2L V6\n",
      "* Transmission: 6-speed manual\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = ['hello, my name is nitish pandey', 'hello']\n",
    "input_tok = tokenizer(inputs, padding = True, truncation = True, return_tensors = 'pt').to(device)\n",
    "output = model.generate(**input_tok, max_length = 300)\n",
    "print(tokenizer.decode(output[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>query</th>\n",
       "      <th>original_question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MATH_AnsAug</td>\n",
       "      <td>Gracie and Joe are choosing numbers on the com...</td>\n",
       "      <td>Gracie and Joe are choosing numbers on the com...</td>\n",
       "      <td>The distance between two points $(x_1,y_1)$ an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSM_Rephrased</td>\n",
       "      <td>What is the total cost of purchasing equipment...</td>\n",
       "      <td>The treasurer of a football team must buy equi...</td>\n",
       "      <td>Each player requires a $25 jersey, a $15.20 pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GSM_SV</td>\n",
       "      <td>Diego baked 12 cakes for his sister's birthday...</td>\n",
       "      <td>Diego baked 12 cakes for his sister's birthday...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MATH_AnsAug</td>\n",
       "      <td>Convert $10101_3$ to a base 10 integer.</td>\n",
       "      <td>Convert $10101_3$ to a base 10 integer.</td>\n",
       "      <td>$10101_3 = 1 \\cdot 3^4 + 0 \\cdot 3^3 + 1 \\cdot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GSM_FOBAR</td>\n",
       "      <td>Sue works in a factory and every 30 minutes, a...</td>\n",
       "      <td>Sue works in a factory and every 30 minutes, a...</td>\n",
       "      <td>We know that every 30 minutes, a machine produ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            type                                              query  \\\n",
       "0    MATH_AnsAug  Gracie and Joe are choosing numbers on the com...   \n",
       "1  GSM_Rephrased  What is the total cost of purchasing equipment...   \n",
       "2         GSM_SV  Diego baked 12 cakes for his sister's birthday...   \n",
       "3    MATH_AnsAug            Convert $10101_3$ to a base 10 integer.   \n",
       "4      GSM_FOBAR  Sue works in a factory and every 30 minutes, a...   \n",
       "\n",
       "                                   original_question  \\\n",
       "0  Gracie and Joe are choosing numbers on the com...   \n",
       "1  The treasurer of a football team must buy equi...   \n",
       "2  Diego baked 12 cakes for his sister's birthday...   \n",
       "3            Convert $10101_3$ to a base 10 integer.   \n",
       "4  Sue works in a factory and every 30 minutes, a...   \n",
       "\n",
       "                                            response  \n",
       "0  The distance between two points $(x_1,y_1)$ an...  \n",
       "1  Each player requires a $25 jersey, a $15.20 pa...  \n",
       "2  To solve this problem, we need to determine th...  \n",
       "3  $10101_3 = 1 \\cdot 3^4 + 0 \\cdot 3^3 + 1 \\cdot...  \n",
       "4  We know that every 30 minutes, a machine produ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = 'meta-math/MetaMathQA'\n",
    "\n",
    "dataset = load_dataset(dataset_name, split = 'train')\n",
    "dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'query', 'original_question', 'response'],\n",
       "    num_rows: 395000\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size = 0.1, shuffle = True)\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'query', 'original_question', 'response'],\n",
       "    num_rows: 355500\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'query', 'original_question', 'response'],\n",
       "    num_rows: 39500\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_alpha = 16\n",
    "lora_dropout = 0.1\n",
    "lora_r = 64\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha = lora_alpha,\n",
    "    lora_dropout = lora_dropout,\n",
    "    r = lora_r,\n",
    "    bias = 'none',\n",
    "    task_type = 'CAUSAL_LM',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 1,242,630,144 || trainable%: 0.5485\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model, peft_config, adapter_name = 'math')\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_device_train_batch_size = 4\n",
    "gradient_accumulation_steps = 4\n",
    "optim = 'paged_adamw_32bit'\n",
    "save_steps = 100\n",
    "logging_steps = 10\n",
    "learning_rate = 2e-4\n",
    "max_grad_norm = 0.3\n",
    "max_steps = 100\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = 'constant'\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir = SAVE_PATH,\n",
    "    per_device_train_batch_size = per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
    "    optim = optim,\n",
    "    save_steps = save_steps,\n",
    "    logging_steps = logging_steps,\n",
    "    learning_rate = learning_rate,\n",
    "    fp16 = True,\n",
    "    max_grad_norm = max_grad_norm,\n",
    "    max_steps = max_steps,\n",
    "    warmup_ratio = warmup_ratio,\n",
    "    group_by_length = True,\n",
    "    lr_scheduler_type = lr_scheduler_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 355500/355500 [00:37<00:00, 9559.93 examples/s] \n",
      "Map: 100%|██████████| 39500/39500 [00:04<00:00, 9330.91 examples/s] \n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 512\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset,\n",
    "    peft_config = peft_config,\n",
    "    dataset_text_field = 'response',\n",
    "    max_seq_length = max_seq_length,\n",
    "    tokenizer = tokenizer,\n",
    "    args = training_arguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters : 762906624\n",
      "Trainable parameters : 6815744\n",
      "Trainable percentage: 0.89%\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "trainable_percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print('Total parameters :', total_params)\n",
    "print('Trainable parameters :', trainable_params)\n",
    "print('Trainable percentage: {:.2f}%'.format(trainable_percentage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [02:13<14:23,  9.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.91, 'grad_norm': 0.13153253495693207, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [03:17<08:10,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0031, 'grad_norm': 0.20793874561786652, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [04:05<05:16,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.96, 'grad_norm': 0.2766612768173218, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [04:32<02:01,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0362, 'grad_norm': 0.2981817126274109, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [04:47<01:22,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2535, 'grad_norm': 1.8408682346343994, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [07:02<06:23,  9.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6949, 'grad_norm': 0.18175795674324036, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [08:03<02:57,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7971, 'grad_norm': 0.21540848910808563, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [08:48<01:26,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9169, 'grad_norm': 0.24530212581157684, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [09:14<00:19,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9769, 'grad_norm': 0.3442721664905548, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:28<00:00,  1.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1093, 'grad_norm': 0.6363228559494019, 'learning_rate': 0.0002, 'epoch': 0.0}\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\n",
    "#save_model.save_pretrained(SAVE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
