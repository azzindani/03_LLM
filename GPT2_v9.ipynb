{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline, GPT2Config, TextDataset\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import TrainingArguments, Trainer, set_seed, EvalPrediction, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, TFAutoModel, AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, PeftModelForSequenceClassification, TaskType, AutoPeftModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = str(pathlib.Path().resolve())\n",
    "DATASET_PATH = MAIN_PATH + '\\\\datasets'\n",
    "MODEL_PATH = MAIN_PATH + '\\\\models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-cased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-cased',\n",
       " 'bert-large-uncased',\n",
       " 'flan-t5-base',\n",
       " 'flan-t5-large',\n",
       " 'flan-t5-small',\n",
       " 'gpt2',\n",
       " 'gpt2-large',\n",
       " 'gpt2-medium']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = os.listdir(MODEL_PATH)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python\\\\LLM_Environment\\\\models\\\\gpt2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = MODEL_PATH + '\\\\' + models[8]\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>query</th>\n",
       "      <th>original_question</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MATH_AnsAug</td>\n",
       "      <td>Gracie and Joe are choosing numbers on the com...</td>\n",
       "      <td>Gracie and Joe are choosing numbers on the com...</td>\n",
       "      <td>The distance between two points $(x_1,y_1)$ an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GSM_Rephrased</td>\n",
       "      <td>What is the total cost of purchasing equipment...</td>\n",
       "      <td>The treasurer of a football team must buy equi...</td>\n",
       "      <td>Each player requires a $25 jersey, a $15.20 pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GSM_SV</td>\n",
       "      <td>Diego baked 12 cakes for his sister's birthday...</td>\n",
       "      <td>Diego baked 12 cakes for his sister's birthday...</td>\n",
       "      <td>To solve this problem, we need to determine th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MATH_AnsAug</td>\n",
       "      <td>Convert $10101_3$ to a base 10 integer.</td>\n",
       "      <td>Convert $10101_3$ to a base 10 integer.</td>\n",
       "      <td>$10101_3 = 1 \\cdot 3^4 + 0 \\cdot 3^3 + 1 \\cdot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GSM_FOBAR</td>\n",
       "      <td>Sue works in a factory and every 30 minutes, a...</td>\n",
       "      <td>Sue works in a factory and every 30 minutes, a...</td>\n",
       "      <td>We know that every 30 minutes, a machine produ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            type                                              query  \\\n",
       "0    MATH_AnsAug  Gracie and Joe are choosing numbers on the com...   \n",
       "1  GSM_Rephrased  What is the total cost of purchasing equipment...   \n",
       "2         GSM_SV  Diego baked 12 cakes for his sister's birthday...   \n",
       "3    MATH_AnsAug            Convert $10101_3$ to a base 10 integer.   \n",
       "4      GSM_FOBAR  Sue works in a factory and every 30 minutes, a...   \n",
       "\n",
       "                                   original_question  \\\n",
       "0  Gracie and Joe are choosing numbers on the com...   \n",
       "1  The treasurer of a football team must buy equi...   \n",
       "2  Diego baked 12 cakes for his sister's birthday...   \n",
       "3            Convert $10101_3$ to a base 10 integer.   \n",
       "4  Sue works in a factory and every 30 minutes, a...   \n",
       "\n",
       "                                            response  \n",
       "0  The distance between two points $(x_1,y_1)$ an...  \n",
       "1  Each player requires a $25 jersey, a $15.20 pa...  \n",
       "2  To solve this problem, we need to determine th...  \n",
       "3  $10101_3 = 1 \\cdot 3^4 + 0 \\cdot 3^3 + 1 \\cdot...  \n",
       "4  We know that every 30 minutes, a machine produ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = \"meta-math/MetaMathQA\"\n",
    "\n",
    "dataset = load_dataset(dataset_name, split='train')\n",
    "dataset.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['type', 'query', 'original_question', 'response'],\n",
       "    num_rows: 395000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(example):\n",
    "    example['prompt'] = f\"\"\"<|system|>\n",
    "    You are a intelligent chatbot and expertise in Mathematics.</s>\n",
    "    <|user|>\n",
    "    {example['query']}.\n",
    "    <|assistant|>\n",
    "    {example['response']}\"\"\"\n",
    "    return example\n",
    "\n",
    "def tokenize_datasets(dataset):\n",
    "    tokenized_dataset = dataset.map(\n",
    "        lambda example: tokenizer(\n",
    "            example['prompt'],\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            ),\n",
    "        batched=True,\n",
    "        remove_columns=['prompt'])\n",
    "    return tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    prepare_datasets, remove_columns=['query', 'response', \"original_question\", \"type\"]\n",
    ")\n",
    "dataset = dataset.shuffle(42).select(range(395000)).train_test_split(test_size=0.1, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=GPT2TokenizerFast(name_or_path='D:\\Python\\LLM_Environment\\models\\gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_path = './model'\n",
    "# Define your training arguments\n",
    "batch_size = 4\n",
    "max_steps = 100\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    gradient_accumulation_steps=batch_size,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    log_level=\"debug\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    save_safetensors=False,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    learning_rate=2e-5,\n",
    "    eval_steps=50,\n",
    "    max_steps=max_steps,\n",
    "    warmup_steps=30,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "Using auto half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    dataset_text_field=\"prompt\",\n",
    "    max_seq_length=512,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently training with a batch size of: 4\n",
      "***** Running training *****\n",
      "  Num examples = 355,500\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 100\n",
      "  Number of trainable parameters = 124,439,808\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      " 50%|█████     | 50/100 [00:24<00:22,  2.21it/s]\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 39500\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3817, 'grad_norm': 3.72003173828125, 'learning_rate': 1.657938725939713e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 50%|█████     | 50/100 [07:01<00:22,  2.21it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.5409225225448608, 'eval_runtime': 396.7402, 'eval_samples_per_second': 99.561, 'eval_steps_per_second': 24.89, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [07:25<00:00,  2.07it/s]  \n",
      "***** Running Evaluation *****\n",
      "  Num examples = 39500\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6179, 'grad_norm': 2.8454039096832275, 'learning_rate': 1.0069334586854106e-08, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 100/100 [13:58<00:00,  2.07it/s]  \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 100/100 [13:58<00:00,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.435025691986084, 'eval_runtime': 392.6447, 'eval_samples_per_second': 100.6, 'eval_steps_per_second': 25.15, 'epoch': 0.0}\n",
      "{'train_runtime': 838.3314, 'train_samples_per_second': 1.909, 'train_steps_per_second': 0.119, 'train_loss': 1.9997915649414062, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=100, training_loss=1.9997915649414062, metrics={'train_runtime': 838.3314, 'train_samples_per_second': 1.909, 'train_steps_per_second': 0.119, 'total_flos': 302157697536000.0, 'train_loss': 1.9997915649414062, 'epoch': 0.00450070323488045})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
