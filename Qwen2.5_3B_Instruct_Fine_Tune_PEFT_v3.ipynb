{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-07T23:48:10.793252Z","iopub.execute_input":"2024-12-07T23:48:10.793540Z","iopub.status.idle":"2024-12-07T23:49:15.329206Z","shell.execute_reply.started":"2024-12-07T23:48:10.793511Z","shell.execute_reply":"2024-12-07T23:49:15.328212Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-12-07T23:49:15.330956Z","iopub.execute_input":"2024-12-07T23:49:15.331230Z","iopub.status.idle":"2024-12-07T23:49:34.823526Z","shell.execute_reply.started":"2024-12-07T23:49:15.331203Z","shell.execute_reply":"2024-12-07T23:49:34.822564Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-12-07T23:49:34.824680Z","iopub.execute_input":"2024-12-07T23:49:34.824945Z","iopub.status.idle":"2024-12-07T23:49:34.831600Z","shell.execute_reply.started":"2024-12-07T23:49:34.824920Z","shell.execute_reply":"2024-12-07T23:49:34.830593Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"model_name = 'Qwen/Qwen2.5-3B-Instruct'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-07T23:49:34.834341Z","iopub.execute_input":"2024-12-07T23:49:34.835117Z","iopub.status.idle":"2024-12-07T23:49:34.865859Z","shell.execute_reply.started":"2024-12-07T23:49:34.835076Z","shell.execute_reply":"2024-12-07T23:49:34.865141Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-12-07T23:49:34.866958Z","iopub.execute_input":"2024-12-07T23:49:34.867282Z","iopub.status.idle":"2024-12-07T23:49:34.878289Z","shell.execute_reply.started":"2024-12-07T23:49:34.867245Z","shell.execute_reply":"2024-12-07T23:49:34.877490Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:49:34.879159Z","iopub.execute_input":"2024-12-07T23:49:34.879423Z","iopub.status.idle":"2024-12-07T23:52:10.715434Z","shell.execute_reply.started":"2024-12-07T23:49:34.879397Z","shell.execute_reply":"2024-12-07T23:52:10.714543Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d57487fd2ac440758ad84b8c6e159658"}},"metadata":{}},{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"443c519d10e24bb5bbc75b656e6d6004"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e2aa8532828439e97c7235b0ccc222a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2538ab8f55b404487224cb73466736d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a34f08e6374f4716a99d85b6f0fcd864"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9b43f66c3af437d8b901f60d23433c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dddcd542d4164c028a3f1a35e35c786b"}},"metadata":{}},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 2048)\n    (layers): ModuleList(\n      (0-35): 36 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:10.716795Z","iopub.execute_input":"2024-12-07T23:52:10.717533Z","iopub.status.idle":"2024-12-07T23:52:10.725822Z","shell.execute_reply.started":"2024-12-07T23:52:10.717493Z","shell.execute_reply":"2024-12-07T23:52:10.724853Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1698672640\nTrainable parameters : 311314432\nTrainable percentage: 18.33%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:10.727091Z","iopub.execute_input":"2024-12-07T23:52:10.727488Z","iopub.status.idle":"2024-12-07T23:52:15.431041Z","shell.execute_reply.started":"2024-12-07T23:52:10.727442Z","shell.execute_reply":"2024-12-07T23:52:15.430078Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91bc8d98b5a14f78ac1c76229bb09feb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"949291d728f8490187d4009b8aca2573"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa37eb166de1423a838b5312b8ab2543"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6e1781a821a4c2db037d7e8b25e1130"}},"metadata":{}}],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"dataset_name = 'microsoft/orca-math-word-problems-200k'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:15.432279Z","iopub.execute_input":"2024-12-07T23:52:15.432652Z","iopub.status.idle":"2024-12-07T23:52:15.437461Z","shell.execute_reply.started":"2024-12-07T23:52:15.432595Z","shell.execute_reply":"2024-12-07T23:52:15.436427Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 384","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:15.439961Z","iopub.execute_input":"2024-12-07T23:52:15.440208Z","iopub.status.idle":"2024-12-07T23:52:15.454348Z","shell.execute_reply.started":"2024-12-07T23:52:15.440184Z","shell.execute_reply":"2024-12-07T23:52:15.453671Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:15.455262Z","iopub.execute_input":"2024-12-07T23:52:15.455568Z","iopub.status.idle":"2024-12-07T23:52:20.242143Z","shell.execute_reply.started":"2024-12-07T23:52:15.455529Z","shell.execute_reply":"2024-12-07T23:52:20.241261Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/6.91k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a6db88b6c27453ea7bf6e6e6332e4dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/84.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b075b2229714d7795fcb46e68647f36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/200035 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d2b936d14d34b8792038b841fe9dcd2"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'answer'],\n    num_rows: 200035\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:20.243298Z","iopub.execute_input":"2024-12-07T23:52:20.243575Z","iopub.status.idle":"2024-12-07T23:52:20.249577Z","shell.execute_reply.started":"2024-12-07T23:52:20.243548Z","shell.execute_reply":"2024-12-07T23:52:20.248930Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:20.250819Z","iopub.execute_input":"2024-12-07T23:52:20.251077Z","iopub.status.idle":"2024-12-07T23:52:20.283144Z","shell.execute_reply.started":"2024-12-07T23:52:20.251052Z","shell.execute_reply":"2024-12-07T23:52:20.282352Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:20.284052Z","iopub.execute_input":"2024-12-07T23:52:20.284291Z","iopub.status.idle":"2024-12-07T23:52:20.292301Z","shell.execute_reply.started":"2024-12-07T23:52:20.284267Z","shell.execute_reply":"2024-12-07T23:52:20.291510Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:20.293558Z","iopub.execute_input":"2024-12-07T23:52:20.293988Z","iopub.status.idle":"2024-12-07T23:52:20.302745Z","shell.execute_reply.started":"2024-12-07T23:52:20.293951Z","shell.execute_reply":"2024-12-07T23:52:20.301962Z"}},"outputs":[{"name":"stdout","text":"['question', 'answer']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:20.303665Z","iopub.execute_input":"2024-12-07T23:52:20.303994Z","iopub.status.idle":"2024-12-07T23:52:20.313533Z","shell.execute_reply.started":"2024-12-07T23:52:20.303969Z","shell.execute_reply":"2024-12-07T23:52:20.312873Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  input = examples['question']\n  output = examples['answer']\n  \n  text = prompt_format.format(input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"id":"0wXJNFBWWNYP","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:20.314589Z","iopub.execute_input":"2024-12-07T23:52:20.314904Z","iopub.status.idle":"2024-12-07T23:52:20.323872Z","shell.execute_reply.started":"2024-12-07T23:52:20.314880Z","shell.execute_reply":"2024-12-07T23:52:20.323245Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:20.324815Z","iopub.execute_input":"2024-12-07T23:52:20.325064Z","iopub.status.idle":"2024-12-07T23:52:20.782244Z","shell.execute_reply.started":"2024-12-07T23:52:20.325026Z","shell.execute_reply":"2024-12-07T23:52:20.781334Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b1888b2feb34f87ac0c107bde4429e3"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"Kidf8H5zefDC","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:20.783394Z","iopub.execute_input":"2024-12-07T23:52:20.783701Z","iopub.status.idle":"2024-12-07T23:52:20.788885Z","shell.execute_reply.started":"2024-12-07T23:52:20.783674Z","shell.execute_reply":"2024-12-07T23:52:20.787823Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:20.790264Z","iopub.execute_input":"2024-12-07T23:52:20.790527Z","iopub.status.idle":"2024-12-07T23:52:20.818655Z","shell.execute_reply.started":"2024-12-07T23:52:20.790502Z","shell.execute_reply":"2024-12-07T23:52:20.817951Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:20.819533Z","iopub.execute_input":"2024-12-07T23:52:20.819785Z","iopub.status.idle":"2024-12-07T23:52:30.265339Z","shell.execute_reply.started":"2024-12-07T23:52:20.819761Z","shell.execute_reply":"2024-12-07T23:52:30.264461Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3801d712b5ab4c54ba1cc68e637633a3"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.266579Z","iopub.execute_input":"2024-12-07T23:52:30.267227Z","iopub.status.idle":"2024-12-07T23:52:30.274154Z","shell.execute_reply.started":"2024-12-07T23:52:30.267188Z","shell.execute_reply":"2024-12-07T23:52:30.273464Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|im_end|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.275556Z","iopub.execute_input":"2024-12-07T23:52:30.275909Z","iopub.status.idle":"2024-12-07T23:52:30.353753Z","shell.execute_reply.started":"2024-12-07T23:52:30.275872Z","shell.execute_reply":"2024-12-07T23:52:30.352754Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.354822Z","iopub.execute_input":"2024-12-07T23:52:30.355122Z","iopub.status.idle":"2024-12-07T23:52:30.365083Z","shell.execute_reply.started":"2024-12-07T23:52:30.355094Z","shell.execute_reply":"2024-12-07T23:52:30.364312Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.365951Z","iopub.execute_input":"2024-12-07T23:52:30.366221Z","iopub.status.idle":"2024-12-07T23:52:30.392413Z","shell.execute_reply.started":"2024-12-07T23:52:30.366195Z","shell.execute_reply":"2024-12-07T23:52:30.391666Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  ### Question:\\nThere is a two-digit natural nu...   \n1  ### Question:\\nIn a big box, there are marbles...   \n2  ### Question:\\nAdam goes to a small school, wh...   \n3  ### Question:\\nLisa is looking to attempt a Wo...   \n4  ### Question:\\nThere is a rectangular-shaped p...   \n\n                                           input_ids  \\\n0  [14374, 15846, 510, 3862, 374, 264, 1378, 4834...   \n1  [14374, 15846, 510, 641, 264, 2409, 3745, 11, ...   \n2  [14374, 15846, 510, 37575, 5780, 311, 264, 261...   \n3  [14374, 15846, 510, 72749, 374, 3330, 311, 477...   \n4  [14374, 15846, 510, 3862, 374, 264, 51424, 347...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nThere is a two-digit natural nu...</td>\n      <td>[14374, 15846, 510, 3862, 374, 264, 1378, 4834...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nIn a big box, there are marbles...</td>\n      <td>[14374, 15846, 510, 641, 264, 2409, 3745, 11, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nAdam goes to a small school, wh...</td>\n      <td>[14374, 15846, 510, 37575, 5780, 311, 264, 261...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nLisa is looking to attempt a Wo...</td>\n      <td>[14374, 15846, 510, 72749, 374, 3330, 311, 477...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere is a rectangular-shaped p...</td>\n      <td>[14374, 15846, 510, 3862, 374, 264, 51424, 347...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.393293Z","iopub.execute_input":"2024-12-07T23:52:30.393514Z","iopub.status.idle":"2024-12-07T23:52:30.398964Z","shell.execute_reply.started":"2024-12-07T23:52:30.393490Z","shell.execute_reply":"2024-12-07T23:52:30.397886Z"}},"outputs":[{"name":"stdout","text":"### Question:\nThere is a two-digit natural number whose tens place is 3. Let A and B be the quotient of this number by 10 and the remainder of division by 10, respectively. If B multiplied by 10 plus A is 9 less than A multiplied by 10 plus B, what is the first number?\n### Answer:\nLet's denote the two-digit number as \\( XY \\), where \\( X \\) is the digit in the tens place and \\( Y \\) is the digit in the ones place. Since the tens place is 3, we have \\( X = 3 \\).\n\nAccording to the problem, \\( A \\) is the quotient of the number by 10, and \\( B \\) is the remainder of the division by 10. Therefore, \\( A = X = 3 \\) and \\( B = Y \\).\n\nThe problem states that \\( B \\times 10 + A \\) is 9 less than \\( A \\times 10 + B \\). This can be written as an equation:\n\n\\[ B \\times 10 + A = A \\times 10 + B - 9 \\]\n\nSubstituting \\( A \\) and \\( B \\) with \\( 3 \\) and \\( Y \\), respectively, we get:\n\n\\[ Y \\times 10 + 3 = 3 \\times 10 + Y - 9 \\]\n\nSimplifying the equation:\n\n\\[ 10Y + 3 = 30 + Y - 9 \\]\n\n\\[ 10Y + 3 = Y + 21 \\]\n\nSubtract \\( Y \\) from both sides:\n\n\\[ 9Y + 3 = 21 \\]\n\nSubtract 3 from both sides:\n\n\\[ 9Y = 18 \\]\n\nDivide both sides by 9:\n\n\\[ Y = 2 \\]\n\nSo the ones place digit is 2. Since we already know the tens place digit is 3, the two-digit number is \\( 32 \\).<|im_end|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.400005Z","iopub.execute_input":"2024-12-07T23:52:30.400287Z","iopub.status.idle":"2024-12-07T23:52:30.409843Z","shell.execute_reply.started":"2024-12-07T23:52:30.400243Z","shell.execute_reply":"2024-12-07T23:52:30.408974Z"}},"outputs":[{"name":"stdout","text":"[14374, 15846, 510, 3862, 374, 264, 1378, 48342, 5810, 1372, 6693, 22008, 1992, 374, 220, 18, 13, 6771, 362, 323, 425, 387, 279, 74762, 315, 419, 1372, 553, 220, 16, 15, 323, 279, 26313, 315, 12804, 553, 220, 16, 15, 11, 15576, 13, 1416, 425, 54916, 553, 220, 16, 15, 5519, 362, 374, 220, 24, 2686, 1091, 362, 54916, 553, 220, 16, 15, 5519, 425, 11, 1128, 374, 279, 1156, 1372, 5267, 14374, 21806, 510, 10061, 594, 78064, 279, 1378, 48342, 1372, 438, 17767, 57319, 1124, 701, 1380, 17767, 1599, 1124, 8, 374, 279, 15723, 304, 279, 22008, 1992, 323, 17767, 809, 1124, 8, 374, 279, 15723, 304, 279, 6174, 1992, 13, 8704, 279, 22008, 1992, 374, 220, 18, 11, 582, 614, 17767, 1599, 284, 220, 18, 1124, 3593, 11190, 311, 279, 3491, 11, 17767, 362, 1124, 8, 374, 279, 74762, 315, 279, 1372, 553, 220, 16, 15, 11, 323, 17767, 425, 1124, 8, 374, 279, 26313, 315, 279, 12804, 553, 220, 16, 15, 13, 15277, 11, 17767, 362, 284, 1599, 284, 220, 18, 1124, 8, 323, 17767, 425, 284, 809, 1124, 3593, 785, 3491, 5302, 429, 17767, 425, 1124, 15136, 220, 16, 15, 488, 362, 1124, 8, 374, 220, 24, 2686, 1091, 17767, 362, 1124, 15136, 220, 16, 15, 488, 425, 1124, 568, 1096, 646, 387, 5326, 438, 458, 23606, 1447, 78045, 425, 1124, 15136, 220, 16, 15, 488, 362, 284, 362, 1124, 15136, 220, 16, 15, 488, 425, 481, 220, 24, 1124, 2533, 3136, 3696, 10607, 17767, 362, 1124, 8, 323, 17767, 425, 1124, 8, 448, 17767, 220, 18, 1124, 8, 323, 17767, 809, 1124, 701, 15576, 11, 582, 633, 1447, 78045, 809, 1124, 15136, 220, 16, 15, 488, 220, 18, 284, 220, 18, 1124, 15136, 220, 16, 15, 488, 809, 481, 220, 24, 1124, 2533, 50, 6383, 7766, 279, 23606, 1447, 78045, 220, 16, 15, 56, 488, 220, 18, 284, 220, 18, 15, 488, 809, 481, 220, 24, 1124, 2533, 78045, 220, 16, 15, 56, 488, 220, 18, 284, 809, 488, 220, 17, 16, 1124, 2533, 3136, 2144, 17767, 809, 1124, 8, 504, 2176, 11067, 1447, 78045, 220, 24, 56, 488, 220, 18, 284, 220, 17, 16, 1124, 2533, 3136, 2144, 220, 18, 504, 2176, 11067, 1447, 78045, 220, 24, 56, 284, 220, 16, 23, 1124, 2533, 12509, 577, 2176, 11067]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.415538Z","iopub.execute_input":"2024-12-07T23:52:30.415839Z","iopub.status.idle":"2024-12-07T23:52:30.422120Z","shell.execute_reply.started":"2024-12-07T23:52:30.415814Z","shell.execute_reply":"2024-12-07T23:52:30.421194Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.423127Z","iopub.execute_input":"2024-12-07T23:52:30.423411Z","iopub.status.idle":"2024-12-07T23:52:30.433139Z","shell.execute_reply.started":"2024-12-07T23:52:30.423385Z","shell.execute_reply":"2024-12-07T23:52:30.432425Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.434032Z","iopub.execute_input":"2024-12-07T23:52:30.434266Z","iopub.status.idle":"2024-12-07T23:52:30.446205Z","shell.execute_reply.started":"2024-12-07T23:52:30.434242Z","shell.execute_reply":"2024-12-07T23:52:30.445356Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.447221Z","iopub.execute_input":"2024-12-07T23:52:30.447478Z","iopub.status.idle":"2024-12-07T23:52:30.456435Z","shell.execute_reply.started":"2024-12-07T23:52:30.447453Z","shell.execute_reply":"2024-12-07T23:52:30.455558Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n\n#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.457515Z","iopub.execute_input":"2024-12-07T23:52:30.458103Z","iopub.status.idle":"2024-12-07T23:52:30.466859Z","shell.execute_reply.started":"2024-12-07T23:52:30.458076Z","shell.execute_reply":"2024-12-07T23:52:30.465992Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:30.467824Z","iopub.execute_input":"2024-12-07T23:52:30.468158Z","iopub.status.idle":"2024-12-07T23:52:32.052791Z","shell.execute_reply.started":"2024-12-07T23:52:30.468132Z","shell.execute_reply":"2024-12-07T23:52:32.051783Z"}},"outputs":[{"name":"stdout","text":"trainable params: 119,734,272 || all params: 3,205,672,960 || trainable%: 3.7351\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:32.053945Z","iopub.execute_input":"2024-12-07T23:52:32.054245Z","iopub.status.idle":"2024-12-07T23:52:32.075783Z","shell.execute_reply.started":"2024-12-07T23:52:32.054213Z","shell.execute_reply":"2024-12-07T23:52:32.074847Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1818406912\nTrainable parameters : 119734272\nTrainable percentage: 6.58%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:32.076979Z","iopub.execute_input":"2024-12-07T23:52:32.077469Z","iopub.status.idle":"2024-12-07T23:52:32.087346Z","shell.execute_reply.started":"2024-12-07T23:52:32.077441Z","shell.execute_reply":"2024-12-07T23:52:32.086529Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 2\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:32.088404Z","iopub.execute_input":"2024-12-07T23:52:32.088712Z","iopub.status.idle":"2024-12-07T23:52:32.128135Z","shell.execute_reply.started":"2024-12-07T23:52:32.088685Z","shell.execute_reply":"2024-12-07T23:52:32.127294Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec07_23-52-32_8db4e44d5d1f,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:32.129155Z","iopub.execute_input":"2024-12-07T23:52:32.129421Z","iopub.status.idle":"2024-12-07T23:52:34.437968Z","shell.execute_reply.started":"2024-12-07T23:52:32.129397Z","shell.execute_reply":"2024-12-07T23:52:34.437072Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7ebef03d7400>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T23:52:34.439064Z","iopub.execute_input":"2024-12-07T23:52:34.439323Z","iopub.status.idle":"2024-12-08T00:36:15.406280Z","shell.execute_reply.started":"2024-12-07T23:52:34.439296Z","shell.execute_reply":"2024-12-08T00:36:15.405433Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 2\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 119,734,272\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113134400000035, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6043dd34d38047609f86d46cf13eef52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241207_235240-bs8e4gau</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/bs8e4gau' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/bs8e4gau' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/bs8e4gau</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 43:22, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.730400</td>\n      <td>0.685563</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.660100</td>\n      <td>0.586599</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.556600</td>\n      <td>0.503176</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.504300</td>\n      <td>0.468708</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.465900</td>\n      <td>0.460935</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.437100</td>\n      <td>0.456713</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.449000</td>\n      <td>0.453976</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.446800</td>\n      <td>0.452676</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.446500</td>\n      <td>0.452165</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.457500</td>\n      <td>0.452086</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=0.515424222946167, metrics={'train_runtime': 2620.4999, 'train_samples_per_second': 0.611, 'train_steps_per_second': 0.076, 'total_flos': 1.11117028294656e+16, 'train_loss': 0.515424222946167, 'epoch': 0.17777777777777778})"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:36:15.407314Z","iopub.execute_input":"2024-12-08T00:36:15.407577Z","iopub.status.idle":"2024-12-08T00:37:48.445946Z","shell.execute_reply.started":"2024-12-08T00:36:15.407551Z","shell.execute_reply":"2024-12-08T00:37:48.444760Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 01:31]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.45208632946014404, 'eval_runtime': 93.0252, 'eval_samples_per_second': 2.15, 'eval_steps_per_second': 0.537, 'epoch': 0.17777777777777778}\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:37:48.447008Z","iopub.execute_input":"2024-12-08T00:37:48.447327Z","iopub.status.idle":"2024-12-08T00:37:51.052859Z","shell.execute_reply.started":"2024-12-08T00:37:48.447302Z","shell.execute_reply":"2024-12-08T00:37:51.052152Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:37:51.053806Z","iopub.execute_input":"2024-12-08T00:37:51.054028Z","iopub.status.idle":"2024-12-08T00:37:51.346139Z","shell.execute_reply.started":"2024-12-08T00:37:51.054005Z","shell.execute_reply":"2024-12-08T00:37:51.345192Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:37:51.347155Z","iopub.execute_input":"2024-12-08T00:37:51.347398Z","iopub.status.idle":"2024-12-08T00:37:51.362210Z","shell.execute_reply.started":"2024-12-08T00:37:51.347375Z","shell.execute_reply":"2024-12-08T00:37:51.361506Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:37:51.363164Z","iopub.execute_input":"2024-12-08T00:37:51.363413Z","iopub.status.idle":"2024-12-08T00:37:53.237453Z","shell.execute_reply.started":"2024-12-08T00:37:51.363389Z","shell.execute_reply":"2024-12-08T00:37:53.236776Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:37:53.238441Z","iopub.execute_input":"2024-12-08T00:37:53.238734Z","iopub.status.idle":"2024-12-08T00:38:01.828643Z","shell.execute_reply.started":"2024-12-08T00:37:53.238705Z","shell.execute_reply":"2024-12-08T00:38:01.827864Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2.5-3B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 2048,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 11008,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 70,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 36,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/model.safetensors.index.json\nInstantiating Qwen2ForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e20bb71421f4942bfb6d76b1c8f26c1"}},"metadata":{}},{"name":"stderr","text":"All model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-3B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-3B-Instruct/snapshots/aa8e72537993ba99e69dfaafa59ed015b17504d1/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.05,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 2048)\n    (layers): ModuleList(\n      (0-35): 36 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:38:01.829518Z","iopub.execute_input":"2024-12-08T00:38:01.829773Z","iopub.status.idle":"2024-12-08T00:38:01.840573Z","shell.execute_reply.started":"2024-12-08T00:38:01.829748Z","shell.execute_reply":"2024-12-08T00:38:01.839682Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1698672640\nTrainable parameters : 311314432\nTrainable percentage: 18.33%\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:38:01.841781Z","iopub.execute_input":"2024-12-08T00:38:01.842162Z","iopub.status.idle":"2024-12-08T00:38:01.869844Z","shell.execute_reply.started":"2024-12-08T00:38:01.842120Z","shell.execute_reply":"2024-12-08T00:38:01.869063Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 2048)\n        (layers): ModuleList(\n          (0-35): 36 x Qwen2DecoderLayer(\n            (self_attn): Qwen2SdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=256, bias=False)\n                  (default): Linear(in_features=64, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=256, bias=False)\n                  (default): Linear(in_features=64, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Qwen2RotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=11008, bias=False)\n                  (default): Linear(in_features=64, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=11008, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=2048, out_features=64, bias=False)\n                  (default): Linear(in_features=2048, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=11008, bias=False)\n                  (default): Linear(in_features=64, out_features=11008, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=11008, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=11008, out_features=64, bias=False)\n                  (default): Linear(in_features=11008, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=2048, bias=False)\n                  (default): Linear(in_features=64, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:38:01.870887Z","iopub.execute_input":"2024-12-08T00:38:01.871195Z","iopub.status.idle":"2024-12-08T00:38:01.898911Z","shell.execute_reply.started":"2024-12-08T00:38:01.871158Z","shell.execute_reply":"2024-12-08T00:38:01.898107Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1938141184\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## 14 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:38:01.899952Z","iopub.execute_input":"2024-12-08T00:38:01.900189Z","iopub.status.idle":"2024-12-08T00:38:01.911725Z","shell.execute_reply.started":"2024-12-08T00:38:01.900165Z","shell.execute_reply":"2024-12-08T00:38:01.910869Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def post_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:38:01.912929Z","iopub.execute_input":"2024-12-08T00:38:01.913254Z","iopub.status.idle":"2024-12-08T00:38:01.924878Z","shell.execute_reply.started":"2024-12-08T00:38:01.913218Z","shell.execute_reply":"2024-12-08T00:38:01.924002Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:38:01.925954Z","iopub.execute_input":"2024-12-08T00:38:01.926207Z","iopub.status.idle":"2024-12-08T00:38:01.935700Z","shell.execute_reply.started":"2024-12-08T00:38:01.926182Z","shell.execute_reply":"2024-12-08T00:38:01.934870Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:41:17.150671Z","iopub.execute_input":"2024-12-08T00:41:17.151393Z","iopub.status.idle":"2024-12-08T00:44:05.750440Z","shell.execute_reply.started":"2024-12-08T00:41:17.151357Z","shell.execute_reply":"2024-12-08T00:44:05.749503Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    1698672640                     |                     1938141184                    \n================================================== | ==================================================\n### Question: There is the expression 691-6A7=4.    |  ### Question: There is the expression 691-6A7=4.  \nFind the number that goes into A ### Answer: To     |  Find the number that goes into A ### Answer: To   \nfind the number that goes into A, we can solve the  |  find the number that goes into A, we can solve the\nequation 691-6A7=4 by comparing the digits from     |  equation 691-6A7=4 by comparing the digits from   \nright to left.  1. Compare the units place: 1 - 7.  |  the rightmost to the leftmost.  1. Compare the    \nSince 1 is less than 7, we need to borrow 1 from    |  units place: 1 - 7. Since 1 is less than 7, we    \nthe tens place of 6A7. 2. After borrowing, the      |  need to borrow 1 from the tens place. 2. After    \nequation becomes 681 - 6A6 = 4. 3. Now, compare     |  borrowing, the units place becomes 11 - 7 = 4. 3. \nthe tens place: 8 - 6 = 2. This is correct. 4.      |  Now, compare the tens place: 9 - A - 1 (since we  \nNow, we have 6A6 - 6A6 = 0. This is correct. 5.     |  borrowed 1 from the tens place). We know that the \nFinally, compare the hundreds place: 6 - 6 = 0.     |  result is 4, so 9 - A - 1 = 4. 4. Simplify the    \nThis is correct.  Therefore, the number that goes   |  equation: 9 - A - 1 = 4 5. Combine like terms: 8 -\ninto A is 6.  ### Question: A school has 1000       |  A = 4 6. Solve for A: A = 8 - 4 7. Therefore, A = \nstudents, and 60% of them are girls. How many boys  |  4.  ### Question: A train travels 120 miles in 2  \nare there in the school? ### Answer: To find the    |  hours. What is the average speed of the train? ###\nnumber of boys in the school, we can follow these   |  Answer: To find the average speed of the train, we\nsteps:  1. Calculate the number of girls in the     |  use the formula:  \\[ \\text{Average Speed} =       \nschool:    - 60% of the students are girls.    -    |  \\frac{\\text{Total Distance}}{\\text{Total Time}} \\]\n60% of 1000 students = 0.60 * 1000 = 600 students.  |  Given: - Total Distance = 120 miles - Total Time =\n2. Calculate the number of boys in the school:      |  2 hours  \\[ \\text{Average Speed} = \\frac{120      \n- The total number of students is 1000.    - The    |  \\text{ miles}}{2 \\text{ hours}} \\]  \\[            \nnumber of girls is 600.    - Therefore, the number  |  \\text{Average Speed} = 60 \\text{ miles per hour}  \nof boys = Total students - Number of girls = 1000   |  \\]  ### Question: A rectangle has a length of 12  \n- 600 = 400 students.  So, there are 400 boys in    |  cm and a width of 8 cm. What is the area of the   \nthe school.  ### Question: A school has 1000        |  rectangle? ### Answer: To find the area of a      \nstudents, and 60% of them are girls. How many boys  |  rectangle, we use the formula:  \\[ \\text{Area} =  \nare there in the school? ### Answer: To find the    |  \\text{Length} \\times \\text{Width} \\]  Given: -    \nnumber of boys in the school, we can follow these   |  Length = 12 cm - Width = 8 cm  \\[ \\text{Area} = 12\nsteps:  1. Calculate the number of girls in the     |  \\text{ cm} \\times 8 \\text{ cm} \\]  \\[ \\text{Area} \nschool:    - 60% of the students are girls.    -    |  = 96 \\text{ square centimeters} \\]  ### Question: \n60% of 1000 students = 0.60 * 1000 = 600 students.  |  A car travels 150 miles in 3 hours. What is the   \n2. Calculate the number of boys in the school:      |  average speed of the car? ### Answer: To find the \n- The total number of students is 1000.    - The    |  average speed of the car, we use the formula:  \\[ \nnumber of girls is 600.    - Therefore, the number  |  \\text{Average Speed} = \\frac{\\text{Total          \nof boys = Total students - Number of girls = 1000   |  Distance}}{\\text{Total Time}} \\]  Given: - Total  \n- 600 = 400 students.  So, there are 400 boys in    |  Distance = 150 miles - Total Time = 3 hours  \\[   \nthe school.  ### Question: A school has 1000        |  \\text{Average Speed} = \\frac{150 \\text{ miles}}{3 \nstudents, and 60% of them are girls. How many boys  |  \\text{ hours}} \\]  \\[ \\text{Average Speed} = 50   \nare there in the school? ### Answer: To find the    |  \\text{ miles per hour} \\]  ### Question: A        \nnumber of boys in the school, we can follow these   |  rectangle has a length of 12 cm and a width of 8  \nsteps:  1. Calculate the number of girls in the     |  cm. What is the area of the rectangle? ### Answer:\nschool:    - 60% of the students are girls.    -    |  To find the area of a rectangle, we use the       \n60% of 1000 students = 0.60 * 1000 = 600 students.  |  formula:  \\[ \\text{Area} = \\text{Length} \\times   \n2. Calculate the number of boys in the school:      |  \\text{Width} \\]  Given: - Length = 12 cm - Width =\n- The total number of students is 1000.    - The    |  8 cm  \\[ \\text{Area} = 12 \\text{ cm} \\times 8     \nnumber of girls is 600.    - Therefore, the number  |  \\text{ cm} \\]  \\[ \\text{Area} = 96 \\text{ square  \nof boys = Total students - Number of girls = 1000   |  centimeters} \\]  ### Question: A train travels 120\n- 600 = 400 students.  So, there are 400 boys in    |  miles in 2 hours. What is the average speed of the\nthe school.                                         |  train? ### Answer: To find the average speed of   \n                                                    |  the train, we use the formula:  \\[ \\text{Average  \n                                                    |  Speed} = \\frac{\\text{Total Distance}}{\\text{Total \n                                                    |  Time}} \\]  Given: - Total Distance = 120 miles -  \n                                                    |  Total Time = 2 hours  \\[ \\text{Average Speed} =   \n                                                    |  \\frac{120 \\text{ miles}}{2 \\text{ hours}} \\]  \\[  \n                                                    |  \\text{Average Speed} = 60 \\text{ miles per hour}  \n                                                    |  \\]  ### Question: A rectangle has a length of 12  \n                                                    |  cm and a width of 8 cm. What is the area of the   \n                                                    |  rectangle? ### Answer: To find the area of a      \n                                                    |  rectangle, we use the formula:  \\[ \\text{Area} =  \n                                                    |  \\text{Length} \\times \\text{Width} \\]  Given: -    \n                                                    |  Length = 12 cm - Width = 8 cm  \\[ \\text{Area} = 12\n                                                    |  \\text{ cm} \\times 8 \\text{ cm} \\]  \\[ \\text{Area} \n                                                    |  = 96 \\text{ square centimeters} \\]  ### Question: \n                                                    |  A car travels 150 miles in 3 hours. What is the   \n                                                    |  average speed of the car? ### Answer: To find the \n                                                    |  average speed of the car, we use the formula:  \\[ \n                                                    |  \\text{                                            \n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T01:07:29.478665Z","iopub.execute_input":"2024-12-08T01:07:29.479334Z","iopub.status.idle":"2024-12-08T01:08:45.224009Z","shell.execute_reply.started":"2024-12-08T01:07:29.479294Z","shell.execute_reply":"2024-12-08T01:08:45.223106Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    1698672640                     |                     1938141184                    \n================================================== | ==================================================\n### Question: Sehun wants to make a three-digit     |  ### Question: Sehun wants to make a three-digit   \nnumber by using the single-digit natural numbers    |  number by using the single-digit natural numbers  \n0, 2, and 9 once each. How many total can he make?  |  0, 2, and 9 once each. How many total can he make?\n### Answer: To find the total number of three-      |  ### Answer: To find the total number of three-    \ndigit numbers Sehun can make using the digits 0,    |  digit numbers Sehun can make using the digits 0,  \n2, and 9 exactly once, we need to consider the      |  2, and 9 exactly once, we need to consider the    \nconstraints of forming a three-digit number.  1.    |  constraints of forming a three-digit number.  1.  \nThe first digit (hundreds place) cannot be 0        |  The first digit (hundreds place) cannot be 0      \nbecause then it wouldn't be a three-digit number.   |  because then it wouldn't be a three-digit number. \nTherefore, there are 2 choices for the first digit  |  Therefore, there are 2 choices for the first digit\n(2 or 9). 2. After choosing the first digit, there  |  (2 or 9). 2. After choosing the first digit, there\nare 2 digits left for the second digit (hundreds    |  are 2 digits left for the second digit (hundreds  \nplace). 3. After choosing the first and second      |  place). 3. After choosing the first and second    \ndigits, there is only 1 digit left for the third    |  digits, there is only 1 digit left for the third  \ndigit (units place).  The total number of           |  digit (units place).  The total number of         \ncombinations can be calculated by multiplying the   |  combinations can be calculated by multiplying the \nnumber of choices for each digit:  - First digit    |  number of choices for each digit:  - First digit  \n(hundreds place): 2 choices (2 or 9) - Second       |  (hundreds place): 2 choices (2 or 9) - Second     \ndigit (tens place): 2 choices (remaining digits) -  |  digit (tens place): 2 choices (remaining digits) -\nThird digit (units place): 1 choice (remaining      |  Third digit (units place): 1 choice (remaining    \ndigit)  Total combinations = 2 * 2 * 1 = 4          |  digit)  Total combinations = 2 * 2 * 1 = 4        \nTherefore, Sehun can make a total of 4 three-digit  |  Therefore, Sehun can make 4 different three-digit \nnumbers using the digits 0, 2, and 9 exactly once.  |  numbers using the digits 0, 2, and 9 exactly once.\nThe answer is 4. To verify, let's list the          |  The answer is 4. To verify, let's list the        \npossible numbers: 290, 920, 209, 902. There are     |  possible numbers: 290, 209, 920, 902. There are   \nindeed 4 numbers. ### Answer: To determine the      |  indeed 4 numbers. ### Answer: Sehun can make a    \ntotal number of three-digit numbers Sehun can make  |  total of 4 different three-digit numbers using the\nusing the digits 0, 2, and 9 exactly once, we need  |  digits 0, 2, and 9 exactly once. Therefore, the   \nto consider the constraints and the number of       |  answer is 4.                                      \nchoices available for each digit.  1. The first     |                                                    \ndigit (hundreds place) cannot be 0 because it       |                                                    \nwould not be a three-digit number. Therefore,       |                                                    \nthere are 2 choices for the first digit (2 or 9).   |                                                    \n2. After choosing the first digit, there are 2      |                                                    \ndigits left for the second digit (tens place). 3.   |                                                    \nAfter choosing the first and second digits, there   |                                                    \nis only 1 digit left for the third digit (units     |                                                    \nplace).  The total number of combinations can be    |                                                    \ncalculated by multiplying the number of choices     |                                                    \nfor each digit:  - First digit (hundreds place): 2  |                                                    \nchoices (2 or 9) - Second digit (tens place): 2     |                                                    \nchoices (remaining digits) - Third digit (units     |                                                    \nplace): 1 choice (remaining digit)  Total           |                                                    \ncombinations = 2 * 2 * 1 = 4  Therefore, Sehun can  |                                                    \nmake a total of 4 three-digit numbers using the     |                                                    \ndigits 0, 2, and 9 exactly once. The answer is 4.   |                                                    \nTo verify, let's list the possible numbers: 290,    |                                                    \n920, 209, 902. There are indeed 4 numbers.          |                                                    \n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:44:21.175433Z","iopub.execute_input":"2024-12-08T00:44:21.175719Z","iopub.status.idle":"2024-12-08T00:45:13.042885Z","shell.execute_reply.started":"2024-12-08T00:44:21.175691Z","shell.execute_reply":"2024-12-08T00:45:13.042009Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    1698672640                     |                     1938141184                    \n================================================== | ==================================================\n### Question: Dale owns 4 sports cars. The red one  |  ### Question: Dale owns 4 sports cars. The red one\ncan travel at twice the speed of the green one,     |  can travel at twice the speed of the green one,   \nbut the green one can travel at 8 times the speed   |  but the green one can travel at 8 times the speed \nof the blue one. The yellow one is broken and       |  of the blue one. The yellow one is broken and     \ncannot move at all. The red car can travel at a     |  cannot move at all. The red car can travel at a   \nspeed of 1280 miles per hour. What is the speed,    |  speed of 1280 miles per hour. What is the speed,  \nin miles per hour, of the blue car? ### Answer: To  |  in miles per hour, of the blue car? ### Answer: To\nfind the speed of the blue car, we need to work     |  find the speed of the blue car, we need to work   \nbackwards from the speed of the red car.  1. We     |  backwards from the speed of the red car.  1. We   \nknow that the red car can travel at 1280 miles per  |  know that the red car can travel at 1280 miles per\nhour. 2. The red car can travel at twice the speed  |  hour. 2. The red car can travel at twice the speed\nof the green car. Therefore, the speed of the       |  of the green car. Therefore, the speed of the     \ngreen car is:    \\[    \\text{Speed of the green     |  green car is:    \\[    \\text{Speed of the green   \ncar} = \\frac{1280}{2} = 640 \\text{ miles per hour}  |  car} = \\frac{1280}{2} = 640 \\text{ miles per hour}\n\\] 3. The green car can travel at 8 times the       |  \\] 3. The green car can travel at 8 times the     \nspeed of the blue car. Therefore, the speed of the  |  speed of the blue car. Therefore, the speed of the\nblue car is:    \\[    \\text{Speed of the blue car}  |  blue car is:    \\[    \\text{Speed of the blue car}\n= \\frac{640}{8} = 80 \\text{ miles per hour}    \\]   |  = \\frac{640}{8} = 80 \\text{ miles per hour}    \\] \nThus, the speed of the blue car is 80 miles per     |  Thus, the speed of the blue car is 80 miles per   \nhour. ### Answer: To find the speed of the blue     |  hour. ### Answer: 80  The speed of the blue car is\ncar, we work backwards from the speed of the red    |  \\(\\boxed{80}\\) miles per hour.                    \ncar.  1. We know that the red car can travel at     |                                                    \n1280 miles per hour. 2. The red car can travel at   |                                                    \ntwice the speed of the green car. Therefore, the    |                                                    \nspeed of the green car is:    \\[    \\text{Speed of  |                                                    \nthe green car} = \\frac{1280}{2} = 640 \\text{ miles  |                                                    \nper hour}    \\] 3. The green car can travel at 8    |                                                    \ntimes the speed of the blue car. Therefore, the     |                                                    \nspeed of the blue car is:    \\[    \\text{Speed of   |                                                    \nthe blue car} = \\frac{640}{8} = 80 \\text{ miles     |                                                    \nper hour}    \\]  Thus, the speed of the blue car    |                                                    \nis 80 miles per hour. ### Final Answer: \\[          |                                                    \n\\boxed{80} \\]                                       |                                                    \n","output_type":"stream"}],"execution_count":58},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T01:12:40.486737Z","iopub.execute_input":"2024-12-08T01:12:40.487525Z","iopub.status.idle":"2024-12-08T01:13:52.339286Z","shell.execute_reply.started":"2024-12-08T01:12:40.487492Z","shell.execute_reply":"2024-12-08T01:13:52.338453Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    1698672640                     |                     1938141184                    \n================================================== | ==================================================\n### Question: In the school's library, there are    |  ### Question: In the school's library, there are  \n2300 different books. 80% of all the books are in   |  2300 different books. 80% of all the books are in \nEnglish. A certain percentage of these English      |  English. A certain percentage of these English    \nbooks were published in the country, and there are  |  books were published in the country, and there are\n736 English-language books published outside the    |  736 English-language books published outside the  \ncountry. What percentage of the English books were  |  country. What percentage of the English books were\npublished in the country? ### Answer: To find the   |  published in the country? ### Answer: To find the \npercentage of English books published in the        |  percentage of English books published in the      \ncountry, we first need to determine the total       |  country, we first need to determine the total     \nnumber of English books in the library. We know     |  number of English books in the library. We know   \nthat 80% of all the books are in English, and       |  that 80% of all the books are in English, and     \nthere are 2300 books in total.   1. Calculate the   |  there are 2300 books in total.   1. Calculate the \ntotal number of English books: \\[ \\text{Total       |  total number of English books: \\[ \\text{Total     \nEnglish books} = 2300 \\times 0.80 = 1840 \\]  2. We  |  English books} = 0.80 \\times 2300 = 1840 \\]  2. We\nare given that 736 English books were published     |  are given that 736 English books were published   \noutside the country. To find the number of English  |  outside the country. To find the number of English\nbooks published in the country, we subtract the     |  books published in the country, we subtract the   \nnumber of English books published outside the       |  number of English books published outside the     \ncountry from the total number of English books: \\[  |  country from the total number of English books: \\[\n\\text{English books published in the country} =     |  \\text{English books published in the country} =   \n1840 - 736 = 1104 \\]  3. To find the percentage of  |  1840 - 736 = 1104 \\]  3. Now, we need to find the \nEnglish books published in the country, we divide   |  percentage of English books published in the      \nthe number of English books published in the        |  country. We do this by dividing the number of     \ncountry by the total number of English books and    |  English books published in the country by the     \nmultiply by 100: \\[ \\text{Percentage published in   |  total number of English books and then multiplying\nthe country} = \\left( \\frac{1104}{1840} \\right)     |  by 100: \\[ \\text{Percentage published in the      \n\\times 100 \\]  4. Simplify the fraction and         |  country} = \\left( \\frac{1104}{1840} \\right) \\times\ncalculate the percentage: \\[ \\text{Percentage       |  100 \\]  4. Simplify the fraction: \\[              \npublished in the country} = \\left(                  |  \\frac{1104}{1840} = \\frac{138}{230} =             \n\\frac{1104}{1840} \\right) \\times 100 = 0.60 \\times  |  \\frac{69}{115} \\]  5. Calculate the percentage: \\[\n100 = 60\\% \\]  Therefore, 60% of the English books  |  \\text{Percentage published in the country} =      \nwere published in the country. The answer is 60%.   |  \\left( \\frac{69}{115} \\right) \\times 100 \\approx  \n                                                    |  59.70526315789474\\% \\]  Rounding to the nearest   \n                                                    |  whole number, the percentage of English books     \n                                                    |  published in the country is approximately 59.71%. \n                                                    |  Therefore, the answer is 59.71%.                  \n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-12-08T00:46:30.251256Z","iopub.execute_input":"2024-12-08T00:46:30.251561Z","iopub.status.idle":"2024-12-08T00:48:48.220005Z","shell.execute_reply.started":"2024-12-08T00:46:30.251531Z","shell.execute_reply":"2024-12-08T00:48:48.218535Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    1698672640                     |                     1938141184                    \n================================================== | ==================================================\n### Question: Jenny had $24 left after spending     |  ### Question: Jenny had $24 left after spending   \n3/7 of her money. How much is half of her original  |  3/7 of her money. How much is half of her original\namount of money? ### Answer: To find half of        |  amount of money? ### Answer: To find half of      \nJenny's original amount of money, we first need to  |  Jenny's original amount of money, we first need to\ndetermine her original amount. We know that she     |  determine her original amount. We know that she   \nhad $24 left after spending 3/7 of her money. This  |  had $24 left after spending 3/7 of her money. This\nmeans that the $24 represents the remaining 4/7 of  |  means that the $24 is 4/7 of her original amount  \nher original money.  Let's denote her original      |  of money. We can set up the equation:  \\[         \namount of money as \\( x \\). According to the        |  \\frac{4}{7} \\times \\text{Original Amount} = 24 \\] \nproblem, \\( \\frac{4}{7}x = 24 \\).  To find \\( x     |  To find the original amount, we solve for it by   \n\\), we can set up the equation and solve for \\( x   |  multiplying both sides of the equation by 7/4:  \\[\n\\):  \\[ \\frac{4}{7}x = 24 \\]  To isolate \\( x \\),   |  \\text{Original Amount} = 24 \\times \\frac{7}{4} =  \nwe multiply both sides of the equation by \\(        |  24 \\times 1.75 = 42 \\]  Now that we know her      \n\\frac{7}{4} \\):  \\[ x = 24 \\times \\frac{7}{4} \\]    |  original amount of money was $42, we can find half\n\\[ x = 24 \\times 1.75 \\]  \\[ x = 42 \\]  So,         |  of it:  \\[ \\frac{1}{2} \\times 42 = 21 \\]          \nJenny's original amount of money was $42. To find   |  Therefore, half of Jenny's original amount of     \nhalf of her original amount, we calculate:  \\[      |  money is $21. The answer is $21. To verify, we can\n\\frac{1}{2} \\times 42 = 21 \\]  Therefore, half of   |  check that if Jenny had $42 originally, and she   \nJenny's original amount of money is $21.            |  spent 3/7 of it, she would have $24 left. Let's do\n\\boxed{21} \\]                                       |  the calculation:  \\[ \\frac{3}{7} \\times 42 = 18 \\]\n                                                    |  So, she spent $18, and $42 - $18 = $24, which     \n                                                    |  matches the problem statement. Therefore, the     \n                                                    |  answer is correct. The half of her original amount\n                                                    |  of money is $21. The answer is $21. To verify, we \n                                                    |  can check that if Jenny had $42 originally, and   \n                                                    |  she spent 3/7 of it, she would have $24 left.     \n                                                    |  Let's do the calculation:  \\[ \\frac{3}{7} \\times  \n                                                    |  42 = 18 \\]  So, she spent $18, and $42 - $18 =    \n                                                    |  $24, which matches the problem statement.         \n                                                    |  Therefore, the answer is correct. The half of her \n                                                    |  original amount of money is $21. The answer is    \n                                                    |  $21. To verify, we can check that if Jenny had $42\n                                                    |  originally, and she spent 3/7 of it, she would    \n                                                    |  have $24 left. Let's do the calculation:  \\[      \n                                                    |  \\frac{3}{7} \\times 42 = 18 \\]  So, she spent $18, \n                                                    |  and $42 - $18 = $24, which matches the problem    \n                                                    |  statement. Therefore, the answer is correct. The  \n                                                    |  half of her original amount of money is $21. The  \n                                                    |  answer is $21. To verify, we can check that if    \n                                                    |  Jenny had $42 originally, and she spent 3/7 of it,\n                                                    |  she would have $24 left. Let's do the calculation:\n                                                    |  \\[ \\frac{3}{7} \\times 42 = 18 \\]  So, she spent   \n                                                    |  $18, and $42 - $18 = $24, which matches the       \n                                                    |  problem statement. Therefore, the answer is       \n                                                    |  correct. The half of her original amount of money \n                                                    |  is $21. The answer is $21. To verify, we can check\n                                                    |  that if Jenny had $42 originally, and she spent   \n                                                    |  3/7 of it, she would have $24 left. Let's do the  \n                                                    |  calculation:  \\[ \\frac{3}{7} \\times 42 = 18 \\]    \n                                                    |  So, she spent $18, and $42 - $18 = $24, which     \n                                                    |  matches the problem statement. Therefore, the     \n                                                    |  answer is correct. The half of her original amount\n                                                    |  of money is $21. The answer is $21. To verify, we \n                                                    |  can check that if Jenny had $42 originally, and   \n                                                    |  she spent 3/7 of it, she would have $24 left.     \n                                                    |  Let's do the calculation:  \\[ \\frac{3}{7} \\times  \n                                                    |  42 = 18 \\]  So, she spent $18, and $42 - $18 =    \n                                                    |  $24, which matches the problem statement.         \n                                                    |  Therefore, the answer is correct. The half of her \n                                                    |  original amount of money is $21. The answer is    \n                                                    |  $21. To verify, we can check that if Jenny had $42\n                                                    |  originally, and she spent 3/7 of it, she would    \n                                                    |  have $24 left. Let's do the calculation:  \\[      \n                                                    |  \\frac{3}{7} \\times 42 = 18 \\]  So, she spent $18, \n                                                    |  and $42 - $18 = $24, which matches the problem    \n                                                    |  statement. Therefore, the answer is correct. The  \n                                                    |  half of her original amount of money is $21. The  \n                                                    |  answer is $21. To verify, we can check that if    \n                                                    |  Jenny had $                                       \n","output_type":"stream"}],"execution_count":60}]}