{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "GPU is available!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, pipeline, GPT2Config, TextDataset\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import statistics\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from transformers import TrainingArguments, Trainer, set_seed, EvalPrediction, DataCollatorWithPadding\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModel, AutoTokenizer, TFAutoModel, AutoModelForSequenceClassification\n",
    "from peft import LoraConfig, PeftModelForSequenceClassification, TaskType, AutoPeftModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_PATH = str(pathlib.Path().resolve())\n",
    "DATASET_PATH = MAIN_PATH + '\\\\datasets'\n",
    "MODEL_PATH = MAIN_PATH + '\\\\models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bert-base-cased',\n",
       " 'bert-base-multilingual-cased',\n",
       " 'bert-base-uncased',\n",
       " 'bert-large-cased',\n",
       " 'bert-large-uncased',\n",
       " 'flan-t5-base',\n",
       " 'flan-t5-large',\n",
       " 'flan-t5-small',\n",
       " 'gpt2',\n",
       " 'gpt2-large',\n",
       " 'gpt2-medium']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = os.listdir(MODEL_PATH)\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python\\\\LLM_Environment\\\\models\\\\gpt2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = MODEL_PATH + '\\\\' + models[8]\n",
    "model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cached_lm_GPT2Tokenizer_128_Shakespeare_Dataset.txt',\n",
       " 'Customer.csv',\n",
       " 'Html.csv',\n",
       " 'Recipes.csv',\n",
       " 'Recipes_1000.csv',\n",
       " 'Shakespeare_Dataset.txt',\n",
       " 'Taylor_Swift_Lyrics.csv',\n",
       " 'Twitter.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = os.listdir(DATASET_PATH)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Python\\\\LLM_Environment\\\\datasets\\\\Twitter.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = DATASET_PATH + '\\\\' + filenames[7]\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>emoji</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>bet you'll get hungry</td>\n",
       "      <td>heart_eyes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>starbucks employee confuses boyfriend by sayin...</td>\n",
       "      <td>yum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>when your starbucks store makes you an iced mo...</td>\n",
       "      <td>sob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>being told \"girl your romper looks fierce!\" at...</td>\n",
       "      <td>blush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>i got a starbucks drink at school today, shit ...</td>\n",
       "      <td>sob</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet       emoji\n",
       "0           0                              bet you'll get hungry  heart_eyes\n",
       "1           1  starbucks employee confuses boyfriend by sayin...         yum\n",
       "2           2  when your starbucks store makes you an iced mo...         sob\n",
       "3           3  being told \"girl your romper looks fierce!\" at...       blush\n",
       "4           4  i got a starbucks drink at school today, shit ...         sob"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the emoji labels into numerical format\n",
    "unique_emojis = df['emoji'].unique()\n",
    "emoji2id = {emoji: id for id, emoji in enumerate(unique_emojis)}\n",
    "id2emoji = {id: emoji for emoji, id in emoji2id.items()}\n",
    "\n",
    "# Add a new column for the encoded labels\n",
    "df['label'] = df['emoji'].map(emoji2id)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, stratify=df['label'])\n",
    "\n",
    "# Convert the dataframes into Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "val_dataset = Dataset.from_pandas(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 202797/202797 [00:33<00:00, 6119.71 examples/s]\n",
      "Map: 100%|██████████| 22534/22534 [00:03<00:00, 5775.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize and convert\n",
    "def tokenize_and_encode(examples):\n",
    "    tokenized_inputs = tokenizer(examples['tweet'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    tokenized_inputs['labels'] = examples['label']\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_and_encode, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_and_encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'tweet', 'emoji', 'label', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 202797\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics function\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    return {\"accuracy\": accuracy_score(p.label_ids, preds), \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at D:\\Python\\LLM_Environment\\models\\gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(unique_emojis))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "save_path = './model'\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    warmup_ratio=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer with compute_metrics\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset.select(range(10000)),\n",
    "    eval_dataset=val_dataset.select(range(1000)),\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1250 [00:00<?, ?it/s]d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:545: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  8%|▊         | 100/1250 [00:44<08:24,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.7605, 'grad_norm': 34.516395568847656, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 200/1250 [01:28<07:49,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3396, 'grad_norm': 11.90102767944336, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 300/1250 [02:13<07:08,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.2224, 'grad_norm': 8.523725509643555, 'learning_rate': 1.688888888888889e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 400/1250 [02:59<06:32,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1284, 'grad_norm': 7.627568244934082, 'learning_rate': 1.5111111111111112e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 500/1250 [03:46<05:56,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1081, 'grad_norm': 10.487789154052734, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 600/1250 [04:34<05:09,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.128, 'grad_norm': 8.383912086486816, 'learning_rate': 1.1555555555555556e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 700/1250 [05:23<04:30,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0693, 'grad_norm': 7.677370548248291, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 800/1250 [06:12<03:38,  2.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0753, 'grad_norm': 8.724266052246094, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 900/1250 [07:01<02:51,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0191, 'grad_norm': 10.59865951538086, 'learning_rate': 6.222222222222223e-06, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1000/1250 [07:50<02:02,  2.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9927, 'grad_norm': 10.749558448791504, 'learning_rate': 4.444444444444444e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1100/1250 [08:39<01:14,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9915, 'grad_norm': 12.196487426757812, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1200/1250 [09:28<00:24,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0317, 'grad_norm': 8.614356994628906, 'learning_rate': 8.88888888888889e-07, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [09:53<00:00,  2.00it/s]d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "\n",
      "100%|██████████| 1250/1250 [10:14<00:00,  2.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.004523277282715, 'eval_accuracy': 0.301, 'eval_f1': 0.2562244446522303, 'eval_precision': 0.34590116264367254, 'eval_recall': 0.301, 'eval_runtime': 19.6195, 'eval_samples_per_second': 50.97, 'eval_steps_per_second': 6.371, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [10:16<00:00,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 617.0898, 'train_samples_per_second': 16.205, 'train_steps_per_second': 2.026, 'train_loss': 2.468174346923828, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=2.468174346923828, metrics={'train_runtime': 617.0898, 'train_samples_per_second': 16.205, 'train_steps_per_second': 2.026, 'total_flos': 2613156249600000.0, 'train_loss': 2.468174346923828, 'epoch': 1.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEFT model configuration\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=4,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at D:\\Python\\LLM_Environment\\models\\gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(unique_emojis))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 155,136 || all params: 124,602,624 || trainable%: 0.1245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\peft\\tuners\\lora\\layer.py:1150: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_model = PeftModelForSequenceClassification(model, peft_config)\n",
    "\n",
    "# Print\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics function\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
    "    return {\"accuracy\": accuracy_score(p.label_ids, preds), \"f1\": f1, \"precision\": precision, \"recall\": recall}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\transformers\\training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "peft_path = './peft_model'\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=peft_path,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs/peft_model',\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    "    warmup_ratio=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Trainer with compute_metrics\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset.select(range(10000)),\n",
    "    eval_dataset=val_dataset.select(range(1000)),\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 100/1250 [00:38<07:30,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.7796, 'grad_norm': 197.31263732910156, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 200/1250 [01:17<06:50,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.3495, 'grad_norm': 146.02621459960938, 'learning_rate': 1.866666666666667e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 300/1250 [01:56<06:15,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 6.3327, 'grad_norm': 138.65345764160156, 'learning_rate': 1.688888888888889e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 400/1250 [02:35<05:27,  2.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.8677, 'grad_norm': 37.408973693847656, 'learning_rate': 1.5111111111111112e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 500/1250 [03:14<04:52,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9151, 'grad_norm': 70.88046264648438, 'learning_rate': 1.3333333333333333e-05, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 600/1250 [03:54<04:18,  2.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6592, 'grad_norm': 23.853919982910156, 'learning_rate': 1.1555555555555556e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 700/1250 [04:33<03:35,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6112, 'grad_norm': 25.212936401367188, 'learning_rate': 9.777777777777779e-06, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 800/1250 [05:12<02:56,  2.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5161, 'grad_norm': 19.571550369262695, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 900/1250 [05:52<02:17,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5713, 'grad_norm': 27.05467987060547, 'learning_rate': 6.222222222222223e-06, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 1000/1250 [06:31<01:38,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.479, 'grad_norm': 37.3812370300293, 'learning_rate': 4.444444444444444e-06, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 1100/1250 [07:10<00:59,  2.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4262, 'grad_norm': 20.682153701782227, 'learning_rate': 2.666666666666667e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 1200/1250 [07:50<00:19,  2.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4537, 'grad_norm': 19.646015167236328, 'learning_rate': 8.88888888888889e-07, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1250/1250 [08:09<00:00,  2.57it/s]d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "                                                   \n",
      "100%|██████████| 1250/1250 [08:31<00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.6149892807006836, 'eval_accuracy': 0.15, 'eval_f1': 0.1148174003077631, 'eval_precision': 0.1205855721157312, 'eval_recall': 0.15, 'eval_runtime': 21.0982, 'eval_samples_per_second': 47.397, 'eval_steps_per_second': 5.925, 'epoch': 1.0}\n",
      "{'train_runtime': 511.5661, 'train_samples_per_second': 19.548, 'train_steps_per_second': 2.443, 'train_loss': 4.01282431640625, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=4.01282431640625, metrics={'train_runtime': 511.5661, 'train_samples_per_second': 19.548, 'train_steps_per_second': 2.443, 'total_flos': 2617922027520000.0, 'train_loss': 4.01282431640625, 'epoch': 1.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:20<00:00,  5.97it/s]d:\\Python\\LLM_Environment\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "100%|██████████| 125/125 [00:20<00:00,  6.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 2.6149892807006836, 'eval_accuracy': 0.15, 'eval_f1': 0.1148174003077631, 'eval_precision': 0.1205855721157312, 'eval_recall': 0.15, 'eval_runtime': 20.8651, 'eval_samples_per_second': 47.927, 'eval_steps_per_second': 5.991, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#peft_model.save_pretrained(peft_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at D:\\Python\\LLM_Environment\\models\\gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "inference_model = AutoPeftModelForSequenceClassification.from_pretrained(\n",
    "    peft_path + '/checkpoint-1250',\n",
    "    num_labels=len(unique_emojis)\n",
    ")\n",
    "inference_model.config.pad_token_id = inference_model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=inference_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2817/2817 [07:01<00:00,  6.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 2.542323589324951, 'eval_model_preparation_time': 0.002, 'eval_accuracy': 0.1676133842194018, 'eval_f1': 0.12954224179975385, 'eval_precision': 0.1325410241609454, 'eval_recall': 0.1676133842194018, 'eval_runtime': 421.8097, 'eval_samples_per_second': 53.422, 'eval_steps_per_second': 6.678}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluation_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", evaluation_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sentence: str) -> str:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inference_model.to(device)\n",
    "\n",
    "    # Prepare the input text\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = inference_model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    predicted_class_id = probabilities.argmax().item()\n",
    "    predicted_label = id2emoji[predicted_class_id]\n",
    "\n",
    "    return predicted_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'I'm sad and i wanna cry'\n",
      "Predicted label: sob\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "sentence = \"I'm sad and i wanna cry\"\n",
    "predicted_label = predict(sentence)\n",
    "print(f\"Sentence: '{sentence}'\\nPredicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'That was delicious!!'\n",
      "Predicted label: flushed\n"
     ]
    }
   ],
   "source": [
    "sentence = \"That was delicious!!\"\n",
    "predicted_label = predict(sentence)\n",
    "print(f\"Sentence: '{sentence}'\\nPredicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'it was yummy'\n",
      "Predicted label: sob\n"
     ]
    }
   ],
   "source": [
    "sentence = \"it was yummy\"\n",
    "predicted_label = predict(sentence)\n",
    "print(f\"Sentence: '{sentence}'\\nPredicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'I love you!'\n",
      "Predicted label: yum\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love you!\"\n",
    "predicted_label = predict(sentence)\n",
    "print(f\"Sentence: '{sentence}'\\nPredicted label: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i bet you know the feeling. this skirt just makes me happy! also, it's a pretty cool th\n",
      "label:  relaxed\n",
      "prediction: smirk\n",
      "\n",
      "way ahead of you\n",
      "label:  heart_eyes\n",
      "prediction: yum\n",
      "\n",
      "ight bet pooh\n",
      "label:  yum\n",
      "prediction: heart_eyes\n",
      "\n",
      "working 30 hours next week is a bet\n",
      "label:  heart_eyes\n",
      "prediction: smirk\n",
      "\n",
      "i thought i'd try this. bet i won't get more than... hmm.. 30?.. let's see\n",
      "label:  wink\n",
      "prediction: grin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indices_for_review = [0, 1, 2, 3, 4]\n",
    "\n",
    "for idx in indices_for_review:\n",
    "    item = val_dataset[idx]\n",
    "\n",
    "\n",
    "    print(item['tweet'][:100])\n",
    "    actual_label_id = item['label']\n",
    "    actual_label = id2emoji[actual_label_id]\n",
    "    print(f'label:  {actual_label}')\n",
    "\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(item['tweet'], return_tensors=\"pt\").to(inference_model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = inference_model(**inputs).logits\n",
    "\n",
    "    predictions = torch.argmax(logits, dim=1).item()\n",
    "    predicted_label = id2emoji[predictions]\n",
    "    print(f'prediction: {predicted_label}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
