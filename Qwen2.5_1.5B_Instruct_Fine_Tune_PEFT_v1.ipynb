{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"#!pip install --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-11-17T03:50:58.664077Z","iopub.execute_input":"2024-11-17T03:50:58.665087Z","iopub.status.idle":"2024-11-17T03:51:45.891940Z","shell.execute_reply.started":"2024-11-17T03:50:58.664988Z","shell.execute_reply":"2024-11-17T03:51:45.890730Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-11-17T03:51:45.893981Z","iopub.execute_input":"2024-11-17T03:51:45.894299Z","iopub.status.idle":"2024-11-17T03:51:54.260966Z","shell.execute_reply.started":"2024-11-17T03:51:45.894266Z","shell.execute_reply":"2024-11-17T03:51:54.260002Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-11-17T03:51:54.262266Z","iopub.execute_input":"2024-11-17T03:51:54.262591Z","iopub.status.idle":"2024-11-17T03:51:54.270152Z","shell.execute_reply.started":"2024-11-17T03:51:54.262556Z","shell.execute_reply":"2024-11-17T03:51:54.269272Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"#url = 'https://huggingface.co/Qwen/Qwen2.5-0.5B'\n#model_name = url.split('.co/')[-1]\n\nmodel_name = 'Qwen/Qwen2.5-1.5B-Instruct'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-11-17T03:51:54.272561Z","iopub.execute_input":"2024-11-17T03:51:54.272986Z","iopub.status.idle":"2024-11-17T03:51:54.279351Z","shell.execute_reply.started":"2024-11-17T03:51:54.272938Z","shell.execute_reply":"2024-11-17T03:51:54.278586Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-11-17T03:51:54.280360Z","iopub.execute_input":"2024-11-17T03:51:54.280629Z","iopub.status.idle":"2024-11-17T03:51:54.289852Z","shell.execute_reply.started":"2024-11-17T03:51:54.280598Z","shell.execute_reply":"2024-11-17T03:51:54.289022Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:51:54.290986Z","iopub.execute_input":"2024-11-17T03:51:54.291820Z","iopub.status.idle":"2024-11-17T03:51:58.562265Z","shell.execute_reply.started":"2024-11-17T03:51:54.291749Z","shell.execute_reply":"2024-11-17T03:51:58.561302Z"}},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 1536)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:51:58.563584Z","iopub.execute_input":"2024-11-17T03:51:58.563938Z","iopub.status.idle":"2024-11-17T03:51:58.574221Z","shell.execute_reply.started":"2024-11-17T03:51:58.563903Z","shell.execute_reply":"2024-11-17T03:51:58.573257Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 888616448\nTrainable parameters : 233461248\nTrainable percentage: 26.27%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:51:58.575781Z","iopub.execute_input":"2024-11-17T03:51:58.576119Z","iopub.status.idle":"2024-11-17T03:51:59.110389Z","shell.execute_reply.started":"2024-11-17T03:51:58.576085Z","shell.execute_reply":"2024-11-17T03:51:59.109138Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"#url = 'https://huggingface.co/datasets/KingNish/reasoning-base-20k'\n#dataset_name = url.split('datasets/')[-1]\n\ndataset_name = 'mlabonne/FineTome-100k'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:51:59.111651Z","iopub.execute_input":"2024-11-17T03:51:59.111986Z","iopub.status.idle":"2024-11-17T03:51:59.116119Z","shell.execute_reply.started":"2024-11-17T03:51:59.111950Z","shell.execute_reply":"2024-11-17T03:51:59.115224Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 1024","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:51:59.120534Z","iopub.execute_input":"2024-11-17T03:51:59.120911Z","iopub.status.idle":"2024-11-17T03:51:59.125222Z","shell.execute_reply.started":"2024-11-17T03:51:59.120879Z","shell.execute_reply":"2024-11-17T03:51:59.124345Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:51:59.126351Z","iopub.execute_input":"2024-11-17T03:51:59.126685Z","iopub.status.idle":"2024-11-17T03:52:00.671569Z","shell.execute_reply.started":"2024-11-17T03:51:59.126652Z","shell.execute_reply":"2024-11-17T03:52:00.670584Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['conversations', 'source', 'score'],\n    num_rows: 100000\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.672641Z","iopub.execute_input":"2024-11-17T03:52:00.672956Z","iopub.status.idle":"2024-11-17T03:52:00.681317Z","shell.execute_reply.started":"2024-11-17T03:52:00.672922Z","shell.execute_reply":"2024-11-17T03:52:00.680417Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.682568Z","iopub.execute_input":"2024-11-17T03:52:00.682943Z","iopub.status.idle":"2024-11-17T03:52:00.712188Z","shell.execute_reply.started":"2024-11-17T03:52:00.682899Z","shell.execute_reply":"2024-11-17T03:52:00.711195Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                       conversations  \\\n0  [{'from': 'human', 'value': 'Explain what bool...   \n1  [{'from': 'human', 'value': 'Explain how recur...   \n2  [{'from': 'human', 'value': 'Explain what bool...   \n3  [{'from': 'human', 'value': 'Explain the conce...   \n4  [{'from': 'human', 'value': 'Print the reverse...   \n\n                     source     score  \n0  infini-instruct-top-500k  5.212621  \n1  infini-instruct-top-500k  5.157649  \n2  infini-instruct-top-500k  5.147540  \n3  infini-instruct-top-500k  5.053656  \n4  infini-instruct-top-500k  5.045648  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>conversations</th>\n      <th>source</th>\n      <th>score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[{'from': 'human', 'value': 'Explain what bool...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.212621</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[{'from': 'human', 'value': 'Explain how recur...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.157649</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[{'from': 'human', 'value': 'Explain what bool...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.147540</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[{'from': 'human', 'value': 'Explain the conce...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.053656</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[{'from': 'human', 'value': 'Print the reverse...</td>\n      <td>infini-instruct-top-500k</td>\n      <td>5.045648</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.713199Z","iopub.execute_input":"2024-11-17T03:52:00.713474Z","iopub.status.idle":"2024-11-17T03:52:00.720119Z","shell.execute_reply.started":"2024-11-17T03:52:00.713442Z","shell.execute_reply":"2024-11-17T03:52:00.719233Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'conversations': [{'from': 'human',\n   'value': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.'},\n  {'from': 'gpt',\n   'value': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.'}],\n 'source': 'infini-instruct-top-500k',\n 'score': 5.212620735168457}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.721458Z","iopub.execute_input":"2024-11-17T03:52:00.721785Z","iopub.status.idle":"2024-11-17T03:52:00.728386Z","shell.execute_reply.started":"2024-11-17T03:52:00.721750Z","shell.execute_reply":"2024-11-17T03:52:00.727458Z"}},"outputs":[{"name":"stdout","text":"['conversations', 'source', 'score']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"def transform_conversations(example):\n  role_map = {\n    'human' : 'user',\n    'gpt' : 'assistant'\n  }\n\n  transformed_conversations = [\n    {\n      'role' : role_map.get(turn['from'], turn['from']),\n      'content' : turn['value']\n    }\n    for turn in example['conversations']\n  ]\n  return {'conversations': transformed_conversations}","metadata":{"id":"0wXJNFBWWNYP","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.729602Z","iopub.execute_input":"2024-11-17T03:52:00.729906Z","iopub.status.idle":"2024-11-17T03:52:00.736489Z","shell.execute_reply.started":"2024-11-17T03:52:00.729862Z","shell.execute_reply":"2024-11-17T03:52:00.735678Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"formatted_dataset = dataset.map(transform_conversations, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.737672Z","iopub.execute_input":"2024-11-17T03:52:00.738058Z","iopub.status.idle":"2024-11-17T03:52:00.752491Z","shell.execute_reply.started":"2024-11-17T03:52:00.737988Z","shell.execute_reply":"2024-11-17T03:52:00.751645Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['conversations'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"print(formatted_dataset[0]['conversations'])","metadata":{"id":"cZya4tPEWUc9","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.753639Z","iopub.execute_input":"2024-11-17T03:52:00.753940Z","iopub.status.idle":"2024-11-17T03:52:00.759529Z","shell.execute_reply.started":"2024-11-17T03:52:00.753907Z","shell.execute_reply":"2024-11-17T03:52:00.758634Z"}},"outputs":[{"name":"stdout","text":"[{'content': 'Explain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \\n\\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\\n\\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.', 'role': 'user'}, {'content': 'Boolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\\n\\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\\n\\n```python\\nx = 5\\ny = 10\\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\\n```\\n\\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\\n\\n```python\\nx = 5\\nresult = not (x > 10)  # This expression evaluates to True\\n```\\n\\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\\n\\n```python\\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\\n```\\n\\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\\n\\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\\n\\n```python\\nif x != 0 and (y / x) > 10:\\n    # Perform some operation\\n```\\n\\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here\\'s an example in pseudocode:\\n\\n```\\nif x != 0 {\\n    if (y / x) > 10 {\\n        // Perform some operation\\n    }\\n}\\n```\\n\\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\\n\\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\\n\\n```python\\nx = 5\\nresult = x  # The value of x is truthy, so result is also truthy\\n```\\n\\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\\n\\n```python\\nx = 5\\nresult = bool(x)  # Explicitly converting x to a boolean value\\n```\\n\\nThis ensures that the result is always a boolean value, regardless of the language\\'s truthiness and falsiness rules.', 'role': 'assistant'}]\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"def format_conversation(example):\n  for entry in example['conversations']:\n    role = entry['role']\n    content = entry['content']\n\n    if role == 'user':\n      formatted_text = f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}\\n<|eot_id|>\"\n    elif role == 'assistant':\n      formatted_text += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{content}\\n<|eot_id|>\"\n\n  return {'prompt': formatted_text}","metadata":{"id":"3bmXGueQWWzt","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.760869Z","iopub.execute_input":"2024-11-17T03:52:00.761263Z","iopub.status.idle":"2024-11-17T03:52:00.768115Z","shell.execute_reply.started":"2024-11-17T03:52:00.761221Z","shell.execute_reply":"2024-11-17T03:52:00.767213Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"formatted_dataset = formatted_dataset.map(\n  format_conversation,\n  remove_columns = list(formatted_dataset.features.keys())\n)\nformatted_dataset","metadata":{"id":"Z6sSaCr5eaL7","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.769107Z","iopub.execute_input":"2024-11-17T03:52:00.769392Z","iopub.status.idle":"2024-11-17T03:52:00.781783Z","shell.execute_reply.started":"2024-11-17T03:52:00.769361Z","shell.execute_reply":"2024-11-17T03:52:00.780860Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"Kidf8H5zefDC","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.782921Z","iopub.execute_input":"2024-11-17T03:52:00.783796Z","iopub.status.idle":"2024-11-17T03:52:00.789149Z","shell.execute_reply.started":"2024-11-17T03:52:00.783745Z","shell.execute_reply":"2024-11-17T03:52:00.788257Z"}},"outputs":[{"name":"stdout","text":"<|start_header_id|>user<|end_header_id|>\n\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \n\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\n\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\n\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\n```\n\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\n```\n\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\n\n```python\nx = 5\nresult = not (x > 10)  # This expression evaluates to True\n```\n\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\n\n```python\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\n```\n\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\n\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\n\n```python\nif x != 0 and (y / x) > 10:\n    # Perform some operation\n```\n\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here's an example in pseudocode:\n\n```\nif x != 0 {\n    if (y / x) > 10 {\n        // Perform some operation\n    }\n}\n```\n\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\n\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\n\n```python\nx = 5\nresult = x  # The value of x is truthy, so result is also truthy\n```\n\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\n\n```python\nx = 5\nresult = bool(x)  # Explicitly converting x to a boolean value\n```\n\nThis ensures that the result is always a boolean value, regardless of the language's truthiness and falsiness rules.\n<|eot_id|>\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.790399Z","iopub.execute_input":"2024-11-17T03:52:00.791040Z","iopub.status.idle":"2024-11-17T03:52:00.796870Z","shell.execute_reply.started":"2024-11-17T03:52:00.790977Z","shell.execute_reply":"2024-11-17T03:52:00.796159Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.798106Z","iopub.execute_input":"2024-11-17T03:52:00.798421Z","iopub.status.idle":"2024-11-17T03:52:00.941080Z","shell.execute_reply.started":"2024-11-17T03:52:00.798386Z","shell.execute_reply":"2024-11-17T03:52:00.940052Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.942597Z","iopub.execute_input":"2024-11-17T03:52:00.942919Z","iopub.status.idle":"2024-11-17T03:52:00.949697Z","shell.execute_reply.started":"2024-11-17T03:52:00.942884Z","shell.execute_reply":"2024-11-17T03:52:00.948768Z"}},"outputs":[{"name":"stdout","text":"<|start_header_id|>user<|end_header_id|>\n\nExplain what boolean operators are, what they do, and provide examples of how they can be used in programming. Additionally, describe the concept of operator precedence and provide examples of how it affects the evaluation of boolean expressions. Discuss the difference between short-circuit evaluation and normal evaluation in boolean expressions and demonstrate their usage in code. \n\nFurthermore, add the requirement that the code must be written in a language that does not support short-circuit evaluation natively, forcing the test taker to implement their own logic for short-circuit evaluation.\n\nFinally, delve into the concept of truthiness and falsiness in programming languages, explaining how it affects the evaluation of boolean expressions. Add the constraint that the test taker must write code that handles cases where truthiness and falsiness are implemented differently across different programming languages.\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nBoolean operators are logical operators used in programming to manipulate boolean values. They operate on one or more boolean operands and return a boolean result. The three main boolean operators are \"AND\" (&&), \"OR\" (||), and \"NOT\" (!).\n\nThe \"AND\" operator returns true if both of its operands are true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) and (y < 20)  # This expression evaluates to True\n```\n\nThe \"OR\" operator returns true if at least one of its operands is true, and false otherwise. For example:\n\n```python\nx = 5\ny = 10\nresult = (x > 0) or (y < 20)  # This expression evaluates to True\n```\n\nThe \"NOT\" operator negates the boolean value of its operand. It returns true if the operand is false, and false if the operand is true. For example:\n\n```python\nx = 5\nresult = not (x > 10)  # This expression evaluates to True\n```\n\nOperator precedence refers to the order in which operators are evaluated in an expression. It ensures that expressions are evaluated correctly. In most programming languages, logical AND has higher precedence than logical OR. For example:\n\n```python\nresult = True or False and False  # This expression is evaluated as (True or (False and False)), which is True\n```\n\nShort-circuit evaluation is a behavior where the second operand of a logical operator is not evaluated if the result can be determined based on the value of the first operand. In short-circuit evaluation, if the first operand of an \"AND\" operator is false, the second operand is not evaluated because the result will always be false. Similarly, if the first operand of an \"OR\" operator is true, the second operand is not evaluated because the result will always be true.\n\nIn programming languages that support short-circuit evaluation natively, you can use it to improve performance or avoid errors. For example:\n\n```python\nif x != 0 and (y / x) > 10:\n    # Perform some operation\n```\n\nIn languages without native short-circuit evaluation, you can implement your own logic to achieve the same behavior. Here's an example in pseudocode:\n\n```\nif x != 0 {\n    if (y / x) > 10 {\n        // Perform some operation\n    }\n}\n```\n\nTruthiness and falsiness refer to how non-boolean values are evaluated in boolean contexts. In many programming languages, non-zero numbers and non-empty strings are considered truthy, while zero, empty strings, and null/None values are considered falsy.\n\nWhen evaluating boolean expressions, truthiness and falsiness come into play. For example:\n\n```python\nx = 5\nresult = x  # The value of x is truthy, so result is also truthy\n```\n\nTo handle cases where truthiness and falsiness are implemented differently across programming languages, you can explicitly check the desired condition. For example:\n\n```python\nx = 5\nresult = bool(x)  # Explicitly converting x to a boolean value\n```\n\nThis ensures that the result is always a boolean value, regardless of the language's truthiness and falsiness rules.\n<|eot_id|>\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.950769Z","iopub.execute_input":"2024-11-17T03:52:00.951062Z","iopub.status.idle":"2024-11-17T03:52:00.966791Z","shell.execute_reply.started":"2024-11-17T03:52:00.951002Z","shell.execute_reply":"2024-11-17T03:52:00.965872Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.967887Z","iopub.execute_input":"2024-11-17T03:52:00.968183Z","iopub.status.idle":"2024-11-17T03:52:00.974087Z","shell.execute_reply.started":"2024-11-17T03:52:00.968151Z","shell.execute_reply":"2024-11-17T03:52:00.973183Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:00.975474Z","iopub.execute_input":"2024-11-17T03:52:00.975847Z","iopub.status.idle":"2024-11-17T03:52:01.003722Z","shell.execute_reply.started":"2024-11-17T03:52:00.975804Z","shell.execute_reply":"2024-11-17T03:52:01.002848Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n1  <|start_header_id|>user<|end_header_id|>\\n\\nWr...   \n2  <|start_header_id|>user<|end_header_id|>\\n\\nWh...   \n3  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n4  <|start_header_id|>user<|end_header_id|>\\n\\nHo...   \n\n                                           input_ids  \\\n0  [27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91,...   \n1  [27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91,...   \n2  [27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91,...   \n3  [27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91,...   \n4  [27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91,...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWr...</td>\n      <td>[27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nWh...</td>\n      <td>[27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\nHo...</td>\n      <td>[27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91,...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:01.010657Z","iopub.execute_input":"2024-11-17T03:52:01.011027Z","iopub.status.idle":"2024-11-17T03:52:01.017147Z","shell.execute_reply.started":"2024-11-17T03:52:01.010982Z","shell.execute_reply":"2024-11-17T03:52:01.016212Z"}},"outputs":[{"name":"stdout","text":"<|start_header_id|>user<|end_header_id|>\n\nHow do you graph the inequality #x > -4#?\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nTo graph the inequality #x > -4#, follow these steps:\n\n1. Start by plotting the boundary line: Draw a vertical line at #x = -4# on the X-axis. This line divides the X-axis into two halves.\n\n2. Identify the direction of the inequality: Since the inequality is #x > -4#, it indicates that we are looking for all values of #x# that are greater than -4.\n\n3. Shade the appropriate region: Shade the entire region to the right of the line at #x = -4#. This shaded area represents all the points whose X-coordinate is greater than -4.\n\n4. Include a dashed line: Since the inequality is \"greater than\" (#>,# not \"#≥#\"), draw the line at #x = -4# as a dashed line, indicating that the points on the line itself are not included in the solution.\n\nThe graph consists of the entire half-plane to the right of the dashed vertical line at #x = -4#, excluding the line itself.\n<|eot_id|>\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:01.018174Z","iopub.execute_input":"2024-11-17T03:52:01.018456Z","iopub.status.idle":"2024-11-17T03:52:01.028744Z","shell.execute_reply.started":"2024-11-17T03:52:01.018416Z","shell.execute_reply":"2024-11-17T03:52:01.027856Z"}},"outputs":[{"name":"stdout","text":"[27, 91, 2468, 8757, 842, 91, 29, 872, 27, 91, 408, 8757, 842, 91, 1339, 4340, 653, 498, 4771, 279, 31205, 671, 87, 861, 481, 19, 2, 5267, 27, 91, 68, 354, 842, 91, 1784, 91, 2468, 8757, 842, 91, 29, 77091, 27, 91, 408, 8757, 842, 91, 1339, 1249, 4771, 279, 31205, 671, 87, 861, 481, 19, 60778, 1795, 1493, 7354, 1447, 16, 13, 5145, 553, 43902, 279, 18732, 1555, 25, 11992, 264, 12140, 1555, 518, 671, 87, 284, 481, 19, 2, 389, 279, 1599, 35321, 13, 1096, 1555, 64828, 279, 1599, 35321, 1119, 1378, 74112, 382, 17, 13, 64547, 279, 5106, 315, 279, 31205, 25, 8704, 279, 31205, 374, 671, 87, 861, 481, 19, 60778, 432, 14807, 429, 582, 525, 3330, 369, 678, 2750, 315, 671, 87, 2, 429, 525, 7046, 1091, 481, 19, 382, 18, 13, 79818, 279, 8311, 5537, 25, 79818, 279, 4453, 5537, 311, 279, 1290, 315, 279, 1555, 518, 671, 87, 284, 481, 19, 87846, 1096, 91766, 3082, 10868, 678, 279, 3501, 6693, 1599, 80697, 374, 7046, 1091, 481, 19, 382, 19, 13, 29734, 264, 66722, 1555, 25, 8704, 279, 31205, 374, 330, 65235, 1091, 1, 29083, 8066, 2, 537, 5869, 144481, 2, 3975, 4038, 279, 1555, 518, 671, 87, 284, 481, 19, 2, 438, 264, 66722, 1555, 11, 18860, 429, 279, 3501, 389, 279, 1555, 5086, 525, 537, 5230, 304, 279, 6291, 382, 785, 4771, 17167, 315, 279, 4453, 4279, 89549, 311, 279, 1290, 315, 279, 66722, 12140, 1555, 518, 671, 87, 284, 481, 19, 60778, 43778, 279, 1555, 5086, 624, 27, 91, 68, 354, 842, 91, 29, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643]\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:01.029807Z","iopub.execute_input":"2024-11-17T03:52:01.030395Z","iopub.status.idle":"2024-11-17T03:52:01.041133Z","shell.execute_reply.started":"2024-11-17T03:52:01.030360Z","shell.execute_reply":"2024-11-17T03:52:01.040075Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:01.042043Z","iopub.execute_input":"2024-11-17T03:52:01.042347Z","iopub.status.idle":"2024-11-17T03:52:01.050541Z","shell.execute_reply.started":"2024-11-17T03:52:01.042298Z","shell.execute_reply":"2024-11-17T03:52:01.049646Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:01.051732Z","iopub.execute_input":"2024-11-17T03:52:01.052124Z","iopub.status.idle":"2024-11-17T03:52:01.060556Z","shell.execute_reply.started":"2024-11-17T03:52:01.052077Z","shell.execute_reply":"2024-11-17T03:52:01.059797Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:01.061593Z","iopub.execute_input":"2024-11-17T03:52:01.061876Z","iopub.status.idle":"2024-11-17T03:52:01.069656Z","shell.execute_reply.started":"2024-11-17T03:52:01.061845Z","shell.execute_reply":"2024-11-17T03:52:01.068899Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:01.070761Z","iopub.execute_input":"2024-11-17T03:52:01.071110Z","iopub.status.idle":"2024-11-17T03:52:01.082136Z","shell.execute_reply.started":"2024-11-17T03:52:01.071068Z","shell.execute_reply":"2024-11-17T03:52:01.081261Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:01.083257Z","iopub.execute_input":"2024-11-17T03:52:01.083630Z","iopub.status.idle":"2024-11-17T03:52:02.147105Z","shell.execute_reply.started":"2024-11-17T03:52:01.083585Z","shell.execute_reply":"2024-11-17T03:52:02.146086Z"}},"outputs":[{"name":"stdout","text":"trainable params: 73,859,072 || all params: 1,617,573,376 || trainable%: 4.5660\n","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"model","metadata":{"id":"ikF6Yfkz1myd","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:02.148306Z","iopub.execute_input":"2024-11-17T03:52:02.148630Z","iopub.status.idle":"2024-11-17T03:52:02.172316Z","shell.execute_reply.started":"2024-11-17T03:52:02.148595Z","shell.execute_reply":"2024-11-17T03:52:02.171314Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 1536)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (k_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=256, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (v_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=256, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (o_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=8960, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (up_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=8960, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (down_proj): lora.Linear4bit(\n            (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n            (lora_dropout): ModuleDict(\n              (LoRA): Dropout(p=0.1, inplace=False)\n            )\n            (lora_A): ModuleDict(\n              (LoRA): Linear(in_features=8960, out_features=64, bias=False)\n            )\n            (lora_B): ModuleDict(\n              (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n            )\n            (lora_embedding_A): ParameterDict()\n            (lora_embedding_B): ParameterDict()\n            (lora_magnitude_vector): ModuleDict()\n          )\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:02.173620Z","iopub.execute_input":"2024-11-17T03:52:02.174088Z","iopub.status.idle":"2024-11-17T03:52:02.200652Z","shell.execute_reply.started":"2024-11-17T03:52:02.174041Z","shell.execute_reply":"2024-11-17T03:52:02.199814Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 962475520\nTrainable parameters : 73859072\nTrainable percentage: 7.67%\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:02.201939Z","iopub.execute_input":"2024-11-17T03:52:02.202654Z","iopub.status.idle":"2024-11-17T03:52:02.208282Z","shell.execute_reply.started":"2024-11-17T03:52:02.202614Z","shell.execute_reply":"2024-11-17T03:52:02.207313Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 1\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:02.209536Z","iopub.execute_input":"2024-11-17T03:52:02.210344Z","iopub.status.idle":"2024-11-17T03:52:02.247590Z","shell.execute_reply.started":"2024-11-17T03:52:02.210298Z","shell.execute_reply":"2024-11-17T03:52:02.246648Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Nov17_03-52-02_84769eed1484,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=1,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:02.248959Z","iopub.execute_input":"2024-11-17T03:52:02.249427Z","iopub.status.idle":"2024-11-17T03:52:03.913109Z","shell.execute_reply.started":"2024-11-17T03:52:02.249381Z","shell.execute_reply":"2024-11-17T03:52:03.912228Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x7c06a4c46b00>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T03:52:03.914093Z","iopub.execute_input":"2024-11-17T03:52:03.914358Z","iopub.status.idle":"2024-11-17T04:39:49.941888Z","shell.execute_reply.started":"2024-11-17T03:52:03.914328Z","shell.execute_reply":"2024-11-17T04:39:49.940932Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 1\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Total train batch size (w. parallel, distributed & accumulation) = 4\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 73,859,072\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterlupakan100\u001b[0m (\u001b[33mterlupakan100-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112785266636314, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ce1e3697d442b0b164a845c6ef7121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241117_035205-8ptjdhej</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/8ptjdhej' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/8ptjdhej' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/8ptjdhej</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 47:33, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.165700</td>\n      <td>1.086022</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.964000</td>\n      <td>0.984770</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.939900</td>\n      <td>0.882856</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.909600</td>\n      <td>0.848809</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.828000</td>\n      <td>0.834959</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.835800</td>\n      <td>0.829584</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.805100</td>\n      <td>0.826648</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.841100</td>\n      <td>0.824919</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.863700</td>\n      <td>0.824234</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.801800</td>\n      <td>0.824147</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=0.8954721546173096, metrics={'train_runtime': 2865.5537, 'train_samples_per_second': 0.279, 'train_steps_per_second': 0.07, 'total_flos': 7166650377830400.0, 'train_loss': 0.8954721546173096, 'epoch': 0.08888888888888889})"},"metadata":{}}],"execution_count":41},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:39:49.943434Z","iopub.execute_input":"2024-11-17T04:39:49.943841Z","iopub.status.idle":"2024-11-17T04:41:59.698670Z","shell.execute_reply.started":"2024-11-17T04:39:49.943795Z","shell.execute_reply":"2024-11-17T04:41:59.697792Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 02:07]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.82414710521698, 'eval_runtime': 129.7407, 'eval_samples_per_second': 1.542, 'eval_steps_per_second': 0.385, 'epoch': 0.08888888888888889}\n","output_type":"stream"}],"execution_count":42},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:41:59.700167Z","iopub.execute_input":"2024-11-17T04:41:59.700809Z","iopub.status.idle":"2024-11-17T04:42:01.332176Z","shell.execute_reply.started":"2024-11-17T04:41:59.700759Z","shell.execute_reply":"2024-11-17T04:42:01.330823Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:42:01.334218Z","iopub.execute_input":"2024-11-17T04:42:01.334798Z","iopub.status.idle":"2024-11-17T04:42:01.732546Z","shell.execute_reply.started":"2024-11-17T04:42:01.334729Z","shell.execute_reply":"2024-11-17T04:42:01.731667Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:42:01.735079Z","iopub.execute_input":"2024-11-17T04:42:01.735798Z","iopub.status.idle":"2024-11-17T04:42:01.743562Z","shell.execute_reply.started":"2024-11-17T04:42:01.735750Z","shell.execute_reply":"2024-11-17T04:42:01.742769Z"}},"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":45},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:42:01.744694Z","iopub.execute_input":"2024-11-17T04:42:01.744983Z","iopub.status.idle":"2024-11-17T04:42:03.007488Z","shell.execute_reply.started":"2024-11-17T04:42:01.744951Z","shell.execute_reply":"2024-11-17T04:42:03.006677Z"}},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:42:03.008701Z","iopub.execute_input":"2024-11-17T04:42:03.009043Z","iopub.status.idle":"2024-11-17T04:42:06.961457Z","shell.execute_reply.started":"2024-11-17T04:42:03.008986Z","shell.execute_reply":"2024-11-17T04:42:06.960539Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2.5-1.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 21,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.2\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/model.safetensors\nInstantiating Qwen2ForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\nAll model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B-Instruct/snapshots/989aa7980e4cf806f80c7fef2b1adb7bc71aa306/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.1,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\n","output_type":"stream"},{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 1536)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:42:06.962811Z","iopub.execute_input":"2024-11-17T04:42:06.963139Z","iopub.status.idle":"2024-11-17T04:42:06.975915Z","shell.execute_reply.started":"2024-11-17T04:42:06.963104Z","shell.execute_reply":"2024-11-17T04:42:06.975080Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 888616448\nTrainable parameters : 233461248\nTrainable percentage: 26.27%\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:42:06.976906Z","iopub.execute_input":"2024-11-17T04:42:06.977485Z","iopub.status.idle":"2024-11-17T04:42:08.069978Z","shell.execute_reply.started":"2024-11-17T04:42:06.977451Z","shell.execute_reply":"2024-11-17T04:42:08.068969Z"}},"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 1536)\n        (layers): ModuleList(\n          (0-27): 28 x Qwen2DecoderLayer(\n            (self_attn): Qwen2SdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n                  (default): Linear(in_features=64, out_features=1536, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=256, bias=False)\n                  (default): Linear(in_features=64, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=256, bias=False)\n                  (default): Linear(in_features=64, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n                  (default): Linear(in_features=64, out_features=1536, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Qwen2RotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8960, bias=False)\n                  (default): Linear(in_features=64, out_features=8960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8960, bias=False)\n                  (default): Linear(in_features=64, out_features=8960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=8960, out_features=64, bias=False)\n                  (default): Linear(in_features=8960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n                  (default): Linear(in_features=64, out_features=1536, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:42:08.071329Z","iopub.execute_input":"2024-11-17T04:42:08.071687Z","iopub.status.idle":"2024-11-17T04:42:08.102175Z","shell.execute_reply.started":"2024-11-17T04:42:08.071649Z","shell.execute_reply":"2024-11-17T04:42:08.101196Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1036334592\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"## 13 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt):\n  messages = [\n    {'role' : 'system', 'content' : 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'},\n    {'role' : 'user', 'content': prompt}\n  ]\n  inputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True,\n    return_tensors = 'pt',\n    truncation = True,\n    padding = True,\n  ).to('cuda')\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    input_ids = inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0])#, skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:42:08.103384Z","iopub.execute_input":"2024-11-17T04:42:08.104310Z","iopub.status.idle":"2024-11-17T04:42:08.111692Z","shell.execute_reply.started":"2024-11-17T04:42:08.104258Z","shell.execute_reply":"2024-11-17T04:42:08.110636Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"def post_assistant(prompt):\n  messages = [\n    {'role' : 'system', 'content' : 'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.'},\n    {'role' : 'user', 'content': prompt}\n  ]\n  inputs = tokenizer.apply_chat_template(\n    messages,\n    tokenize = True,\n    add_generation_prompt = True,\n    return_tensors = 'pt',\n    truncation = True,\n    padding = True,\n  ).to('cuda')\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    input_ids = inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0])#, skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:42:08.112679Z","iopub.execute_input":"2024-11-17T04:42:08.113023Z","iopub.status.idle":"2024-11-17T04:42:08.121394Z","shell.execute_reply.started":"2024-11-17T04:42:08.112974Z","shell.execute_reply":"2024-11-17T04:42:08.120456Z"}},"outputs":[],"execution_count":52},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:42:08.122558Z","iopub.execute_input":"2024-11-17T04:42:08.122853Z","iopub.status.idle":"2024-11-17T04:42:08.134831Z","shell.execute_reply.started":"2024-11-17T04:42:08.122814Z","shell.execute_reply":"2024-11-17T04:42:08.133994Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:49:26.222788Z","iopub.execute_input":"2024-11-17T04:49:26.223488Z","iopub.status.idle":"2024-11-17T04:51:13.755438Z","shell.execute_reply.started":"2024-11-17T04:49:26.223445Z","shell.execute_reply":"2024-11-17T04:51:13.754395Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    888616448                      |                     1036334592                    \n================================================== | ==================================================\n<|im_start|>system You are Qwen, created by         |  <|im_start|>system You are Qwen, created by       \nAlibaba Cloud. You are a helpful                    |  Alibaba Cloud. You are a helpful                  \nassistant.<|im_end|> <|im_start|>user Describe how  |  assistant.<|im_end|> <|im_start|>user Describe how\nairplanes maintain altitude.<|im_end|>              |  airplanes maintain altitude.<|im_end|>            \n<|im_start|>assistant Airplanes maintain altitude   |  <|im_start|>assistant Airplanes maintain altitude \nthrough a combination of factors including the      |  through a combination of factors including the    \nlift generated by their wings, the thrust provided  |  lift generated by their wings, the thrust provided\nby their engines, and the control of their flight   |  by their engines, and the control of their flight \npath. Here's a detailed explanation of how this is  |  path. Here's a detailed explanation of how this is\nachieved:  1. **Lift**: The lift is the upward      |  achieved:  1. **Lift**: The lift is the upward    \nforce that the air exerts on the wings of an        |  force that the air exerts on the wings of an      \nairplane. This force is created by the shape of     |  airplane. This force is created by the shape of   \nthe wings, which are designed to create a           |  the wings, which are designed to create a         \ndifference in air pressure between the top and      |  difference in air pressure between the top and    \nbottom of the wing. When the wings are tilted at    |  bottom of the wing. When the wings are tilted at  \nan angle to the horizontal, the air flowing over    |  an angle to the horizontal, the air flowing over  \nthe top of the wing moves faster than the air       |  the top of the wing moves faster than the air     \nflowing under the wing, creating a pressure         |  flowing under the wing, creating a lower pressure \ndifference. This pressure difference creates a      |  on the top and a higher pressure on the bottom.   \nforce that pushes the airplane upward, providing    |  This pressure difference creates a force that     \nlift.  2. **Thrust**: The thrust is the force that  |  pushes the airplane upward, providing lift.  2.   \nthe engines of an airplane produce to move the      |  **Thrust**: The thrust is the force that the      \naircraft forward. The engines are powered by jet    |  engines of an airplane produce to move the        \nfuel or other fuels, and they convert the chemical  |  aircraft forward. The engines are powered by jet  \nenergy of the fuel into mechanical energy, which    |  fuel or other fuels, and they convert the chemical\nis then used to push the airplane forward. The      |  energy of the fuel into mechanical energy, which  \nthrust is directed through the propellers or the    |  is then used to push the airplane forward. The    \nengines themselves, and it is used to overcome the  |  thrust is directed through the propellers or the  \ndrag and maintain the aircraft's forward motion.    |  engines themselves, and it is used to overcome the\n3. **Control of Flight Path**: The control of the   |  drag and maintain the forward motion of the       \nflight path is achieved through the use of control  |  airplane.  3. **Control of Flight Path**: The     \nsurfaces on the wings and the tail of the           |  control of the flight path is achieved through the\nairplane. The control surfaces, such as the         |  use of control surfaces on the wings and the tail \nailerons (on the wings), elevator (on the tail),    |  of the airplane. The control surfaces, such as the\nand rudder (on the tail), are controlled by the     |  ailerons (on the wings), the elevator (on the     \npilot to adjust the angle of attack of the wings,   |  tail), and the rudder (on the tail), are          \nthe pitch of the nose, and the yaw of the           |  controlled by the pilot to adjust the angle of    \nairplane. These controls help the pilot to          |  attack of the wings, the pitch of the nose, and   \nmaintain the desired altitude, speed, and           |  the yaw of the airplane. These controls help the  \ndirection of flight.  4. **Thrust and Lift          |  pilot to maintain the desired altitude, speed, and\nBalance**: The thrust and lift forces must be       |  direction of the airplane.  4. **Thrust Control**:\nbalanced to maintain the airplane's altitude. If    |  The thrust control is achieved by the pilot       \nthe thrust is greater than the lift, the airplane   |  adjusting the throttle on the engines. The        \nwill accelerate and gain altitude. If the thrust    |  throttle is a control that allows the pilot to    \nis less than the lift, the airplane will            |  increase or decrease the amount of fuel that is   \ndecelerate and lose altitude. The pilot must        |  being burned, thereby increasing or decreasing the\nadjust the thrust and the angle of attack of the    |  thrust. The pilot can also adjust the pitch of the\nwings to maintain the desired balance between the   |  engines, which changes the amount of fuel that is \nthrust and the lift.  5. **Thrust and Drag          |  being burned, thereby changing the thrust.  5.    \nBalance**: The thrust and drag forces must also be  |  **Thrust and Lift Balance**: The thrust and lift  \nbalanced to maintain the airplane's altitude. The   |  forces must be balanced to maintain the altitude. \ndrag is the force that the air exerts on the        |  If the thrust is too high, the airplane will      \nairplane, opposing its motion. The drag is caused   |  accelerate and gain altitude. If the thrust is too\nby the shape of the airplane, the air resistance,   |  low, the airplane will decelerate and lose        \nand the air density. The pilot must adjust the      |  altitude. The pilot must adjust the thrust and the\nthrust and the angle of attack of the wings to      |  angle of attack of the wings to maintain the      \nmaintain the desired balance between the thrust     |  desired altitude.  In summary, airplanes maintain \nand the drag.  In summary, airplanes maintain       |  altitude by generating lift through the wings,    \naltitude by generating lift through the wings,      |  providing thrust through the engines, and         \nproviding thrust through the engines, and           |  controlling the flight path through the control   \ncontrolling the flight path through the control     |  surfaces. The pilot must balance the thrust and   \nsurfaces. The pilot must constantly adjust the      |  lift forces to maintain the desired               \nthrust and the angle of attack of the wings to      |  altitude.<|im_end|>                               \nmaintain the desired balance between the thrust     |                                                    \nand the lift, and the drag, to keep the airplane    |                                                    \nat the desired altitude.<|im_end|>                  |                                                    \n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:43:14.948742Z","iopub.execute_input":"2024-11-17T04:43:14.949088Z","iopub.status.idle":"2024-11-17T04:44:30.962445Z","shell.execute_reply.started":"2024-11-17T04:43:14.949048Z","shell.execute_reply":"2024-11-17T04:44:30.961549Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    888616448                      |                     1036334592                    \n================================================== | ==================================================\n<|im_start|>system You are Qwen, created by         |  <|im_start|>system You are Qwen, created by       \nAlibaba Cloud. You are a helpful                    |  Alibaba Cloud. You are a helpful                  \nassistant.<|im_end|> <|im_start|>user What is the   |  assistant.<|im_end|> <|im_start|>user What is the \nfunction of the three tiny bones in the middle ear  |  function of the three tiny bones in the middle ear\nand how do they contribute to the process of        |  and how do they contribute to the process of      \nhearing?<|im_end|> <|im_start|>assistant The three  |  hearing?<|im_end|> <|im_start|>assistant The three\ntiny bones in the middle ear, known as the          |  tiny bones in the middle ear, known as the        \nossicles, play a crucial role in the process of     |  ossicles, play a crucial role in the process of   \nhearing. These bones, called the malleus, incus,    |  hearing. These bones, called the malleus (hammer),\nand stapes, work together to amplify and transmit   |  incus (anvil), and stapes (stirrup), work together\nthe vibrations of sound waves from the eardrum      |  to amplify and transmit the vibrations of sound   \n(tympanic membrane) to the inner ear. Here's how    |  waves from the eardrum (tympanic membrane) to the \nthey contribute to the process of hearing:  1.      |  inner ear. Here's how they contribute to the      \n**Malleus (hammer)**: This is the largest of the    |  process of hearing:  1. **Malleus (Hammer)**: This\nthree bones and is located at the top of the        |  is the largest of the three bones and is located  \nmiddle ear. It is attached to the eardrum           |  at the top of the middle ear. It is attached to   \n(tympanic membrane) and the incus. When the         |  the eardrum by a small ligament. When the eardrum \neardrum vibrates, the malleus also vibrates,        |  vibrates, the malleus also moves, amplifying the  \namplifying the sound wave.  2. **Incus (anvil)**:   |  sound wave.  2. **Incus (Anvil)**: This bone is   \nThis bone is located between the malleus and the    |  located below the malleus and is connected to the \nstapes. It is attached to the malleus and the       |  malleus by a small ligament. The incus helps to   \nstapes. When the malleus vibrates, the incus also   |  further amplify the sound wave by moving the      \nvibrates, further amplifying the sound wave.  3.    |  malleus.  3. **Stapes (Stirrup)**: This is the    \n**Stapes (stirrup)**: This is the smallest of the   |  smallest bone in the middle ear and is located at \nthree bones and is located at the bottom of the     |  the bottom. It is attached to the incus by a small\nmiddle ear. It is attached to the incus and the     |  ligament. When the malleus and incus move, the    \noval window (the opening to the inner ear). When    |  stapes moves, further amplifying the sound wave.  \nthe incus vibrates, the stapes also vibrates,       |  The process of hearing involves the following     \nfurther amplifying the sound wave.  The amplified   |  steps:  1. **Sound Waves**: Sound waves enter the \nsound wave then travels through the oval window to  |  ear canal and travel to the eardrum (tympanic     \nthe inner ear, where it is converted into neural    |  membrane). 2. **Eardrum Vibration**: The eardrum  \nsignals that are sent to the brain, allowing us to  |  vibrates due to the pressure changes caused by the\nperceive sound. This process is essential for the   |  sound waves. 3. **Ossicles Movement**: The        \namplification of sound waves, which is necessary    |  ossicles move in response to the vibrations of the\nfor hearing to work effectively.<|im_end|>          |  eardrum, amplifying the sound wave. 4. **Hair     \n                                                    |  Cells**: The amplified sound wave causes the hair \n                                                    |  cells in the cochlea to move, generating an       \n                                                    |  electrical signal. 5. **Nerve Conduction**: The   \n                                                    |  electrical signal is then conducted to the brain  \n                                                    |  via the auditory nerve, where it is interpreted as\n                                                    |  sound.  In summary, the three tiny bones in the   \n                                                    |  middle ear work together to amplify and transmit  \n                                                    |  the vibrations of sound waves from the eardrum to \n                                                    |  the inner ear, allowing us to perceive            \n                                                    |  sound.<|im_end|>                                  \n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:55:02.483402Z","iopub.execute_input":"2024-11-17T04:55:02.484360Z","iopub.status.idle":"2024-11-17T04:56:35.428396Z","shell.execute_reply.started":"2024-11-17T04:55:02.484313Z","shell.execute_reply":"2024-11-17T04:56:35.427376Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    888616448                      |                     1036334592                    \n================================================== | ==================================================\n<|im_start|>system You are Qwen, created by         |  <|im_start|>system You are Qwen, created by       \nAlibaba Cloud. You are a helpful                    |  Alibaba Cloud. You are a helpful                  \nassistant.<|im_end|> <|im_start|>user How do viral  |  assistant.<|im_end|> <|im_start|>user How do viral\ninfections affect vulnerable populations such as    |  infections affect vulnerable populations such as  \ninfants, elderly and immunocompromised              |  infants, elderly and immunocompromised            \nindividuals, and what are the most effective        |  individuals, and what are the most effective      \nstrategies for preventing and treating these        |  strategies for preventing and treating these      \ninfections?<|im_end|> <|im_start|>assistant Viral   |  infections?<|im_end|> <|im_start|>assistant Viral \ninfections can have significant impacts on          |  infections can have significant impacts on        \nvulnerable populations such as infants, elderly,    |  vulnerable populations such as infants, elderly,  \nand immunocompromised individuals. These groups     |  and immunocompromised individuals. These groups   \nare more susceptible to severe complications and    |  are more susceptible to severe complications and  \nmay require more intensive medical care. Here are   |  may require more intensive medical care. Here are \nsome ways viral infections can affect these         |  some ways viral infections can affect these       \npopulations:  1. **Infants**: Infants are           |  populations:  1. **Infants**: Infants are         \nparticularly vulnerable to viral infections         |  particularly vulnerable to viral infections       \nbecause their immune systems are not fully          |  because their immune systems are not fully        \ndeveloped. Viral infections in infants can lead to  |  developed. Viral infections in infants can lead to\nsevere complications such as pneumonia, sepsis,     |  severe complications such as pneumonia, sepsis,   \nand even death. Preventive measures include         |  and even death. Preventive measures include       \nvaccination, good hygiene practices, and timely     |  vaccination, good hygiene practices, and timely   \nmedical care.  2. **Elderly**: The elderly are at   |  medical care.  2. **Elderly**: The elderly are at \na higher risk of severe complications from viral    |  a higher risk of severe complications from viral  \ninfections due to their weakened immune systems.    |  infections due to their weakened immune systems.  \nThey may also have underlying health conditions     |  They may also have underlying health conditions   \nthat make them more susceptible to infections.      |  that make them more susceptible to infections.    \nPreventive strategies include regular health        |  Preventive strategies include regular health      \ncheck-ups, maintaining a healthy lifestyle, and     |  check-ups, maintaining a healthy lifestyle, and   \nusing antiviral medications as prescribed.  3.      |  following proper hygiene practices.  3.           \n**Immunocompromised Individuals**: Individuals      |  **Immunocompromised Individuals**: Individuals    \nwith weakened immune systems, such as those         |  with weakened immune systems, such as those       \nundergoing chemotherapy or receiving organ          |  undergoing chemotherapy, HIV/AIDS, or receiving   \ntransplants, are more susceptible to severe         |  organ transplants, are more susceptible to severe \ncomplications from viral infections. They may       |  complications from viral infections. They may     \nrequire more intensive treatment and monitoring.    |  require more intensive treatment and monitoring.  \nPreventive measures include avoiding exposure to    |  Preventive measures include avoiding exposure to  \nviruses, maintaining good hygiene, and following    |  viruses, maintaining good hygiene, and following  \nmedical advice.  The most effective strategies for  |  medical advice.  The most effective strategies for\npreventing and treating viral infections in these   |  preventing and treating viral infections in these \npopulations include:  1. **Vaccination**: Vaccines  |  populations include:  1. **Vaccination**: Vaccines\nare the most effective way to prevent viral         |  are the most effective way to prevent viral       \ninfections. Vaccines work by stimulating the        |  infections. Vaccines work by stimulating the      \nimmune system to recognize and fight specific       |  immune system to recognize and fight specific     \nviruses, thereby reducing the severity and          |  viruses, thereby preventing or reducing the       \nduration of infections.  2. **Good Hygiene          |  severity of the infection.  2. **Good Hygiene     \nPractices**: Maintaining good hygiene practices,    |  Practices**: Maintaining good hygiene practices,  \nsuch as frequent hand washing, covering one's       |  such as frequent hand washing, covering one's     \nmouth and nose when coughing or sneezing, and       |  mouth and nose when coughing or sneezing, and     \navoiding close contact with sick individuals, can   |  avoiding close contact with sick individuals, can \nhelp prevent the spread of viruses.  3.             |  help prevent the spread of viruses.  3. **Early   \n**Antiviral Medications**: Antiviral medications    |  Detection and Treatment**: Early detection of     \ncan be effective in treating certain viral          |  viral infections is crucial for effective         \ninfections, especially in immunocompromised         |  treatment. This includes seeking medical attention\nindividuals. These medications can help reduce the  |  promptly if symptoms develop, especially in       \nseverity and duration of the infection.  4.         |  vulnerable populations. Prompt treatment can help \n**Early Detection and Treatment**: Early detection  |  prevent severe complications and reduce the risk  \nand treatment of viral infections can help prevent  |  of hospitalization.  4. **Supportive Care**: In   \nsevere complications. This includes seeking         |  vulnerable populations, supportive care measures  \nmedical attention promptly if symptoms of a viral   |  such as maintaining a healthy diet, getting enough\ninfection appear.  5. **Supportive Care**: In       |  rest, and managing underlying health conditions   \nsevere cases, supportive care such as oxygen        |  can help the body fight off infections more       \ntherapy, intravenous fluids, and monitoring for     |  effectively.  5. **Public Health Measures**:      \ncomplications can help manage the symptoms and      |  Public health measures such as social distancing, \nimprove outcomes.  In conclusion, understanding     |  mask-wearing, and regular hand washing can help   \nthe unique vulnerabilities of infants, elderly,     |  prevent the spread of viruses in communities,     \nand immunocompromised individuals is crucial in     |  thereby reducing the risk of severe complications \ndeveloping effective strategies for preventing and  |  in vulnerable populations.<|im_end|>              \ntreating viral infections. Vaccination, good        |                                                    \nhygiene practices, antiviral medications, early     |                                                    \ndetection, and supportive care are all important    |                                                    \ncomponents of these strategies.<|im_end|>           |                                                    \n","output_type":"stream"}],"execution_count":61},{"cell_type":"code","source":"prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T04:45:39.481321Z","iopub.execute_input":"2024-11-17T04:45:39.481673Z","iopub.status.idle":"2024-11-17T04:47:09.919731Z","shell.execute_reply.started":"2024-11-17T04:45:39.481625Z","shell.execute_reply":"2024-11-17T04:47:09.918795Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    888616448                      |                     1036334592                    \n================================================== | ==================================================\n<|im_start|>system You are Qwen, created by         |  <|im_start|>system You are Qwen, created by       \nAlibaba Cloud. You are a helpful                    |  Alibaba Cloud. You are a helpful                  \nassistant.<|im_end|> <|im_start|>user What were     |  assistant.<|im_end|> <|im_start|>user What were   \nthe social and economic impacts of the Industrial   |  the social and economic impacts of the Industrial \nRevolution in 19th century England?<|im_end|>       |  Revolution in 19th century England?<|im_end|>     \n<|im_start|>assistant The Industrial Revolution in  |  <|im_start|>assistant The Industrial Revolution in\n19th century England had profound social and        |  19th century England had profound social and      \neconomic impacts that reshaped the country and the  |  economic impacts that reshaped the country and the\nworld. Here are some key effects:  1. **Economic    |  world. Here are some key effects:  1. **Economic  \nGrowth**: The Industrial Revolution led to          |  Growth**: The Industrial Revolution led to        \nsignificant economic growth. The shift from         |  significant economic growth. The shift from       \nagrarian to industrial economies increased the      |  agrarian to industrial economies increased the    \noverall wealth of the country. The demand for raw   |  overall wealth of the country. The demand for raw \nmaterials and the need for skilled labor in         |  materials and the need for skilled labor in       \nfactories created new markets and industries.  2.   |  factories created new markets and industries.  2. \n**Urbanization**: The concentration of people in    |  **Urbanization**: The concentration of people in  \ncities, particularly in the newly industrialized    |  cities, particularly in the newly industrialized  \nareas, led to urbanization. This resulted in the    |  areas, led to urbanization. This resulted in the  \ngrowth of cities like Manchester, Birmingham, and   |  growth of cities like Manchester, Birmingham, and \nLondon, which became centers of industry and        |  London, which became centers of industry and      \ncommerce.  3. **Labor Market Changes**: The shift   |  commerce.  3. **Labor Market Changes**: The shift \nfrom agriculture to industry led to a significant   |  from agriculture to industry led to changes in the\nincrease in the working class. This group, often    |  labor market. There was a significant increase in \nreferred to as the proletariat, faced new           |  the demand for factory workers, which led to the  \nchallenges in terms of working conditions, wages,   |  rise of the working class. However, the working   \nand rights. The working class also included the     |  conditions in factories were often poor, leading  \nindustrial workers who were employed in factories   |  to the rise of labor movements and the formation  \nand mines.  4. **Social Inequality**: The           |  of trade unions.  4. **Social Changes**: The      \nIndustrial Revolution exacerbated social            |  Industrial Revolution also led to significant     \ninequality. The wealthy industrialists and          |  social changes. The working class faced harsh     \nlandowners became even wealthier, while the         |  conditions, long hours, and low wages. This led to\nworking class faced harsher conditions and lower    |  the rise of the labor movement, which fought for  \nwages. This led to the rise of labor movements and  |  better working conditions and fair wages. The     \nthe formation of trade unions.  5. **Technological  |  working class also faced social exclusion and     \nAdvancements**: The revolution brought about        |  discrimination.  5. **Technological               \nsignificant technological advancements in           |  Advancements**: The Industrial Revolution brought \nmanufacturing, transportation, and communication.   |  about significant technological advancements,     \nInnovations like the steam engine, the spinning     |  including the steam engine, which revolutionized  \njenny, and the factory system transformed the way   |  transportation and manufacturing. These           \ngoods were produced and distributed.  6.            |  advancements led to the development of new        \n**Environmental Impact**: The Industrial            |  industries and the expansion of the global        \nRevolution also had a significant environmental     |  economy.  6. **Environmental Impact**: The        \nimpact. The increased demand for coal and other     |  Industrial Revolution also had a significant      \nresources led to deforestation and pollution. The   |  environmental impact. The increased demand for raw\nuse of coal for energy also led to the creation of  |  materials and the use of coal for energy led to   \nthe smog that became a major environmental concern  |  deforestation and air pollution. The rapid        \nin the 20th century.  7. **Political Changes**:     |  industrialization also led to the depletion of    \nThe Industrial Revolution led to political changes  |  natural resources and the pollution of waterways. \nas well. The need for efficient and effective       |  7. **Political Changes**: The Industrial          \ngovernance in the new industrial cities led to the  |  Revolution led to political changes as well. The  \nrise of political parties and the expansion of the  |  need for a strong central government to manage the\nfranchise. The need for efficient taxation and      |  new industrial economy led to the rise of the     \nregulation also led to the development of a more    |  British Empire. The need for a strong central     \ncentralized and bureaucratic state.  8. **Cultural  |  government also led to the development of a strong\nShifts**: The Industrial Revolution also led to     |  parliamentary system.  In summary, the Industrial \ncultural shifts. The new working conditions and     |  Revolution in 19th century England had a profound \nthe lifestyle of the industrial class led to the    |  impact on the social and economic landscape of the\ndevelopment of new forms of entertainment and       |  country, leading to significant changes in the way\nleisure, such as the rise of the cinema and the     |  people lived, worked, and interacted with the     \ntheater.  In summary, the Industrial Revolution in  |  world.<|im_end|>                                  \n19th century England had a profound impact on the   |                                                    \nsocial and economic landscape of the country,       |                                                    \nleading to significant changes in the way people    |                                                    \nlived, worked, and interacted with their            |                                                    \nenvironment.<|im_end|>                              |                                                    \n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"prompt = dataset[randint(0, len(dataset))]['conversations'][0]['value']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-11-17T05:00:42.319558Z","iopub.execute_input":"2024-11-17T05:00:42.320565Z","iopub.status.idle":"2024-11-17T05:03:21.136686Z","shell.execute_reply.started":"2024-11-17T05:00:42.320505Z","shell.execute_reply":"2024-11-17T05:03:21.135656Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    888616448                      |                     1036334592                    \n================================================== | ==================================================\n<|im_start|>system You are Qwen, created by         |  <|im_start|>system You are Qwen, created by       \nAlibaba Cloud. You are a helpful                    |  Alibaba Cloud. You are a helpful                  \nassistant.<|im_end|> <|im_start|>user Provide a     |  assistant.<|im_end|> <|im_start|>user Provide a   \ncomprehensive list of benefits of integrating a     |  comprehensive list of benefits of integrating a   \nfocus on student questioning into the school        |  focus on student questioning into the school      \ncurriculum with a clear emphasis on the             |  curriculum with a clear emphasis on the           \ncorrelation between student question formulation    |  correlation between student question formulation  \nand academic success.<|im_end|>                     |  and academic success.<|im_end|>                   \n<|im_start|>assistant Integrating a focus on        |  <|im_start|>assistant Integrating a focus on      \nstudent questioning into the school curriculum can  |  student questioning into the school curriculum can\nhave numerous benefits for both students and        |  have numerous benefits for both students and      \neducators. Here is a comprehensive list of          |  educators. Here is a comprehensive list of        \nbenefits, including the correlation between         |  benefits, including the correlation between       \nstudent question formulation and academic success:  |  student question formulation and academic success:\n1. **Enhanced Critical Thinking Skills**:           |  1. **Enhanced Critical Thinking Skills**:         \nQuestioning encourages students to think            |  Questioning encourages students to think          \ncritically about the material they are learning.    |  critically about the material they are learning.  \nThis critical thinking is essential for academic    |  This critical thinking is essential for academic  \nsuccess, as it helps students to analyze,           |  success, as it helps students to analyze,         \nevaluate, and synthesize information.  2.           |  evaluate, and synthesize information.  2.         \n**Improved Understanding of Subject Matter**: By    |  **Improved Understanding of Subject Matter**: By  \nasking questions, students are more likely to       |  asking questions, students are more likely to     \nengage with the material and develop a deeper       |  engage with the material and develop a deeper     \nunderstanding of the subject. This leads to better  |  understanding of the subject. This leads to better\nretention of information and a more comprehensive   |  retention of information and a more comprehensive \ngrasp of the subject matter.  3. **Increased        |  grasp of the subject matter.  3. **Increased      \nEngagement and Motivation**: When students are      |  Engagement and Motivation**: When students are    \nencouraged to ask questions, they are more likely   |  encouraged to ask questions, they are more likely \nto be engaged in the learning process. This         |  to be engaged in the learning process. This       \nincreased engagement can lead to higher levels of   |  increased engagement can lead to higher levels of \nmotivation, which is crucial for academic success.  |  motivation, which is crucial for academic success.\n4. **Development of Problem-Solving Skills**:       |  4. **Development of Problem-Solving Skills**:     \nQuestioning encourages students to approach         |  Questioning encourages students to approach       \nproblems from different angles, fostering a more    |  problems from different angles and to think       \nflexible and creative problem-solving approach.     |  creatively. This can lead to the development of   \nThis is essential for academic success, as it       |  problem-solving skills, which are essential for   \nprepares students for the challenges they will      |  academic success.  5. **Enhanced Communication    \nface in their future careers.  5. **Enhanced        |  Skills**: Asking questions requires students to   \nCommunication Skills**: Asking questions requires   |  articulate their thoughts and ideas clearly. This \nstudents to articulate their thoughts and ideas     |  can lead to improved communication skills, which  \nclearly. This can improve their communication       |  are essential for academic success.  6. **Improved\nskills, which are essential for academic success    |  Memory Retention**: When students are encouraged  \nand for their future careers.  6. **Improved        |  to ask questions, they are more likely to engage  \nMemory and Recall**: When students are encouraged   |  with the material and develop a deeper            \nto ask questions, they are more likely to remember  |  understanding of the subject. This can lead to    \nand recall information. This is because asking      |  improved memory retention, which is essential for \nquestions helps to reinforce the material in their  |  academic success.  7. **Increased Self-           \nminds, making it easier to recall later.  7.        |  Confidence**: When students are encouraged to ask \n**Development of Analytical Thinking**:             |  questions, they are more likely to feel more      \nQuestioning encourages students to think            |  confident in their abilities. This can lead to    \nanalytically, which is essential for academic       |  increased self-confidence, which is essential for \nsuccess. This analytical thinking helps students    |  academic success.  8. **Development of Analytical \nto break down complex problems into smaller, more   |  Thinking**: Questioning encourages students to    \nmanageable parts, making it easier to solve them.   |  think analytically about the material they are    \n8. **Increased Creativity**: Questioning            |  learning. This can lead to the development of     \nencourages students to think outside the box,       |  analytical thinking skills, which are essential   \nwhich can lead to more creative and innovative      |  for academic success.  9. **Improved Time         \nsolutions. This is essential for academic success,  |  Management**: When students are encouraged to ask \nas it prepares students for the challenges they     |  questions, they are more likely to be more        \nwill face in their future careers.  9. **Improved   |  efficient in their learning. This can lead to     \nTime Management**: When students are encouraged to  |  improved time management, which is essential for  \nask questions, they are more likely to be           |  academic success.  10. **Increased Creativity**:  \nproactive in their learning. This can lead to       |  Questioning encourages students to think          \nbetter time management, as they are more likely to  |  creatively about the material they are learning.  \nprioritize their learning and focus on the most     |  This can lead to the development of creative      \nimportant material.  10. **Increased Self-          |  thinking skills, which are essential for academic \nConfidence**: When students are encouraged to ask   |  success.  11. **Improved Collaboration Skills**:  \nquestions, they are more likely to feel more        |  When students are encouraged to ask questions,    \nconfident in their abilities. This increased self-  |  they are more likely to engage with their peers.  \nconfidence can lead to better academic performance  |  This can lead to improved collaboration skills,   \nand a more positive attitude towards learning.      |  which are essential for academic success.  12.    \n11. **Improved Collaboration**: Questioning         |  **Increased Empathy**: Questioning encourages     \nencourages students to collaborate with their       |  students to think about the perspectives of       \npeers, which can lead to better learning outcomes.  |  others. This can lead to the development of       \nThis collaboration can also lead to better          |  empathy, which is essential for academic success. \ncommunication skills, which are essential for       |  13. **Improved Memory Retention**: When students  \nacademic success.  12. **Increased Motivation**:    |  are encouraged to ask questions, they are more    \nWhen students are encouraged to ask questions,      |  likely to engage with the material and develop a  \nthey are more likely to be motivated to learn.      |  deeper understanding of the subject. This can lead\nThis increased motivation can lead to better        |  to improved memory retention, which is essential  \nacademic performance and a more positive attitude   |  for academic success.  14. **Increased            \ntowards learning.  In conclusion, integrating a     |  Motivation**: When students are encouraged to ask \nfocus on student questioning into the school        |  questions, they are more likely to be engaged in  \ncurriculum can have numerous benefits for both      |  the learning process. This can lead to increased  \nstudents and educators. The correlation between     |  motivation, which is essential for academic       \nstudent question formulation and academic success   |  success.  15. **Improved Communication Skills**:  \nis clear, and it is essential for students to be    |  Asking questions requires students to articulate  \nencouraged to ask questions. This can lead to       |  their thoughts and ideas clearly. This can lead to\nimproved understanding of subject matter,           |  improved communication skills, which are essential\nincreased engagement and motivation, and better     |  for academic success.  16. **Improved Time        \nacademic performance.<|im_end|>                     |  Management**: When students are encouraged to ask \n                                                    |  questions, they are more likely to be more        \n                                                    |  efficient in their learning. This can lead to     \n                                                    |  improved time management, which is essential for  \n                                                    |  academic success.  17. **Increased Creativity**:  \n                                                    |  Questioning encourages students to think          \n                                                    |  creatively about the material they are learning.  \n                                                    |  This can lead to the development of creative      \n                                                    |  thinking skills, which are essential for academic \n                                                    |  success.  18. **Improved Memory Retention**: When \n                                                    |  students are encouraged to ask questions, they are\n                                                    |  more likely to engage with the material and       \n                                                    |  develop a deeper understanding of the subject.    \n                                                    |  This can lead to improved memory retention, which \n                                                    |  is essential for academic success.  19.           \n                                                    |  **Increased Empathy**: Questioning encourages     \n                                                    |  students to think about the perspectives of       \n                                                    |  others. This can lead to the development of       \n                                                    |  empathy, which is essential for academic success. \n                                                    |  20. **Improved Memory Retention**: When students  \n                                                    |  are encouraged to ask questions, they are more    \n                                                    |  likely to engage with the material and develop a  \n                                                    |  deeper understanding of the subject. This can lead\n                                                    |  to improved memory retention, which is essential  \n                                                    |  for academic success.  In conclusion, integrating \n                                                    |  a focus on student questioning into the school    \n                                                    |  curriculum can have numerous benefits for both    \n                                                    |  students and educators. The correlation between   \n                                                    |  student question formulation and academic success \n                                                    |  is clear, and it is essential for students to be  \n                                                    |  encouraged to ask questions in order to achieve   \n                                                    |  academic success.<|im_end|>                       \n","output_type":"stream"}],"execution_count":63}]}