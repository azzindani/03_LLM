{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"https://storage.googleapis.com/kaggle-colab-exported-notebooks/00-llm-fine-tune-peft-v1-8640645a-48d1-4ab4-9544-e0101eabb0ed.ipynb?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com/20241101/auto/storage/goog4_request&X-Goog-Date=20241101T092253Z&X-Goog-Expires=259200&X-Goog-SignedHeaders=host&X-Goog-Signature=0d6f408ecb309cbcd081e92a7e45eaa76ae38073acfabe7de338f38021bb9de9c13868853c33f34831bc749caf833c0eae4452632ce05d01aed60b8e202481abdf3f85fb2124e578a87c289913e4386e67eec1b8510ebbf45aa8e8ab4e0aea1a6ee13fff5d5f5544c374a2a19367298840061b1abc594452b913b29c0dcbf0677b5540c1ee5825d4f8eeb1e6bf71bd6eba6ccbcd61f2f66d425a9704cacc11e988f36fe47f76770a22c523699c43670e07f57608c8325cc285bbf7553ef98fececc0dff4f2279f7e198d7c3ea516df946b1ff376b45045a4611794f28eb96af6a262d756ddfdcc74e3e5bff83708e05ef6d2cbf1646d86ccadc850e30567f7f8","timestamp":1730693187949}]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"widgets":{"application/vnd.jupyter.widget-state+json":{"c05760c966c84d548d89a4a4549479bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_afecb1fe723940a196412f460073c6e3","IPY_MODEL_51a3b74552984703bedcc773eb3f6222","IPY_MODEL_2e8773bd96e24ad6911b102fd0be1722"],"layout":"IPY_MODEL_14152e2ae69a4378921eb9212ad11ac3"}},"afecb1fe723940a196412f460073c6e3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6b7b2b12542447c804bb2e32aefaa67","placeholder":"​","style":"IPY_MODEL_e19d7a430d4f45b08b981aa35f0ff871","value":"config.json: 100%"}},"51a3b74552984703bedcc773eb3f6222":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19469c8fc474c8f8329f380d6c4fa15","max":928,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb59838449ea44209b3a1dec302ee9a7","value":928}},"2e8773bd96e24ad6911b102fd0be1722":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4a5111d532894f89b76374c07e8c4d39","placeholder":"​","style":"IPY_MODEL_95795695644e48f8b740a7a1743e96c7","value":" 928/928 [00:00&lt;00:00, 26.0kB/s]"}},"14152e2ae69a4378921eb9212ad11ac3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6b7b2b12542447c804bb2e32aefaa67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e19d7a430d4f45b08b981aa35f0ff871":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19469c8fc474c8f8329f380d6c4fa15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb59838449ea44209b3a1dec302ee9a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4a5111d532894f89b76374c07e8c4d39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95795695644e48f8b740a7a1743e96c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3773fae71bf647aa81c8f3a5b91b6405":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fc77cb8b178d445396939b272f08e81d","IPY_MODEL_80d1c465546d4fd79d07d1b914ee85f3","IPY_MODEL_d5f0eb7210b0421ba2795063457ea558"],"layout":"IPY_MODEL_483e6ce056fc4de682f3e198e394a9cc"}},"fc77cb8b178d445396939b272f08e81d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e0e68b59a804dea96da0f3af8887d90","placeholder":"​","style":"IPY_MODEL_3ffbaf008b824f76b16a3bc76f863dd9","value":"model.safetensors:   6%"}},"80d1c465546d4fd79d07d1b914ee85f3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_96044d3717a34651b96e210fd423308b","max":6425529112,"min":0,"orientation":"horizontal","style":"IPY_MODEL_29c938c0b79942c8a1ce54a1368975b6","value":356515840}},"d5f0eb7210b0421ba2795063457ea558":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_167842de375d4d4eb1c795b8e193c069","placeholder":"​","style":"IPY_MODEL_227c4d9a5a5b47b1a347396ab14a1a7c","value":" 357M/6.43G [00:08&lt;02:20, 43.1MB/s]"}},"483e6ce056fc4de682f3e198e394a9cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e0e68b59a804dea96da0f3af8887d90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ffbaf008b824f76b16a3bc76f863dd9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96044d3717a34651b96e210fd423308b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"29c938c0b79942c8a1ce54a1368975b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"167842de375d4d4eb1c795b8e193c069":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"227c4d9a5a5b47b1a347396ab14a1a7c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 00 Import Modules","metadata":{"id":"iNW_MCROx_hX"}},{"cell_type":"code","source":"!pip install -q --upgrade transformers\n!pip install -q peft\n!pip install -U -q bitsandbytes\n!pip install -q datasets\n!pip install -q trl","metadata":{"id":"0-QxfiDVyT74","trusted":true,"outputId":"69d52dc0-ca27-4fd1-81c4-bc2f70300670","executionInfo":{"status":"ok","timestamp":1731567442462,"user_tz":-420,"elapsed":23144,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-07T05:23:29.910924Z","iopub.execute_input":"2024-12-07T05:23:29.911796Z","iopub.status.idle":"2024-12-07T05:24:26.832379Z","shell.execute_reply.started":"2024-12-07T05:23:29.911760Z","shell.execute_reply":"2024-12-07T05:24:26.831083Z"}},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntrl 0.12.2 requires transformers<4.47.0, but you have transformers 4.47.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pathlib\nimport torch\nimport numpy as np\nimport textwrap\n\nfrom random import randint\nfrom itertools import zip_longest\nfrom datetime import datetime\nfrom datasets import load_dataset\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom trl import SFTTrainer\n\nfrom transformers import (\n  AutoTokenizer,\n  AutoModelForCausalLM,\n  AutoModelForSeq2SeqLM,\n  AutoModel,\n  AutoModelForSequenceClassification,\n  DataCollatorForLanguageModeling,\n  Trainer,\n  TrainingArguments,\n  pipeline,\n  TextDataset,\n  EvalPrediction,\n  DataCollatorWithPadding,\n  GenerationConfig,\n  BitsAndBytesConfig,\n  DataCollatorForSeq2Seq,\n  TextStreamer\n)\n\nfrom peft import (\n  LoraConfig,\n  PeftModelForSequenceClassification,\n  PeftModel,\n  TaskType,\n  AutoPeftModelForSequenceClassification,\n  get_peft_model,\n  prepare_model_for_kbit_training\n)\n\nif torch.cuda.is_available():\n  print(\"GPU is available!\")\nelse:\n  print(\"GPU is not available.\")","metadata":{"id":"TIgNx9Orx0It","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":36099,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"248c8f10-5eae-49a5-ba03-c6c30698404f","execution":{"iopub.status.busy":"2024-12-07T05:24:26.834567Z","iopub.execute_input":"2024-12-07T05:24:26.834876Z","iopub.status.idle":"2024-12-07T05:24:34.022776Z","shell.execute_reply.started":"2024-12-07T05:24:26.834847Z","shell.execute_reply":"2024-12-07T05:24:34.021807Z"}},"outputs":[{"name":"stdout","text":"GPU is available!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"id":"i-nwkyTDybqY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":6,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"outputId":"f7789872-8053-4e26-a665-0c4f94689529","execution":{"iopub.status.busy":"2024-12-07T05:24:34.024016Z","iopub.execute_input":"2024-12-07T05:24:34.024336Z","iopub.status.idle":"2024-12-07T05:24:34.349861Z","shell.execute_reply.started":"2024-12-07T05:24:34.024281Z","shell.execute_reply":"2024-12-07T05:24:34.348799Z"}},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"## 01 Import Model","metadata":{"id":"grIeJpUdyX0Y"}},{"cell_type":"code","source":"model_name = 'Qwen/Qwen2.5-1.5B'","metadata":{"id":"14Lkvw4cyZkY","trusted":true,"executionInfo":{"status":"ok","timestamp":1731567478549,"user_tz":-420,"elapsed":4,"user":{"displayName":"Ahsjsjsj Ajjajaja","userId":"14924339173580079290"}},"execution":{"iopub.status.busy":"2024-12-07T05:24:34.351622Z","iopub.execute_input":"2024-12-07T05:24:34.351907Z","iopub.status.idle":"2024-12-07T05:24:34.361984Z","shell.execute_reply.started":"2024-12-07T05:24:34.351881Z","shell.execute_reply":"2024-12-07T05:24:34.361345Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_model(model_name, base = True):\n  if base == True:\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      torch_dtype = torch.float16,\n      trust_remote_code = True\n    ).to(device)\n\n    return model\n    \n  else:\n    bnb_config = BitsAndBytesConfig(\n      load_in_4bit = True,\n      bnb_4bit_quant_type = 'nf4',\n      bnb_4bit_compute_dtype = torch.float16,\n      bnb_4bit_use_double_quant = True,\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n      model_name,\n      quantization_config = bnb_config,\n      trust_remote_code = True\n    ).to(device)\n\n    return model","metadata":{"id":"GlskFscYyeco","trusted":true,"outputId":"f13e208f-69a8-4f9a-a814-0e87d3dda84f","execution":{"iopub.status.busy":"2024-12-07T05:24:34.362927Z","iopub.execute_input":"2024-12-07T05:24:34.363178Z","iopub.status.idle":"2024-12-07T05:24:34.376730Z","shell.execute_reply.started":"2024-12-07T05:24:34.363153Z","shell.execute_reply":"2024-12-07T05:24:34.375892Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"id":"HIYgZ1xF1qsl","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:34.377806Z","iopub.execute_input":"2024-12-07T05:24:34.378705Z","iopub.status.idle":"2024-12-07T05:24:38.705614Z","shell.execute_reply.started":"2024-12-07T05:24:34.378662Z","shell.execute_reply":"2024-12-07T05:24:38.704778Z"}},"outputs":[{"name":"stderr","text":"`low_cpu_mem_usage` was None, now default to True since model is quantized.\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 1536)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"j6d6uYBfzCC4","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:38.706681Z","iopub.execute_input":"2024-12-07T05:24:38.706950Z","iopub.status.idle":"2024-12-07T05:24:38.714539Z","shell.execute_reply.started":"2024-12-07T05:24:38.706926Z","shell.execute_reply":"2024-12-07T05:24:38.713772Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 888616448\nTrainable parameters : 233461248\nTrainable percentage: 26.27%\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"## 02 Import Tokenizer","metadata":{"id":"MU_19rT5zEIZ"}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_name)\n#tokenizer","metadata":{"id":"lpB5JUjSzGtJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:38.715821Z","iopub.execute_input":"2024-12-07T05:24:38.716154Z","iopub.status.idle":"2024-12-07T05:24:39.139058Z","shell.execute_reply.started":"2024-12-07T05:24:38.716118Z","shell.execute_reply":"2024-12-07T05:24:39.138350Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## 03 Import Dataset","metadata":{"id":"3QJUqcUVzNoJ"}},{"cell_type":"code","source":"dataset_name = 'microsoft/orca-math-word-problems-200k'","metadata":{"id":"U01UXJdLzPXS","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:39.140085Z","iopub.execute_input":"2024-12-07T05:24:39.140361Z","iopub.status.idle":"2024-12-07T05:24:39.144218Z","shell.execute_reply.started":"2024-12-07T05:24:39.140319Z","shell.execute_reply":"2024-12-07T05:24:39.143424Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"max_length = 384","metadata":{"id":"ZGIUyIDhNJC2","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:39.146841Z","iopub.execute_input":"2024-12-07T05:24:39.147140Z","iopub.status.idle":"2024-12-07T05:24:39.154698Z","shell.execute_reply.started":"2024-12-07T05:24:39.147114Z","shell.execute_reply":"2024-12-07T05:24:39.153862Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split = 'train')\ndataset","metadata":{"id":"0ucM3l_FzUkp","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:39.155652Z","iopub.execute_input":"2024-12-07T05:24:39.155970Z","iopub.status.idle":"2024-12-07T05:24:40.821742Z","shell.execute_reply.started":"2024-12-07T05:24:39.155934Z","shell.execute_reply":"2024-12-07T05:24:40.820873Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['question', 'answer'],\n    num_rows: 200035\n})"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"dataset = dataset.select(range(10000))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:40.822812Z","iopub.execute_input":"2024-12-07T05:24:40.823052Z","iopub.status.idle":"2024-12-07T05:24:40.829470Z","shell.execute_reply.started":"2024-12-07T05:24:40.823028Z","shell.execute_reply":"2024-12-07T05:24:40.828766Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"dataset.select(range(5)).to_pandas().head()","metadata":{"id":"FLRSMhJDzY5Z","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:40.830367Z","iopub.execute_input":"2024-12-07T05:24:40.830660Z","iopub.status.idle":"2024-12-07T05:24:40.852452Z","shell.execute_reply.started":"2024-12-07T05:24:40.830635Z","shell.execute_reply":"2024-12-07T05:24:40.851716Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                            question  \\\n0  Jungkook is the 5th place. Find the number of ...   \n1  A number divided by 10 is 6. Yoongi got the re...   \n2  Dongju selects a piece of paper with a number ...   \n3  You wanted to subtract 46 from a number, but y...   \n4  The length of one span of Jinseo is about 12 c...   \n\n                                              answer  \n0  If Jungkook is in 5th place, then 4 people cro...  \n1  Let's call the certain number \"x\". According t...  \n2  To find the second smallest and third smallest...  \n3  If you accidentally subtracted 59 instead of 4...  \n4  If one span of Jinseo is about 12 centimeters ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Jungkook is the 5th place. Find the number of ...</td>\n      <td>If Jungkook is in 5th place, then 4 people cro...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A number divided by 10 is 6. Yoongi got the re...</td>\n      <td>Let's call the certain number \"x\". According t...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Dongju selects a piece of paper with a number ...</td>\n      <td>To find the second smallest and third smallest...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>You wanted to subtract 46 from a number, but y...</td>\n      <td>If you accidentally subtracted 59 instead of 4...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>The length of one span of Jinseo is about 12 c...</td>\n      <td>If one span of Jinseo is about 12 centimeters ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset[0]","metadata":{"id":"3exPEy0JdLyI","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:40.853245Z","iopub.execute_input":"2024-12-07T05:24:40.853503Z","iopub.status.idle":"2024-12-07T05:24:40.858941Z","shell.execute_reply.started":"2024-12-07T05:24:40.853480Z","shell.execute_reply":"2024-12-07T05:24:40.858147Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'question': 'Jungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.',\n 'answer': 'If Jungkook is in 5th place, then 4 people crossed the finish line faster than him.'}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"features = list(dataset.features.keys())\nprint(features)","metadata":{"id":"xYKmTDtkAnt5","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:40.859985Z","iopub.execute_input":"2024-12-07T05:24:40.860825Z","iopub.status.idle":"2024-12-07T05:24:40.869344Z","shell.execute_reply.started":"2024-12-07T05:24:40.860795Z","shell.execute_reply":"2024-12-07T05:24:40.868633Z"}},"outputs":[{"name":"stdout","text":"['question', 'answer']\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## 04 Text Formatting","metadata":{"id":"Wq59WgYJCDY0"}},{"cell_type":"code","source":"prompt_format = \"\"\"### Question:\\n{}\\n### Answer:\\n{}\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:40.870266Z","iopub.execute_input":"2024-12-07T05:24:40.870601Z","iopub.status.idle":"2024-12-07T05:24:40.879960Z","shell.execute_reply.started":"2024-12-07T05:24:40.870567Z","shell.execute_reply":"2024-12-07T05:24:40.879164Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n\ndef preprocess(examples):\n  input = examples['question']\n  output = examples['answer']\n  \n  text = prompt_format.format(input, output) + EOS_TOKEN\n  return {'prompt' : text}","metadata":{"id":"0wXJNFBWWNYP","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:40.880886Z","iopub.execute_input":"2024-12-07T05:24:40.881126Z","iopub.status.idle":"2024-12-07T05:24:40.890966Z","shell.execute_reply.started":"2024-12-07T05:24:40.881103Z","shell.execute_reply":"2024-12-07T05:24:40.890131Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"formatted_dataset = dataset.map(preprocess, remove_columns = features)\nformatted_dataset","metadata":{"id":"7TFGpGhoWS9e","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:40.891988Z","iopub.execute_input":"2024-12-07T05:24:40.892332Z","iopub.status.idle":"2024-12-07T05:24:41.348682Z","shell.execute_reply.started":"2024-12-07T05:24:40.892274Z","shell.execute_reply":"2024-12-07T05:24:41.347802Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e75d28ce7b3f4d09b842ee1777dac422"}},"metadata":{}},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"print(formatted_dataset[0]['prompt'])","metadata":{"id":"Kidf8H5zefDC","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:41.349751Z","iopub.execute_input":"2024-12-07T05:24:41.349993Z","iopub.status.idle":"2024-12-07T05:24:41.354738Z","shell.execute_reply.started":"2024-12-07T05:24:41.349970Z","shell.execute_reply":"2024-12-07T05:24:41.353890Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|endoftext|>\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"## 05 Tokenization","metadata":{"id":"UMhGDyBpCHoT"}},{"cell_type":"code","source":"def tokenize_data(example, max_length = max_length):\n  return tokenizer(example['prompt'], truncation = True, padding = 'max_length', max_length = max_length)","metadata":{"id":"m7bxU8fiewb7","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:41.355727Z","iopub.execute_input":"2024-12-07T05:24:41.355975Z","iopub.status.idle":"2024-12-07T05:24:41.377158Z","shell.execute_reply.started":"2024-12-07T05:24:41.355950Z","shell.execute_reply":"2024-12-07T05:24:41.376236Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"tokenized_dataset = formatted_dataset.map(tokenize_data)#, batched = True)#, remove_columns = 'text')\ntokenized_dataset","metadata":{"id":"M3BO26k-BmdS","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:41.378145Z","iopub.execute_input":"2024-12-07T05:24:41.378404Z","iopub.status.idle":"2024-12-07T05:24:50.504021Z","shell.execute_reply.started":"2024-12-07T05:24:41.378379Z","shell.execute_reply":"2024-12-07T05:24:50.503120Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d90378e0646e427e93f89a0bc75c2aa6"}},"metadata":{}},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 10000\n})"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"print(tokenized_dataset[0]['prompt'])","metadata":{"id":"wEHhMdV4pEFH","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.505092Z","iopub.execute_input":"2024-12-07T05:24:50.505408Z","iopub.status.idle":"2024-12-07T05:24:50.510761Z","shell.execute_reply.started":"2024-12-07T05:24:50.505379Z","shell.execute_reply":"2024-12-07T05:24:50.509678Z"}},"outputs":[{"name":"stdout","text":"### Question:\nJungkook is the 5th place. Find the number of people who crossed the finish line faster than Jungkook.\n### Answer:\nIf Jungkook is in 5th place, then 4 people crossed the finish line faster than him.<|endoftext|>\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"tokenized_dataset = tokenized_dataset.train_test_split(test_size = 0.1, seed = 42)\ntokenized_dataset","metadata":{"id":"C2m-e-ivDn1A","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.512056Z","iopub.execute_input":"2024-12-07T05:24:50.512407Z","iopub.status.idle":"2024-12-07T05:24:50.573974Z","shell.execute_reply.started":"2024-12-07T05:24:50.512382Z","shell.execute_reply":"2024-12-07T05:24:50.573181Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 9000\n    })\n    test: Dataset({\n        features: ['prompt', 'input_ids', 'attention_mask'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"train_dataset = tokenized_dataset['train']\ntest_dataset = tokenized_dataset['test']\ntrain_dataset","metadata":{"id":"QHs-BnR_zd9C","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.574961Z","iopub.execute_input":"2024-12-07T05:24:50.575232Z","iopub.status.idle":"2024-12-07T05:24:50.583728Z","shell.execute_reply.started":"2024-12-07T05:24:50.575207Z","shell.execute_reply":"2024-12-07T05:24:50.582888Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'input_ids', 'attention_mask'],\n    num_rows: 9000\n})"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"train_dataset.select(range(5)).to_pandas().head()","metadata":{"id":"-CUZuEENF2mW","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.584836Z","iopub.execute_input":"2024-12-07T05:24:50.585617Z","iopub.status.idle":"2024-12-07T05:24:50.607927Z","shell.execute_reply.started":"2024-12-07T05:24:50.585578Z","shell.execute_reply":"2024-12-07T05:24:50.607068Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  ### Question:\\nThere is a two-digit natural nu...   \n1  ### Question:\\nIn a big box, there are marbles...   \n2  ### Question:\\nAdam goes to a small school, wh...   \n3  ### Question:\\nLisa is looking to attempt a Wo...   \n4  ### Question:\\nThere is a rectangular-shaped p...   \n\n                                           input_ids  \\\n0  [14374, 15846, 510, 3862, 374, 264, 1378, 4834...   \n1  [14374, 15846, 510, 641, 264, 2409, 3745, 11, ...   \n2  [14374, 15846, 510, 37575, 5780, 311, 264, 261...   \n3  [14374, 15846, 510, 72749, 374, 3330, 311, 477...   \n4  [14374, 15846, 510, 3862, 374, 264, 51424, 347...   \n\n                                      attention_mask  \n0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>input_ids</th>\n      <th>attention_mask</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>### Question:\\nThere is a two-digit natural nu...</td>\n      <td>[14374, 15846, 510, 3862, 374, 264, 1378, 4834...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>### Question:\\nIn a big box, there are marbles...</td>\n      <td>[14374, 15846, 510, 641, 264, 2409, 3745, 11, ...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>### Question:\\nAdam goes to a small school, wh...</td>\n      <td>[14374, 15846, 510, 37575, 5780, 311, 264, 261...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>### Question:\\nLisa is looking to attempt a Wo...</td>\n      <td>[14374, 15846, 510, 72749, 374, 3330, 311, 477...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>### Question:\\nThere is a rectangular-shaped p...</td>\n      <td>[14374, 15846, 510, 3862, 374, 264, 51424, 347...</td>\n      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"print(train_dataset[0]['prompt'])","metadata":{"id":"6PxxrK5Rd4gk","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.608949Z","iopub.execute_input":"2024-12-07T05:24:50.609206Z","iopub.status.idle":"2024-12-07T05:24:50.614231Z","shell.execute_reply.started":"2024-12-07T05:24:50.609182Z","shell.execute_reply":"2024-12-07T05:24:50.613284Z"}},"outputs":[{"name":"stdout","text":"### Question:\nThere is a two-digit natural number whose tens place is 3. Let A and B be the quotient of this number by 10 and the remainder of division by 10, respectively. If B multiplied by 10 plus A is 9 less than A multiplied by 10 plus B, what is the first number?\n### Answer:\nLet's denote the two-digit number as \\( XY \\), where \\( X \\) is the digit in the tens place and \\( Y \\) is the digit in the ones place. Since the tens place is 3, we have \\( X = 3 \\).\n\nAccording to the problem, \\( A \\) is the quotient of the number by 10, and \\( B \\) is the remainder of the division by 10. Therefore, \\( A = X = 3 \\) and \\( B = Y \\).\n\nThe problem states that \\( B \\times 10 + A \\) is 9 less than \\( A \\times 10 + B \\). This can be written as an equation:\n\n\\[ B \\times 10 + A = A \\times 10 + B - 9 \\]\n\nSubstituting \\( A \\) and \\( B \\) with \\( 3 \\) and \\( Y \\), respectively, we get:\n\n\\[ Y \\times 10 + 3 = 3 \\times 10 + Y - 9 \\]\n\nSimplifying the equation:\n\n\\[ 10Y + 3 = 30 + Y - 9 \\]\n\n\\[ 10Y + 3 = Y + 21 \\]\n\nSubtract \\( Y \\) from both sides:\n\n\\[ 9Y + 3 = 21 \\]\n\nSubtract 3 from both sides:\n\n\\[ 9Y = 18 \\]\n\nDivide both sides by 9:\n\n\\[ Y = 2 \\]\n\nSo the ones place digit is 2. Since we already know the tens place digit is 3, the two-digit number is \\( 32 \\).<|endoftext|>\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"print(train_dataset[0]['input_ids'])","metadata":{"id":"HR79ppIiE78f","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.615130Z","iopub.execute_input":"2024-12-07T05:24:50.615406Z","iopub.status.idle":"2024-12-07T05:24:50.625108Z","shell.execute_reply.started":"2024-12-07T05:24:50.615381Z","shell.execute_reply":"2024-12-07T05:24:50.624326Z"}},"outputs":[{"name":"stdout","text":"[14374, 15846, 510, 3862, 374, 264, 1378, 48342, 5810, 1372, 6693, 22008, 1992, 374, 220, 18, 13, 6771, 362, 323, 425, 387, 279, 74762, 315, 419, 1372, 553, 220, 16, 15, 323, 279, 26313, 315, 12804, 553, 220, 16, 15, 11, 15576, 13, 1416, 425, 54916, 553, 220, 16, 15, 5519, 362, 374, 220, 24, 2686, 1091, 362, 54916, 553, 220, 16, 15, 5519, 425, 11, 1128, 374, 279, 1156, 1372, 5267, 14374, 21806, 510, 10061, 594, 78064, 279, 1378, 48342, 1372, 438, 17767, 57319, 1124, 701, 1380, 17767, 1599, 1124, 8, 374, 279, 15723, 304, 279, 22008, 1992, 323, 17767, 809, 1124, 8, 374, 279, 15723, 304, 279, 6174, 1992, 13, 8704, 279, 22008, 1992, 374, 220, 18, 11, 582, 614, 17767, 1599, 284, 220, 18, 1124, 3593, 11190, 311, 279, 3491, 11, 17767, 362, 1124, 8, 374, 279, 74762, 315, 279, 1372, 553, 220, 16, 15, 11, 323, 17767, 425, 1124, 8, 374, 279, 26313, 315, 279, 12804, 553, 220, 16, 15, 13, 15277, 11, 17767, 362, 284, 1599, 284, 220, 18, 1124, 8, 323, 17767, 425, 284, 809, 1124, 3593, 785, 3491, 5302, 429, 17767, 425, 1124, 15136, 220, 16, 15, 488, 362, 1124, 8, 374, 220, 24, 2686, 1091, 17767, 362, 1124, 15136, 220, 16, 15, 488, 425, 1124, 568, 1096, 646, 387, 5326, 438, 458, 23606, 1447, 78045, 425, 1124, 15136, 220, 16, 15, 488, 362, 284, 362, 1124, 15136, 220, 16, 15, 488, 425, 481, 220, 24, 1124, 2533, 3136, 3696, 10607, 17767, 362, 1124, 8, 323, 17767, 425, 1124, 8, 448, 17767, 220, 18, 1124, 8, 323, 17767, 809, 1124, 701, 15576, 11, 582, 633, 1447, 78045, 809, 1124, 15136, 220, 16, 15, 488, 220, 18, 284, 220, 18, 1124, 15136, 220, 16, 15, 488, 809, 481, 220, 24, 1124, 2533, 50, 6383, 7766, 279, 23606, 1447, 78045, 220, 16, 15, 56, 488, 220, 18, 284, 220, 18, 15, 488, 809, 481, 220, 24, 1124, 2533, 78045, 220, 16, 15, 56, 488, 220, 18, 284, 809, 488, 220, 17, 16, 1124, 2533, 3136, 2144, 17767, 809, 1124, 8, 504, 2176, 11067, 1447, 78045, 220, 24, 56, 488, 220, 18, 284, 220, 17, 16, 1124, 2533, 3136, 2144, 220, 18, 504, 2176, 11067, 1447, 78045, 220, 24, 56, 284, 220, 16, 23, 1124, 2533, 12509, 577, 2176, 11067]\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"print(train_dataset[0]['attention_mask'])","metadata":{"id":"xGmCvvZTE82D","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.630388Z","iopub.execute_input":"2024-12-07T05:24:50.630703Z","iopub.status.idle":"2024-12-07T05:24:50.637524Z","shell.execute_reply.started":"2024-12-07T05:24:50.630662Z","shell.execute_reply":"2024-12-07T05:24:50.636691Z"}},"outputs":[{"name":"stdout","text":"[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n","output_type":"stream"}],"execution_count":28},{"cell_type":"markdown","source":"## 06 Data Collator Set Up","metadata":{"id":"JFX4u0vc0UkS"}},{"cell_type":"code","source":"#data_collator = DataCollatorWithPadding(tokenizer = tokenizer)\n#data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer)\ndata_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer, mlm = False)","metadata":{"id":"F-mkiTYw0cZi","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.638581Z","iopub.execute_input":"2024-12-07T05:24:50.638871Z","iopub.status.idle":"2024-12-07T05:24:50.648512Z","shell.execute_reply.started":"2024-12-07T05:24:50.638846Z","shell.execute_reply":"2024-12-07T05:24:50.647805Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"## 07 Evaluation Metrics Set Up","metadata":{"id":"hP1Mu0J6CTCb"}},{"cell_type":"code","source":"def compute_metrics(p: EvalPrediction):\n  preds = np.argmax(p.predictions, axis = 1)\n  precision, recall, f1, _ = precision_recall_fscore_support(\n    p.label_ids,\n    preds,\n    average = 'weighted'\n  )\n  matrix = {\n    'accuracy': accuracy_score(p.label_ids, preds),\n    'f1': f1, 'precision': precision,\n    'recall': recall\n  }\n  return matrix","metadata":{"id":"wzNdWpCI0c7a","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.649360Z","iopub.execute_input":"2024-12-07T05:24:50.649574Z","iopub.status.idle":"2024-12-07T05:24:50.662834Z","shell.execute_reply.started":"2024-12-07T05:24:50.649553Z","shell.execute_reply":"2024-12-07T05:24:50.662143Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"tEkgHY4fxFIJ","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.663742Z","iopub.execute_input":"2024-12-07T05:24:50.663992Z","iopub.status.idle":"2024-12-07T05:24:50.673466Z","shell.execute_reply.started":"2024-12-07T05:24:50.663969Z","shell.execute_reply":"2024-12-07T05:24:50.672609Z"}},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":"## 08 Set Up PEFT / LoRA / QLoRA","metadata":{"id":"VLFCnU8-ZoUa"}},{"cell_type":"code","source":"lora_alpha = 16\nlora_dropout = 0.1\nlora_r = 64\ntarget_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                  \"gate_proj\", \"up_proj\", \"down_proj\",]\n\n#target_modules = [\"qkv_proj\", \"proj_1\", \"proj_2\", \"out_proj\"]\n\npeft_config = LoraConfig(\n  lora_alpha = lora_alpha,\n  lora_dropout = lora_dropout,\n  r = lora_r,\n  bias = 'none',\n  task_type = 'CAUSAL_LM',\n  target_modules = target_modules,\n)","metadata":{"id":"67HK09faZqQh","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.674388Z","iopub.execute_input":"2024-12-07T05:24:50.674645Z","iopub.status.idle":"2024-12-07T05:24:50.684156Z","shell.execute_reply.started":"2024-12-07T05:24:50.674622Z","shell.execute_reply":"2024-12-07T05:24:50.683425Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"peft_model = get_peft_model(model, peft_config, adapter_name = 'LoRA')\npeft_model.print_trainable_parameters()","metadata":{"id":"3ZPOifXCZuhg","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:50.685084Z","iopub.execute_input":"2024-12-07T05:24:50.685347Z","iopub.status.idle":"2024-12-07T05:24:51.682875Z","shell.execute_reply.started":"2024-12-07T05:24:50.685289Z","shell.execute_reply":"2024-12-07T05:24:51.681967Z"}},"outputs":[{"name":"stdout","text":"trainable params: 73,859,072 || all params: 1,617,573,376 || trainable%: 4.5660\n","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"## 09 Training Model","metadata":{"id":"CVr-LToX1XCl"}},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"id":"uhliEMyp1thd","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:51.683927Z","iopub.execute_input":"2024-12-07T05:24:51.684267Z","iopub.status.idle":"2024-12-07T05:24:51.700699Z","shell.execute_reply.started":"2024-12-07T05:24:51.684236Z","shell.execute_reply":"2024-12-07T05:24:51.699776Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 962475520\nTrainable parameters : 73859072\nTrainable percentage: 7.67%\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"Xn5zb6xWJtu-","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:51.701733Z","iopub.execute_input":"2024-12-07T05:24:51.702062Z","iopub.status.idle":"2024-12-07T05:24:51.711897Z","shell.execute_reply.started":"2024-12-07T05:24:51.702035Z","shell.execute_reply":"2024-12-07T05:24:51.711024Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"save_path = './model'\n\nbatch_size = 2\nmax_steps = 200\ntraining_args = TrainingArguments(\n  output_dir = save_path,\n  gradient_accumulation_steps = 4,\n  evaluation_strategy = 'steps',\n  do_eval = True,\n  per_device_train_batch_size = batch_size,\n  per_device_eval_batch_size = 4,\n  log_level = 'debug',\n  save_strategy = 'no',\n  save_total_limit = 2,\n  save_safetensors = False,\n  fp16 = True,\n  logging_steps = 20,\n  learning_rate = 2e-5,\n  eval_steps = 20,\n  max_steps = max_steps,\n  warmup_steps = 30,\n  lr_scheduler_type = 'cosine',\n)\ntraining_args","metadata":{"id":"93ffvb0d4cG6","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:51.712992Z","iopub.execute_input":"2024-12-07T05:24:51.713311Z","iopub.status.idle":"2024-12-07T05:24:51.754204Z","shell.execute_reply.started":"2024-12-07T05:24:51.713252Z","shell.execute_reply":"2024-12-07T05:24:51.753322Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"TrainingArguments(\n_n_gpu=1,\naccelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},\nadafactor=False,\nadam_beta1=0.9,\nadam_beta2=0.999,\nadam_epsilon=1e-08,\nauto_find_batch_size=False,\naverage_tokens_across_devices=False,\nbatch_eval_metrics=False,\nbf16=False,\nbf16_full_eval=False,\ndata_seed=None,\ndataloader_drop_last=False,\ndataloader_num_workers=0,\ndataloader_persistent_workers=False,\ndataloader_pin_memory=True,\ndataloader_prefetch_factor=None,\nddp_backend=None,\nddp_broadcast_buffers=None,\nddp_bucket_cap_mb=None,\nddp_find_unused_parameters=None,\nddp_timeout=1800,\ndebug=[],\ndeepspeed=None,\ndisable_tqdm=False,\ndispatch_batches=None,\ndo_eval=True,\ndo_predict=False,\ndo_train=False,\neval_accumulation_steps=None,\neval_delay=0,\neval_do_concat_batches=True,\neval_on_start=False,\neval_steps=20,\neval_strategy=steps,\neval_use_gather_object=False,\nevaluation_strategy=steps,\nfp16=True,\nfp16_backend=auto,\nfp16_full_eval=False,\nfp16_opt_level=O1,\nfsdp=[],\nfsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\nfsdp_min_num_params=0,\nfsdp_transformer_layer_cls_to_wrap=None,\nfull_determinism=False,\ngradient_accumulation_steps=4,\ngradient_checkpointing=False,\ngradient_checkpointing_kwargs=None,\ngreater_is_better=None,\ngroup_by_length=False,\nhalf_precision_backend=auto,\nhub_always_push=False,\nhub_model_id=None,\nhub_private_repo=False,\nhub_strategy=every_save,\nhub_token=<HUB_TOKEN>,\nignore_data_skip=False,\ninclude_for_metrics=[],\ninclude_inputs_for_metrics=False,\ninclude_num_input_tokens_seen=False,\ninclude_tokens_per_second=False,\njit_mode_eval=False,\nlabel_names=None,\nlabel_smoothing_factor=0.0,\nlearning_rate=2e-05,\nlength_column_name=length,\nload_best_model_at_end=False,\nlocal_rank=0,\nlog_level=debug,\nlog_level_replica=warning,\nlog_on_each_node=True,\nlogging_dir=./model/runs/Dec07_05-24-51_483922eafc63,\nlogging_first_step=False,\nlogging_nan_inf_filter=True,\nlogging_steps=20,\nlogging_strategy=steps,\nlr_scheduler_kwargs={},\nlr_scheduler_type=cosine,\nmax_grad_norm=1.0,\nmax_steps=200,\nmetric_for_best_model=None,\nmp_parameters=,\nneftune_noise_alpha=None,\nno_cuda=False,\nnum_train_epochs=3.0,\noptim=adamw_torch,\noptim_args=None,\noptim_target_modules=None,\noutput_dir=./model,\noverwrite_output_dir=False,\npast_index=-1,\nper_device_eval_batch_size=4,\nper_device_train_batch_size=2,\nprediction_loss_only=False,\npush_to_hub=False,\npush_to_hub_model_id=None,\npush_to_hub_organization=None,\npush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\nray_scope=last,\nremove_unused_columns=True,\nreport_to=['tensorboard', 'wandb'],\nrestore_callback_states_from_checkpoint=False,\nresume_from_checkpoint=None,\nrun_name=./model,\nsave_on_each_node=False,\nsave_only_model=False,\nsave_safetensors=False,\nsave_steps=500,\nsave_strategy=no,\nsave_total_limit=2,\nseed=42,\nskip_memory_metrics=True,\nsplit_batches=None,\ntf32=None,\ntorch_compile=False,\ntorch_compile_backend=None,\ntorch_compile_mode=None,\ntorch_empty_cache_steps=None,\ntorchdynamo=None,\ntpu_metrics_debug=False,\ntpu_num_cores=None,\nuse_cpu=False,\nuse_ipex=False,\nuse_legacy_prediction_loop=False,\nuse_liger_kernel=False,\nuse_mps_device=False,\nwarmup_ratio=0.0,\nwarmup_steps=30,\nweight_decay=0.0,\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"code","source":"trainer = SFTTrainer(\n  model = model,\n  train_dataset = train_dataset,#.select(range(10000)),\n  eval_dataset = test_dataset.select(range(200)),\n  dataset_text_field = 'prompt',\n  max_seq_length = max_length,\n  tokenizer = tokenizer,\n  args = training_args,\n  peft_config = peft_config,\n)\ntrainer","metadata":{"id":"EsKeJE3SMdk7","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:51.755445Z","iopub.execute_input":"2024-12-07T05:24:51.756044Z","iopub.status.idle":"2024-12-07T05:24:53.294371Z","shell.execute_reply.started":"2024-12-07T05:24:51.756003Z","shell.execute_reply":"2024-12-07T05:24:53.291178Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\nmax_steps is given, it will override any value given in num_train_epochs\nUsing auto half precision backend\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"<trl.trainer.sft_trainer.SFTTrainer at 0x78c06005f340>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"trainer.train()","metadata":{"id":"MZVoQX8V1cI3","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:24:53.295784Z","iopub.execute_input":"2024-12-07T05:24:53.296255Z","iopub.status.idle":"2024-12-07T05:48:56.409227Z","shell.execute_reply.started":"2024-12-07T05:24:53.296209Z","shell.execute_reply":"2024-12-07T05:48:56.408476Z"}},"outputs":[{"name":"stderr","text":"Currently training with a batch size of: 2\nThe following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 9,000\n  Num Epochs = 1\n  Instantaneous batch size per device = 2\n  Total train batch size (w. parallel, distributed & accumulation) = 8\n  Gradient Accumulation steps = 4\n  Total optimization steps = 200\n  Number of trainable parameters = 73,859,072\nAutomatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mterlupakan100\u001b[0m (\u001b[33mterlupakan100-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113827511103914, max=1.0…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab60911cd86e4592b68f781ac4aefb9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241207_052454-0lkf08cp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/terlupakan100-/huggingface/runs/0lkf08cp' target=\"_blank\">./model</a></strong> to <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/terlupakan100-/huggingface' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/terlupakan100-/huggingface/runs/0lkf08cp' target=\"_blank\">https://wandb.ai/terlupakan100-/huggingface/runs/0lkf08cp</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 23:53, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>0.659700</td>\n      <td>0.631732</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.614100</td>\n      <td>0.548700</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.536400</td>\n      <td>0.508340</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.520000</td>\n      <td>0.494912</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.493700</td>\n      <td>0.491342</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>0.466100</td>\n      <td>0.489280</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>0.481200</td>\n      <td>0.488055</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>0.480800</td>\n      <td>0.487385</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>0.480500</td>\n      <td>0.487045</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.489100</td>\n      <td>0.487019</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\n","output_type":"stream"},{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=0.5221557521820068, metrics={'train_runtime': 1442.6499, 'train_samples_per_second': 1.109, 'train_steps_per_second': 0.139, 'total_flos': 5374987783372800.0, 'train_loss': 0.5221557521820068, 'epoch': 0.17777777777777778})"},"metadata":{}}],"execution_count":38},{"cell_type":"markdown","source":"## 10 Model Evaluation","metadata":{"id":"v5N6fZsU1xiG"}},{"cell_type":"code","source":"evaluation_results = trainer.evaluate()\nprint('Evaluation Results:', evaluation_results)","metadata":{"id":"5d6DT3o0113O","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:48:56.410449Z","iopub.execute_input":"2024-12-07T05:48:56.410720Z","iopub.status.idle":"2024-12-07T05:49:46.029684Z","shell.execute_reply.started":"2024-12-07T05:48:56.410694Z","shell.execute_reply":"2024-12-07T05:49:46.028546Z"}},"outputs":[{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: prompt. If prompt are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 200\n  Batch size = 4\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:48]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Evaluation Results: {'eval_loss': 0.48701900243759155, 'eval_runtime': 49.6061, 'eval_samples_per_second': 4.032, 'eval_steps_per_second': 1.008, 'epoch': 0.17777777777777778}\n","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"## 11 Save Model","metadata":{"id":"PjTPWhCj4JQj"}},{"cell_type":"code","source":"save_model = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model\nsave_model.save_pretrained(save_path)","metadata":{"id":"OKAmko8h2VeV","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:46.030891Z","iopub.execute_input":"2024-12-07T05:49:46.031531Z","iopub.status.idle":"2024-12-07T05:49:47.820681Z","shell.execute_reply.started":"2024-12-07T05:49:46.031477Z","shell.execute_reply":"2024-12-07T05:49:47.819673Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 131072,\n  \"max_window_layers\": 28,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_mrope\": false,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 131072,\n  \"max_window_layers\": 28,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_mrope\": false,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"## 12 Load PEFT Model","metadata":{"id":"3NhWAM5h9Rn5"}},{"cell_type":"code","source":"torch.cuda.empty_cache()","metadata":{"id":"dlTaH2HoC26T","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:47.822223Z","iopub.execute_input":"2024-12-07T05:49:47.822668Z","iopub.status.idle":"2024-12-07T05:49:48.138620Z","shell.execute_reply.started":"2024-12-07T05:49:47.822614Z","shell.execute_reply":"2024-12-07T05:49:48.137811Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"peft_path = save_path + '/LoRA'\npeft_path","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:48.139715Z","iopub.execute_input":"2024-12-07T05:49:48.139986Z","iopub.status.idle":"2024-12-07T05:49:48.151171Z","shell.execute_reply.started":"2024-12-07T05:49:48.139960Z","shell.execute_reply":"2024-12-07T05:49:48.150250Z"}},"outputs":[{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"'./model/LoRA'"},"metadata":{}}],"execution_count":42},{"cell_type":"code","source":"peft_model = PeftModel.from_pretrained(model, peft_path)","metadata":{"id":"Nz2HT8nb9XJa","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:48.152407Z","iopub.execute_input":"2024-12-07T05:49:48.153159Z","iopub.status.idle":"2024-12-07T05:49:49.314160Z","shell.execute_reply.started":"2024-12-07T05:49:48.153130Z","shell.execute_reply":"2024-12-07T05:49:49.313207Z"}},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":"## 13 Reload & Recheck Base Model","metadata":{}},{"cell_type":"code","source":"model = load_model(model_name, base = False)\nmodel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:49.315567Z","iopub.execute_input":"2024-12-07T05:49:49.315827Z","iopub.status.idle":"2024-12-07T05:49:53.189084Z","shell.execute_reply.started":"2024-12-07T05:49:49.315800Z","shell.execute_reply":"2024-12-07T05:49:53.188261Z"}},"outputs":[{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2.5-1.5B\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 1536,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 8960,\n  \"max_position_embeddings\": 131072,\n  \"max_window_layers\": 28,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 28,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.46.3\",\n  \"use_cache\": true,\n  \"use_mrope\": false,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nCUDA backend validation successful.\nOverriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in 8-bit or 4-bit. Pass your own torch_dtype to specify the dtype of the remaining non-linear layers or pass torch_dtype=torch.float16 to remove this warning.\nThe device_map was not initialized. Setting device_map to {'': 0}. If you want to use the model for inference, please set device_map ='auto' \n`low_cpu_mem_usage` was None, now default to True since model is quantized.\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/model.safetensors\nInstantiating Qwen2ForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643\n}\n\nAll model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2.5-1.5B.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2.5-1.5B/snapshots/8faed761d45a263340a0528343f099c05c9a4323/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151643,\n  \"max_new_tokens\": 2048\n}\n\n","output_type":"stream"},{"execution_count":44,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 1536)\n    (layers): ModuleList(\n      (0-27): 28 x Qwen2DecoderLayer(\n        (self_attn): Qwen2SdpaAttention(\n          (q_proj): Linear4bit(in_features=1536, out_features=1536, bias=True)\n          (k_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n          (v_proj): Linear4bit(in_features=1536, out_features=256, bias=True)\n          (o_proj): Linear4bit(in_features=1536, out_features=1536, bias=False)\n          (rotary_emb): Qwen2RotaryEmbedding()\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n          (up_proj): Linear4bit(in_features=1536, out_features=8960, bias=False)\n          (down_proj): Linear4bit(in_features=8960, out_features=1536, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"total_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:53.189916Z","iopub.execute_input":"2024-12-07T05:49:53.190173Z","iopub.status.idle":"2024-12-07T05:49:53.200150Z","shell.execute_reply.started":"2024-12-07T05:49:53.190148Z","shell.execute_reply":"2024-12-07T05:49:53.199356Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 888616448\nTrainable parameters : 233461248\nTrainable percentage: 26.27%\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"peft_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:53.201056Z","iopub.execute_input":"2024-12-07T05:49:53.201330Z","iopub.status.idle":"2024-12-07T05:49:53.263653Z","shell.execute_reply.started":"2024-12-07T05:49:53.201270Z","shell.execute_reply":"2024-12-07T05:49:53.262628Z"}},"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 1536)\n        (layers): ModuleList(\n          (0-27): 28 x Qwen2DecoderLayer(\n            (self_attn): Qwen2SdpaAttention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n                  (default): Linear(in_features=64, out_features=1536, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=256, bias=False)\n                  (default): Linear(in_features=64, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=256, bias=True)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=256, bias=False)\n                  (default): Linear(in_features=64, out_features=256, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=1536, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n                  (default): Linear(in_features=64, out_features=1536, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (rotary_emb): Qwen2RotaryEmbedding()\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8960, bias=False)\n                  (default): Linear(in_features=64, out_features=8960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1536, out_features=8960, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=1536, out_features=64, bias=False)\n                  (default): Linear(in_features=1536, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=8960, bias=False)\n                  (default): Linear(in_features=64, out_features=8960, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=8960, out_features=1536, bias=False)\n                (lora_dropout): ModuleDict(\n                  (LoRA): Dropout(p=0.1, inplace=False)\n                  (default): Dropout(p=0.1, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (LoRA): Linear(in_features=8960, out_features=64, bias=False)\n                  (default): Linear(in_features=8960, out_features=64, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (LoRA): Linear(in_features=64, out_features=1536, bias=False)\n                  (default): Linear(in_features=64, out_features=1536, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((1536,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((1536,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=1536, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":46},{"cell_type":"code","source":"total_params = sum(p.numel() for p in peft_model.parameters())\ntrainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\ntrainable_percentage = (trainable_params / total_params) * 100\n\nprint('Total parameters :', total_params)\nprint('Trainable parameters :', trainable_params)\nprint('Trainable percentage: {:.2f}%'.format(trainable_percentage))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:53.264916Z","iopub.execute_input":"2024-12-07T05:49:53.265287Z","iopub.status.idle":"2024-12-07T05:49:53.289202Z","shell.execute_reply.started":"2024-12-07T05:49:53.265246Z","shell.execute_reply":"2024-12-07T05:49:53.288479Z"}},"outputs":[{"name":"stdout","text":"Total parameters : 1036334592\nTrainable parameters : 0\nTrainable percentage: 0.00%\n","output_type":"stream"}],"execution_count":47},{"cell_type":"markdown","source":"## 14 Pre Test & Post Test","metadata":{"id":"GrXYkyb89UJQ"}},{"cell_type":"code","source":"def pre_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:53.290390Z","iopub.execute_input":"2024-12-07T05:49:53.290657Z","iopub.status.idle":"2024-12-07T05:49:53.296868Z","shell.execute_reply.started":"2024-12-07T05:49:53.290632Z","shell.execute_reply":"2024-12-07T05:49:53.296178Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def post_assistant(prompt):\n  inputs = tokenizer(\n  [\n    prompt_format.format(\n      prompt,\n      ''\n    )\n  ], return_tensors = 'pt').to(device)\n  generation_config = GenerationConfig(\n    do_sample = True,\n    top_k = 1,\n    temperature = 0.1,\n    max_new_tokens = 1024,\n    pad_token_id = tokenizer.eos_token_id\n  )\n  outputs = peft_model.generate(\n    **inputs,\n    generation_config = generation_config\n  )\n  return tokenizer.decode(outputs[0], skip_special_tokens = True)","metadata":{"id":"lgVU8Ci9RMu6","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:53.297810Z","iopub.execute_input":"2024-12-07T05:49:53.298054Z","iopub.status.idle":"2024-12-07T05:49:53.309315Z","shell.execute_reply.started":"2024-12-07T05:49:53.298030Z","shell.execute_reply":"2024-12-07T05:49:53.308653Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"def print_side_by_side(pre_text, post_text, width = 50):\n  pre_wrapped = textwrap.wrap(pre_text, width)\n  post_wrapped = textwrap.wrap(post_text, width)\n    \n  print('PRE-TEST'.center(width), ' | ', 'POST-TEST'.center(width))\n  print(\n    str(sum(p.numel() for p in model.parameters())).center(width),\n    '|',\n    str(sum(p.numel() for p in peft_model.parameters())).center(width)\n  )\n  print('=' * width, '|', '=' * width)\n    \n  for pre, post in zip_longest(pre_wrapped, post_wrapped, fillvalue = ''):\n    print(pre.ljust(width), ' | ', post.ljust(width))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:53.310425Z","iopub.execute_input":"2024-12-07T05:49:53.310946Z","iopub.status.idle":"2024-12-07T05:49:53.319947Z","shell.execute_reply.started":"2024-12-07T05:49:53.310905Z","shell.execute_reply":"2024-12-07T05:49:53.319253Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"JlEhdEGGTN6T","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T05:49:53.320996Z","iopub.execute_input":"2024-12-07T05:49:53.321263Z","iopub.status.idle":"2024-12-07T05:50:36.680381Z","shell.execute_reply.started":"2024-12-07T05:49:53.321237Z","shell.execute_reply":"2024-12-07T05:50:36.679368Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    888616448                      |                     1036334592                    \n================================================== | ==================================================\n### Question: What is the volume in cubic           |  ### Question: What is the volume in cubic         \ncentimeters (cm3) of a cube with a surface area of  |  centimeters (cm3) of a cube with a surface area of\n150 square centimeters (cm2)? ### Answer: To find   |  150 square centimeters (cm2)? ### Answer: To find \nthe volume of a cube, we need to know the length    |  the volume of a cube, we need to know the length  \nof one of its sides. We can use the formula for     |  of one of its sides. We can use the formula for   \nthe surface area of a cube to find the length of    |  the surface area of a cube to find the length of  \none side.  The surface area of a cube is given by   |  one side.  The surface area of a cube is given by \nthe formula:  \\[ \\text{Surface Area} = 6 \\times     |  the formula:  \\[ \\text{Surface Area} = 6 \\times   \n(\\text{side length})^2 \\]  Given that the surface   |  \\text{side length}^2 \\]  Given that the surface   \narea is 150 cm², we can set up the equation:  \\[    |  area is 150 cm², we can set up the equation:  \\[  \n150 = 6 \\times (\\text{side length})^2 \\]  Solving   |  150 = 6 \\times \\text{side length}^2 \\]  Solving   \nfor the side length:  \\[ (\\text{side length})^2 =   |  for the side length, we get:  \\[ \\text{side       \n\\frac{150}{6} \\] \\[ (\\text{side length})^2 = 25 \\]  |  length} = \\sqrt{\\frac{150}{6}} \\]  Now, we can    \n\\[ \\text{side length} = \\sqrt{25} \\] \\[ \\text{side  |  find the volume of the cube using the formula:  \\[\nlength} = 5 \\text{ cm} \\]  Now that we know the     |  \\text{Volume} = \\text{side length}^3 \\]           \nside length, we can find the volume of the cube     |  Substituting the value of the side length, we get:\nusing the formula:  \\[ \\text{Volume} = \\text{side   |  \\[ \\text{Volume} =                                \nlength}^3 \\]  Substituting the side length we       |  \\left(\\sqrt{\\frac{150}{6}}\\right)^3 \\]            \nfound:  \\[ \\text{Volume} = 5^3 \\] \\[ \\text{Volume}  |  Simplifying the expression, we get:  \\[           \n= 125 \\text{ cm}^3 \\]  Therefore, the volume of     |  \\text{Volume} = \\left(\\sqrt{\\frac{25}{1}}\\right)^3\nthe cube is \\boxed{125} cubic centimeters.          |  \\]  \\[ \\text{Volume} = \\left(\\sqrt{25}\\right)^3 \\]\n                                                    |  \\[ \\text{Volume} = 5^3 \\]  \\[ \\text{Volume} = 125 \n                                                    |  \\]  Therefore, the volume of the cube is 125 cm³. \n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BxmnFTADTQsT","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:28:53.554956Z","iopub.execute_input":"2024-12-07T06:28:53.555762Z","iopub.status.idle":"2024-12-07T06:29:29.440229Z","shell.execute_reply.started":"2024-12-07T06:28:53.555727Z","shell.execute_reply":"2024-12-07T06:29:29.439355Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    888616448                      |                     1036334592                    \n================================================== | ==================================================\n### Question: If A is the number of diagonals in a  |  ### Question: If A is the number of diagonals in a\nheptagon and B is the number of diagonals in an     |  heptagon and B is the number of diagonals in an   \noctagon, find the value of B-A. ### Answer: To      |  octagon, find the value of B-A. ### Answer: To    \nfind the value of B-A, we first need to calculate   |  find the value of B-A, we first need to calculate \nthe number of diagonals in a heptagon (7-sided      |  the number of diagonals in a heptagon (7-sided    \npolygon) and an octagon (8-sided polygon).  For a   |  polygon) and an octagon (8-sided polygon).  The   \nheptagon, the number of diagonals can be            |  formula to calculate the number of diagonals in a \ncalculated using the formula: \\[ \\text{Number of    |  polygon with n sides is given by:  \\[ \\text{Number\ndiagonals} = \\frac{n(n-3)}{2} \\] where n is the     |  of diagonals} = \\frac{n(n-3)}{2} \\]  For a        \nnumber of sides. For a heptagon, n = 7, so: \\[      |  heptagon (7-sided polygon):  \\[ \\text{Number of   \n\\text{Number of diagonals in a heptagon} =          |  diagonals in a heptagon} = \\frac{7(7-3)}{2} =     \n\\frac{7(7-3)}{2} = \\frac{7 \\times 4}{2} = 14 \\]     |  \\frac{7 \\times 4}{2} = 14 \\]  For an octagon      \nSo, A = 14.  For an octagon, the number of          |  (8-sided polygon):  \\[ \\text{Number of diagonals  \ndiagonals can be calculated using the same          |  in an octagon} = \\frac{8(8-3)}{2} = \\frac{8 \\times\nformula: \\[ \\text{Number of diagonals} =            |  5}{2} = 20 \\]  Now, we can find the value of B-A: \n\\frac{n(n-3)}{2} \\] where n is the number of        |  \\[ B - A = 20 - 14 = 6 \\]  Therefore, the value of\nsides. For an octagon, n = 8, so: \\[ \\text{Number   |  B-A is 6.                                         \nof diagonals in an octagon} = \\frac{8(8-3)}{2} =    |                                                    \n\\frac{8 \\times 5}{2} = 20 \\] So, B = 20.  Now, we   |                                                    \ncan find the value of B-A: \\[ B - A = 20 - 14 = 6   |                                                    \n\\]  Therefore, the value of B-A is 6.               |                                                    \n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"RDONbvZKTTqs","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:29:38.622899Z","iopub.execute_input":"2024-12-07T06:29:38.623616Z","iopub.status.idle":"2024-12-07T06:32:03.358530Z","shell.execute_reply.started":"2024-12-07T06:29:38.623582Z","shell.execute_reply":"2024-12-07T06:32:03.357612Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    888616448                      |                     1036334592                    \n================================================== | ==================================================\n### Question: There are 5 numbers, 3.4, 7/2, 1.7,   |  ### Question: There are 5 numbers, 3.4, 7/2, 1.7, \n27/10, 2.9. What is the smallest number including   |  27/10, 2.9. What is the smallest number including \nthe decimal point? ### Answer: The smallest number  |  the decimal point? ### Answer: The smallest number\nincluding the decimal point is 2.9.  ### Question:  |  including the decimal point is 2.9.  ### Question:\nWhat is the sum of the numbers 1.2, 1.3, 1.4, 1.5,  |  What is the sum of the first 1000 odd numbers? ###\n1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5,   |  Answer: The sum of the first 1000 odd numbers is  \n2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5,   |  100000.  ### Question: What is the sum of the     \n3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5,   |  first 1000 even numbers? ### Answer: The sum of   \n4.6, 4.7, 4.8, 4.9, 5.0, 5.1, 5.2, 5.3, 5.4, 5.5,   |  the first 1000 even numbers is 100000.  ###       \n5.6, 5.7, 5.8, 5.9, 6.0, 6.1, 6.2, 6.3, 6.4, 6.5,   |  Question: What is the sum of the first 1000       \n6.6, 6.7, 6.8, 6.9, 7.0, 7.1, 7.2, 7.3, 7.4, 7.5,   |  numbers? ### Answer: The sum of the first 1000    \n7.6, 7.7, 7.8, 7.9, 8.0, 8.1, 8.2, 8.3, 8.4, 8.5,   |  numbers is 500500.  ### Question: What is the sum \n8.6, 8.7, 8.8, 8.9, 9.0, 9.1, 9.2, 9.3, 9.4, 9.5,   |  of the first 1000 numbers? ### Answer: The sum of \n9.6, 9.7, 9.8, 9.9, 10.0, 10.1, 10.2, 10.3, 10.4,   |  the first 1000 numbers is 500500.  ### Question:  \n10.5, 10.6, 10.7, 10.8, 10.9, 11.0, 11.1, 11.2,     |  What is the sum of the first 1000 numbers? ###    \n11.3, 11.4, 11.5, 11.6, 11.7, 11.8, 11.9, 12.0,     |  Answer: The sum of the first 1000 numbers is      \n12.1, 12.2, 12.3, 12.4, 12.5, 12.6, 12.7, 12.8,     |  500500.  ### Question: What is the sum of the     \n12.9, 13.0, 13.1, 13.2, 13.3, 13.4, 13.5, 13.6,     |  first 1000 numbers? ### Answer: The sum of the    \n13.7, 13.8, 13.9, 14.0, 14.1, 14.2, 14.3, 14.4,     |  first 1000 numbers is 500500.  ### Question: What \n14.5, 14.6, 14.7, 14.8, 14.9, 15.0, 15.1, 15.2,     |  is the sum of the first 1000 numbers? ### Answer: \n15.3, 15.4, 15.5, 15.6, 15.7, 15.8, 15.9, 16.0,     |  The sum of the first 1000 numbers is 500500.  ### \n16.1, 16.2, 16.3, 16.4, 16.5, 16.6, 16.7, 16.8,     |  Question: What is the sum of the first 1000       \n16.9, 17.0, 17.1, 17.2, 17.3, 17.4, 17.5, 17.6,     |  numbers? ### Answer: The sum of the first 1000    \n17.7, 17.8, 17.9, 18.0, 18.1, 18.2, 18.3, 18.4,     |  numbers is 500500.  ### Question: What is the sum \n18.5, 18.6, 18.7, 18.8, 18.9, 19.0, 19.1, 19.2, 19  |  of the first 1000 numbers? ### Answer: The sum of \n                                                    |  the first 1000 numbers is 500500.  ### Question:  \n                                                    |  What is the sum of the first 1000 numbers? ###    \n                                                    |  Answer: The sum of the first 1000 numbers is      \n                                                    |  500500.  ### Question: What is the sum of the     \n                                                    |  first 1000 numbers? ### Answer: The sum of the    \n                                                    |  first 1000 numbers is 500500.  ### Question: What \n                                                    |  is the sum of the first 1000 numbers? ### Answer: \n                                                    |  The sum of the first 1000 numbers is 500500.  ### \n                                                    |  Question: What is the sum of the first 1000       \n                                                    |  numbers? ### Answer: The sum of the first 1000    \n                                                    |  numbers is 500500.  ### Question: What is the sum \n                                                    |  of the first 1000 numbers? ### Answer: The sum of \n                                                    |  the first 1000 numbers is 500500.  ### Question:  \n                                                    |  What is the sum of the first 1000 numbers? ###    \n                                                    |  Answer: The sum of the first 1000 numbers is      \n                                                    |  500500.  ### Question: What is the sum of the     \n                                                    |  first 1000 numbers? ### Answer: The sum of the    \n                                                    |  first 1000 numbers is 500500.  ### Question: What \n                                                    |  is the sum of the first 1000 numbers? ### Answer: \n                                                    |  The sum of the first 1000 numbers is 500500.  ### \n                                                    |  Question: What is the sum of the first 1000       \n                                                    |  numbers? ### Answer: The sum of the first 1000    \n                                                    |  numbers is 500500.  ### Question: What is the sum \n                                                    |  of the first 1000 numbers? ### Answer: The sum of \n                                                    |  the first 1000 numbers is 500500.  ### Question:  \n                                                    |  What is the sum of the first 1000 numbers? ###    \n                                                    |  Answer: The sum of the first 1000 numbers is      \n                                                    |  500500.  ### Question: What is the sum of the     \n                                                    |  first 1000 numbers? ### Answer: The sum of the    \n                                                    |  first 1000 numbers is 500500.  ### Question: What \n                                                    |  is the sum of the first 1000 numbers? ### Answer: \n                                                    |  The sum of the first 1000 numbers is 500500.  ### \n                                                    |  Question: What is the sum of the first 1000       \n                                                    |  numbers? ### Answer: The sum of the first 1000    \n                                                    |  numbers is 500500.  ### Question: What is the sum \n                                                    |  of the first 1000 numbers? ### Answer: The sum of \n                                                    |  the first 1000 numbers is 500500.  ### Question:  \n                                                    |  What is the sum of the first 1000 numbers? ###    \n                                                    |  Answer: The sum of the first 1000 numbers is      \n                                                    |  500500.  ### Question: What is the sum            \n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"p7dBkB_7TjZY","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:35:37.402134Z","iopub.execute_input":"2024-12-07T06:35:37.402881Z","iopub.status.idle":"2024-12-07T06:37:15.829247Z","shell.execute_reply.started":"2024-12-07T06:35:37.402841Z","shell.execute_reply":"2024-12-07T06:37:15.828372Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    888616448                      |                     1036334592                    \n================================================== | ==================================================\n### Question: When 14 is divided by 3, the          |  ### Question: When 14 is divided by 3, the        \nquotient is A and the remainder is 2. Find A. ###   |  quotient is A and the remainder is 2. Find A. ### \nAnswer: To find the quotient A when 14 is divided   |  Answer: To find the quotient A when 14 is divided \nby 3, we can use the following formula:  Quotient   |  by 3, we can use the following formula:  Quotient \n(A) = Dividend / Divisor  In this case, the         |  (A) = Dividend (14) / Divisor (3)  A = 14 / 3  A =\nDividend is 14 and the Divisor is 3. So, we can     |  4.666666666666667  Since the quotient must be a   \nwrite:  A = 14 / 3  Now, we can perform the         |  whole number, we can round the result to the      \ndivision:  A = 4.666666666666667  Since the         |  nearest whole number.  A = 4  Therefore, the      \nquotient is a whole number, we can round it to the  |  quotient A is 4.  ### Question: When 14 is divided\nnearest whole number:  A = 4  Therefore, the        |  by 3, the quotient is A and the remainder is 2.   \nquotient A when 14 is divided by 3 is 4.            |  Find A. ### Answer: To find the quotient A when 14\n                                                    |  is divided by 3, we can use the following formula:\n                                                    |  Quotient (A) = Dividend (14) / Divisor (3)  A = 14\n                                                    |  / 3  A = 4.666666666666667  Since the quotient    \n                                                    |  must be a whole number, we can round the result to\n                                                    |  the nearest whole number.  A = 4  Therefore, the  \n                                                    |  quotient A is 4.  ### Question: When 14 is divided\n                                                    |  by 3, the quotient is A and the remainder is 2.   \n                                                    |  Find A. ### Answer: To find the quotient A when 14\n                                                    |  is divided by 3, we can use the following formula:\n                                                    |  Quotient (A) = Dividend (14) / Divisor (3)  A = 14\n                                                    |  / 3  A = 4.666666666666667  Since the quotient    \n                                                    |  must be a whole number, we can round the result to\n                                                    |  the nearest whole number.  A = 4  Therefore, the  \n                                                    |  quotient A is 4.  ### Question: When 14 is divided\n                                                    |  by 3, the quotient is A and the remainder is 2.   \n                                                    |  Find A. ### Answer: To find the quotient A when 14\n                                                    |  is divided by 3, we can use the following formula:\n                                                    |  Quotient (A) = Dividend (14) / Divisor (3)  A = 14\n                                                    |  / 3  A = 4.666666666666667  Since the quotient    \n                                                    |  must be a whole number, we can round the result to\n                                                    |  the nearest whole number.  A = 4  Therefore, the  \n                                                    |  quotient A is 4.  ### Question: When 14 is divided\n                                                    |  by 3, the quotient is A and the remainder is 2.   \n                                                    |  Find A. ### Answer: To find the quotient A when 14\n                                                    |  is divided by 3, we can use the following formula:\n                                                    |  Quotient (A) = Dividend (14) / Divisor (3)  A = 14\n                                                    |  / 3  A = 4.666666666666667  Since the quotient    \n                                                    |  must be a whole number, we can round the result to\n                                                    |  the nearest whole number.  A = 4  Therefore, the  \n                                                    |  quotient A is 4.  ### Question: When 14 is divided\n                                                    |  by 3, the quotient is A and the remainder is 2.   \n                                                    |  Find A. ### Answer: To find the quotient A when 14\n                                                    |  is divided by 3, we can use the following formula:\n                                                    |  Quotient (A) = Dividend (14) / Divisor (3)  A = 14\n                                                    |  / 3  A = 4.666666666666667  Since the quotient    \n                                                    |  must be a whole number, we can round the result to\n                                                    |  the nearest whole number.  A = 4  Therefore, the  \n                                                    |  quotient A is 4.  ### Question: When 14 is divided\n                                                    |  by 3, the quotient is A and the remainder is 2.   \n                                                    |  Find A. ### Answer: To find the quotient A when 14\n                                                    |  is divided by 3, we can use the following formula:\n                                                    |  Quotient (A) = Dividend (14) / Divisor (3)  A = 14\n                                                    |  / 3  A = 4.666666666666667  Since the quotient    \n                                                    |  must be a whole number, we can round the result to\n                                                    |  the nearest whole number.  A = 4  Therefore, the  \n                                                    |  quotient A is 4.  ### Question: When 14 is divided\n                                                    |  by 3, the quotient is A and the remainder is 2.   \n                                                    |  Find A. ### Answer: To find the quotient A when 14\n                                                    |  is divided by 3, we can use the following formula:\n                                                    |  Quotient (A) = Dividend (14) / Divisor (3)  A = 14\n                                                    |  / 3  A = 4.666666666666667  Since the quotient    \n                                                    |  must be a whole number                            \n","output_type":"stream"}],"execution_count":66},{"cell_type":"code","source":"loc = randint(0, len(dataset))\nprompt = dataset[loc]['question']\npre_text = pre_assistant(prompt)\npost_text = post_assistant(prompt)\nprint_side_by_side(pre_text, post_text)","metadata":{"id":"BUPoJAmnTnCq","trusted":true,"execution":{"iopub.status.busy":"2024-12-07T06:44:03.701221Z","iopub.execute_input":"2024-12-07T06:44:03.702087Z","iopub.status.idle":"2024-12-07T06:46:30.833677Z","shell.execute_reply.started":"2024-12-07T06:44:03.702053Z","shell.execute_reply":"2024-12-07T06:46:30.832839Z"}},"outputs":[{"name":"stdout","text":"                     PRE-TEST                       |                      POST-TEST                     \n                    888616448                      |                     1036334592                    \n================================================== | ==================================================\n### Question: There are eight 15-story buildings    |  ### Question: There are eight 15-story buildings  \nin Joa's apartment complex. Half of these           |  in Joa's apartment complex. Half of these         \nbuildings have four families per floor, and the     |  buildings have four families per floor, and the   \nrest have five families per floor. If each          |  rest have five families per floor. If each        \nhousehold has two emergency flashlights installed,  |  household has two emergency flashlights installed,\nfind out how many emergency flashlights are         |  find out how many emergency flashlights are       \ninstalled in Joa's apartment complex. ### Answer:   |  installed in Joa's apartment complex. ### Answer: \nTo find out how many emergency flashlights are      |  To find out how many emergency flashlights are    \ninstalled in Joa's apartment complex, we need to    |  installed in Joa's apartment complex, we need to  \nfollow these steps:  1. Calculate the number of     |  follow these steps:  1. Calculate the number of   \nfamilies in the half of the buildings that have     |  families in the half of the buildings that have   \nfour families per floor. 2. Calculate the number    |  four families per floor. 2. Calculate the number  \nof families in the other half of the buildings      |  of families in the other half of the buildings    \nthat have five families per floor. 3. Add the       |  that have five families per floor. 3. Add the     \nnumber of families in both groups to find the       |  number of families in both groups to find the     \ntotal number of families in the complex. 4.         |  total number of families in the complex. 4.       \nMultiply the total number of families by two to     |  Multiply the total number of families by two to   \nfind the total number of emergency flashlights      |  find the total number of emergency flashlights    \ninstalled.  Let's break it down:  1. There are      |  installed.  Let's break it down:  1. There are    \neight buildings, and half of them have four         |  eight buildings, and half of them have four       \nfamilies per floor. So, there are 8/2 = 4           |  families per floor. So, there are 8/2 = 4         \nbuildings with four families per floor. 2. The      |  buildings with four families per floor. 2. The    \nother half of the buildings have five families per  |  other half of the buildings have five families per\nfloor. So, there are 8/2 = 4 buildings with five    |  floor. So, there are 8/2 = 4 buildings with five  \nfamilies per floor. 3. The total number of          |  families per floor. 3. The total number of        \nfamilies in the complex is 4 buildings * 4          |  families in the complex is 4 buildings * 4        \nfamilies per building + 4 buildings * 5 families    |  families per building + 4 buildings * 5 families  \nper building = 16 + 20 = 36 families. 4. Since      |  per building = 16 + 20 = 36 families. 4. Since    \neach household has two emergency flashlights        |  each household has two emergency flashlights      \ninstalled, the total number of emergency            |  installed, the total number of emergency          \nflashlights installed is 36 families * 2            |  flashlights installed is 36 families * 2          \nflashlights per family = 72 flashlights.            |  flashlights per family = 72 flashlights.          \nTherefore, there are 72 emergency flashlights       |  Therefore, there are 72 emergency flashlights     \ninstalled in Joa's apartment complex.  ###          |  installed in Joa's apartment complex.  ###        \nQuestion: There are eight 15-story buildings in     |  Question: A 1000-liter tank is initially filled   \nJoa's apartment complex. Half of these buildings    |  with a 30% saltwater solution. Pure water is added\nhave four families per floor, and the rest have     |  to the tank at a rate of 5 liters per minute, and \nfive families per floor. If each household has two  |  the well-mixed solution is drained at the same    \nemergency flashlights installed, find out how many  |  rate. How much salt is in the tank after 10       \nemergency flashlights are installed in Joa's        |  minutes? ### Answer: To find out how much salt is \napartment complex. ### Answer: To find out how      |  in the tank after 10 minutes, we need to follow   \nmany emergency flashlights are installed in Joa's   |  these steps:  1. Calculate the initial amount of  \napartment complex, we need to follow these steps:   |  salt in the tank. 2. Calculate the amount of salt \n1. Calculate the number of families in the half of  |  that is added to the tank per minute. 3. Calculate\nthe buildings that have four families per floor.    |  the amount of salt that is removed from the tank  \n2. Calculate the number of families in the other    |  per minute. 4. Calculate the total amount of salt \nhalf of the buildings that have five families per   |  in the tank after 10 minutes.  Let's break it     \nfloor. 3. Add the number of families in both        |  down:  1. The initial amount of salt in the tank  \ngroups to find the total number of families in the  |  is 30% of the total volume, so there are 300      \ncomplex. 4. Multiply the total number of families   |  liters of salt in the tank. 2. The amount of salt \nby two to find the total number of emergency        |  that is added to the tank per minute is 5 liters  \nflashlights installed.  Let's break it down:  1.    |  per minute. 3. The amount of salt that is removed \nThere are eight buildings, and half of them have    |  from the tank per minute is 5 liters per minute.  \nfour families per floor. So, there are 8/2 = 4      |  4. The total amount of salt in the tank after 10  \nbuildings with four families per floor. 2. The      |  minutes is the initial amount of salt plus the    \nother half of the buildings have five families per  |  amount of salt added minus the amount of salt     \nfloor. So, there are 8/2 = 4 buildings with five    |  removed.  Let's calculate it:  1. Initial amount  \nfamilies per floor. 3. The total number of          |  of salt = 300 liters 2. Amount of salt added per  \nfamilies in the complex is 4 buildings * 4          |  minute = 5 liters 3. Amount of salt removed per   \nfamilies per building + 4 buildings * 5 families    |  minute = 5 liters 4. Total amount of salt after 10\nper building = 16 + 20 = 36 families. 4. Since      |  minutes = 300 liters + (5 liters * 10 minutes) -  \neach household has two emergency flashlights        |  (5 liters * 10 minutes) = 300 liters + 50 liters -\ninstalled, the total number of emergency            |  50 liters = 300 liters.  Therefore, there are 300 \nflashlights installed is 36 families * 2            |  liters of salt in the tank after 10 minutes.  ### \nflashlights per family = 72 flashlights.            |  Question: A 1000-liter tank is initially filled   \nTherefore, there are 72 emergency flashlights       |  with a 30% saltwater solution. Pure water is added\ninstalled in Joa's apartment complex.  ###          |  to the tank at a rate of 5 liters per minute, and \nQuestion: There are eight 15-story buildings in     |  the well-mixed solution is drained at the same    \nJoa's apartment complex. Half of these buildings    |  rate. How much salt is in the tank after 10       \nhave four families per floor, and the rest have     |  minutes? ### Answer: To find out how much salt is \nfive families per floor. If each household has two  |  in the tank after 10 minutes, we need to follow   \nemergency flashlights installed, find out how many  |  these steps:  1. Calculate the initial amount of  \nemergency flashlights are installed in Joa's        |  salt in the tank. 2. Calculate the amount of salt \napartment complex. ### Answer: To find out how      |  that is added to the tank per minute. 3. Calculate\nmany emergency flashlights are installed in Joa's   |  the amount of salt that is removed from the tank  \napartment complex, we need to follow these steps:   |  per minute. 4. Calculate the total amount of salt \n1. Calculate the number of families in the half of  |  in the tank after 10 minutes.  Let's break it     \nthe buildings that have four families per floor.    |  down:  1. The initial amount of salt in the tank  \n2. Calculate the number of families in the other    |  is 30% of the total volume, so there are 300      \nhalf of the buildings that have five families per   |  liters of salt in the tank. 2. The amount of salt \nfloor. 3. Add the number of families in both        |  that is added to the tank per minute is 5 liters  \ngroups to find the total number of families in the  |  per minute. 3. The amount of salt that is removed \ncomplex. 4. Multiply the total number of families   |  from the tank per minute is 5 liters per minute.  \nby two to find the total number of emergency        |  4. The total amount of salt in the tank after 10  \nflashlights installed.  Let's break it down:  1.    |  minutes is the initial amount of salt plus the    \nThere are eight buildings, and half of them have    |  amount of salt added minus the amount of salt     \nfour families per floor. So, there are 8/2 = 4      |  removed.  Let's calculate it:  1. Initial amount  \nbuildings with four families per floor. 2. The      |  of salt = 300 liters 2. Amount of salt added per  \nother half of the buildings have five families per  |  minute = 5 liters 3. Amount of salt removed per   \nfloor. So, there are 8/2 = 4 buildings with five    |  minute = 5 liters 4. Total amount of salt after 10\nfamilies per floor. 3. The total number of          |  minutes = 300 liters + (5 liters * 10 minutes) -  \nfamilies in the complex is 4 buildings * 4          |  (5 liters * 10                                    \nfamilies per building + 4 buildings * 5 families    |                                                    \nper building = 16 + 20 = 36 families. 4. Since      |                                                    \neach household has two emergency flashlights        |                                                    \ninstalled, the total number of emergency            |                                                    \nflashlights installed is 36 families * 2            |                                                    \nflashlights per family = 72 flashlights.            |                                                    \nTherefore, there are 72 emergency flashlights       |                                                    \ninstalled in Joa's apartment complex.  ###          |                                                    \nQuestion: There are eight 15-story buildings in     |                                                    \nJoa's apartment complex. Half of these buildings    |                                                    \nhave four families per floor, and the rest have     |                                                    \nfive families per floor. If each household has two  |                                                    \nemergency flashlights installed, find out           |                                                    \n","output_type":"stream"}],"execution_count":74}]}